{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spelling Correction using Deep Learning\n",
    "\n",
    "Inspired by https://medium.com/@majortal/deep-spelling-9ffef96a24f6. Code can be found at https://github.com/MajorTal/DeepSpell/blob/master/keras_spell.py.\n",
    "\n",
    "**Character Sequence to Sequence** code pulled from https://github.com/mdcramer/deep-learning/tree/master/seq2seq.\n",
    "\n",
    "Environment initialization:\n",
    "* open Acaconda terminal\n",
    "* \\>activate tensorflow\n",
    "* \\>jupyter notebook\n",
    "* Comment out email calls\n",
    "\n",
    "When running on EC2 with Udactiy AMI:\n",
    "* Fix email addresses\n",
    "* \\>source activate dl\n",
    "* \\>conda update --all\n",
    "* \\>pip install tensorflow-gpu==1.1 # Tensorflow v1.1 is required\n",
    "* \\>jupyter notebook\n",
    "\n",
    "Useful commands:\n",
    "* \\>nohup python -u deep_speeling.py small > small_output.txt & # '2>nohup.err </dev/null' before '&' is optional\n",
    "* \\>nohup python -u deep_speeling.py > large_output.txt & # '2>nohup.err </dev/null' before '&' is optional\n",
    "* \\>jobs # list all nohup jobs\n",
    "* \\>ps -ef # list all running processes\n",
    "* \\>kill PID # kills process with specific PID\n",
    "* \\>watch -n 0.5 nvidia-smi # display GPU utilization\n",
    "* \\>rm -r mydir # removes directory\n",
    "\n",
    "\"I see that you have made three spelling mistakes.\" - Marquis de Favras, purportedly, upon the reading of his death warrant prior to be hanged in 1790.\n",
    "<img src=\"images/MarquisdeFavras.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize global variables\n",
    "**Make sure to run this cell first each time**\n",
    "\n",
    "Continue to work with big data. Jump down to work with small data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Command line args are: ['C:\\\\Users\\\\mcram\\\\Anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\site-packages\\\\ipykernel_launcher.py', '-f', 'C:\\\\Users\\\\mcram\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-a0aa4949-70cf-4b51-aa04-d1572abf985c.json']\n",
      "Using the large data.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import errno\n",
    "\n",
    "# Global variable around input length\n",
    "MIN_INPUT_LEN = 5 # minimum number of characters in a sentence\n",
    "MAX_INPUT_LEN = 60 # maximum number of characters in a sentence\n",
    "\n",
    "# Filenames\n",
    "NEWS_FILE_NAME = os.path.join(os.path.expanduser(\"data\"), \"news.2013.en.shuffled\") # uncompressed data file\n",
    "NEWS_FILE_NAME_CLEAN = os.path.join(os.path.expanduser(\"data\"), \"news.2013.en.clean\") # clean data file\n",
    "NEWS_FILE_NAME_FILTERED = os.path.join(os.path.expanduser(\"data\"), \"news.2013.en.filtered\")\n",
    "NEWS_FILE_NAME_TRAIN = os.path.join(os.path.expanduser(\"data\"), \"news.2013.en.train\")\n",
    "NEWS_FILE_NAME_VALIDATE = os.path.join(os.path.expanduser(\"data\"), \"news.2013.en.validate\")\n",
    "\n",
    "# Check for command line argument to use small data\n",
    "print (\"Command line args are: {}\".format(str(sys.argv)))\n",
    "small = 'small' in str(sys.argv)\n",
    "# small = True # Use this to force small data. Comment out when running script.\n",
    "\n",
    "if (small):\n",
    "    print(\"Using the small data.\")\n",
    "    directory = \"small_graph\"\n",
    "    # This is where the small graph is going to be saved and reloaded\n",
    "    GRAPH_PARAMETERS = \"small_graph/graph_params\" # Filename for storing parameters associated with the graph    \n",
    "    SOURCE_INT_TO_LETTER = \"small_graph/sourceinttoletter.json\" # Filename for INT to letter List for source sentences\n",
    "    TARGET_INT_TO_LETTER = \"small_graph/targetinttoletter.json\" # Filename for INT to letter List for target sentences\n",
    "    SOURCE_LETTER_TO_INT = \"small_graph/sourcelettertoint.json\" # Filename for letter to INT List for source sentences\n",
    "    TARGET_LETTER_TO_INT = \"small_graph/targetlettertoint.json\" # Filename for letter to INT List for source sentences\n",
    "    checkpoint = \"./small_graph/best_model.ckpt\"\n",
    "else:\n",
    "    print(\"Using the large data.\")\n",
    "    # This is where the large graph is going to be saved and reloaded\n",
    "    directory = \"large_graph\"\n",
    "    GRAPH_PARAMETERS = \"large_graph/graph_params\" # Filename for storing parameters associated with the graph    \n",
    "    SOURCE_INT_TO_LETTER = \"large_graph/sourceinttoletter.json\" # Filename for INT to letter List for source sentences\n",
    "    TARGET_INT_TO_LETTER = \"large_graph/targetinttoletter.json\" # Filename for INT to letter List for target sentences\n",
    "    SOURCE_LETTER_TO_INT = \"large_graph/sourcelettertoint.json\" # Filename for letter to INT List for source sentences\n",
    "    TARGET_LETTER_TO_INT = \"large_graph/targetlettertoint.json\" # Filename for letter to INT List for source sentences\n",
    "    checkpoint = \"./large_graph/best_model.ckpt\"\n",
    "\n",
    "# create directory for data, large or small, if it does not already exist\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except OSError as exception:\n",
    "    if exception.errno != errno.EEXIST:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for sending email updates from AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "# AWS Config\n",
    "EMAIL_HOST = 'email-smtp.us-west-2.amazonaws.com'\n",
    "EMAIL_HOST_USER = 'AKIAJKVANBDPILI5UNYA'\n",
    "EMAIL_HOST_PASSWORD = 'Ava4fqQT7ux9iz22ToSjFqvZB5mzHE/wzD3Ib4p/27VJ'\n",
    "EMAIL_PORT = 587\n",
    "\n",
    "def send_email(subject, message):\n",
    "\n",
    "    # Do not upload to Github with real email addresses\n",
    "    me = \"m@mba.edu\"\n",
    "    you = [\"m@alum.edu\", \"bf@gmail.com\"]\n",
    "\n",
    "    # Construct email\n",
    "    msg = MIMEMultipart('alternative')\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] = me\n",
    "    msg['To'] = \", \".join(you)\n",
    "    msg.attach(MIMEText(message, 'plain'))\n",
    "\n",
    "    # html = open('index.html').read()\n",
    "    # mime_text = MIMEText(html, 'html')\n",
    "    # msg.attach(mime_text)\n",
    "\n",
    "    s = smtplib.SMTP(EMAIL_HOST, EMAIL_PORT)\n",
    "    s.starttls()\n",
    "    s.login(EMAIL_HOST_USER, EMAIL_HOST_PASSWORD)\n",
    "    s.sendmail(me, you, msg.as_string()) # (from, to, message)\n",
    "    s.quit()\n",
    "    \n",
    "    print(\"Email update sent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Download raw data file from the internet and uncompress it\n",
    "\n",
    "[One Billion Word Benchmark for Measuring Progress in Statistical Language Modeling](https://research.google.com/pubs/pub41880.html)\n",
    "\n",
    "**Start here if you are going to work with the big dataset** The dataset lives in the /data/ folder.\n",
    "\n",
    "**Skip to below to work with the small dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download_raw_datafile():\n",
    "    \n",
    "    import errno\n",
    "    import requests\n",
    "    import gzip\n",
    "\n",
    "    NEWS_FILE_NAME_COMPRESSED = os.path.join(os.path.expanduser(\"data\"), \"news.2013.en.shuffled.gz\") # 1.1 GB file\n",
    "    DATA_FILES_URL = \"http://www.statmt.org/wmt14/training-monolingual-news-crawl/news.2013.en.shuffled.gz\" # file location\n",
    "\n",
    "    # create directory for data, if it does not already exist\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(NEWS_FILE_NAME_COMPRESSED))\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "    # check size of current data file\n",
    "    try:\n",
    "        current_size = os.path.getsize(NEWS_FILE_NAME_COMPRESSED)\n",
    "    except:\n",
    "        current_size = 0\n",
    "\n",
    "    # check size of data file on internet\n",
    "    response = requests.get(DATA_FILES_URL, stream=True)\n",
    "    total_length = response.headers.get('content-length') # returns a str\n",
    "    total_length = int(total_length)\n",
    "\n",
    "    # download file if it is larger than the one already in the data directory\n",
    "    if (total_length > current_size):\n",
    "        print(\"Download compressed data file\")\n",
    "        with open(NEWS_FILE_NAME_COMPRESSED, \"wb\") as output_file: # open for writing in binary mode\n",
    "            downloaded = percentage = 0\n",
    "            print(\"»\"*100)\n",
    "            for data in response.iter_content(chunk_size=4096):\n",
    "                downloaded += len(data)\n",
    "                output_file.write(data)\n",
    "                new_percentage = 100 * downloaded // total_length # // is floor divide\n",
    "                if new_percentage > percentage:\n",
    "                    print(\"o\", end=\"\") # end=\"\" remove carriage return\n",
    "                    percentage = new_percentage\n",
    "        print() # add carriage return at the end of progress indicator\n",
    "    else:\n",
    "        print(\"Local copy of compressed data file is up to date.\")\n",
    "\n",
    "    # uncompress data\n",
    "    if (os.path.isfile(NEWS_FILE_NAME_COMPRESSED[:-3])): # check to see if file already exists\n",
    "        print(\"Data file is already uncompressed.\")\n",
    "    else:\n",
    "        print(\"Uncompress data file.\") # uncompress the file if it does not\n",
    "        with gzip.open(NEWS_FILE_NAME_COMPRESSED, 'rb') as compressed_file:\n",
    "            with open(NEWS_FILE_NAME_COMPRESSED[:-3], 'wb') as outfile: #2.5 GB file\n",
    "                outfile.write(compressed_file.read())\n",
    "        print(\"Data file uncompressed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data\n",
    "Takes the `news.2013.en.clean` and input and produces `news.2013.en.shuffled`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_data():\n",
    "    \n",
    "    import re\n",
    "\n",
    "    NORMALIZE_WHITESPACE_REGEX = re.compile(r'[^\\S\\n]+', re.UNICODE) # match all whitespace except newlines\n",
    "    RE_DASH_FILTER = re.compile(r'[\\-\\˗\\֊\\‐\\‑\\‒\\–\\—\\⁻\\₋\\−\\﹣\\－]', re.UNICODE)\n",
    "    RE_APOSTROPHE_FILTER = re.compile(r'&#39;|[ʼ՚＇‘’‛❛❜ߴߵ`‵´ˊˋ{}{}{}{}{}{}{}{}{}]'\n",
    "                                      .format(chr(768), chr(769), chr(832), chr(833), chr(2387),\n",
    "                                              chr(5151), chr(5152), chr(65344), chr(8242)), re.UNICODE)\n",
    "    RE_LEFT_PARENTH_FILTER = re.compile(r'[\\(\\[\\{\\⁽\\₍\\❨\\❪\\﹙\\（]', re.UNICODE)\n",
    "    RE_RIGHT_PARENTH_FILTER = re.compile(r'[\\)\\]\\}\\⁾\\₎\\❩\\❫\\﹚\\）]', re.UNICODE)\n",
    "    ALLOWED_CURRENCIES = \"\"\"¥£₪$€฿₨\"\"\"\n",
    "    ALLOWED_PUNCTUATION = \"\"\"-!?/;\"'%&<>.()[]{}@#:,|=*\"\"\"\n",
    "    RE_BASIC_CLEANER = re.compile(r'[^\\w\\s{}{}]'\n",
    "                                  .format(re.escape(ALLOWED_CURRENCIES), re.escape(ALLOWED_PUNCTUATION)), re.UNICODE)\n",
    "\n",
    "    def file_len(fname):\n",
    "        with open(fname, encoding=\"utf8\") as f:\n",
    "            for i, l in enumerate(f):\n",
    "                pass\n",
    "        return i + 1\n",
    "\n",
    "    def clean_text(text):\n",
    "        # Clean the text - remove unwanted chars, fold punctuation etc.\n",
    "        result = NORMALIZE_WHITESPACE_REGEX.sub(' ', text.strip())\n",
    "        result = RE_DASH_FILTER.sub('-', result)\n",
    "        result = RE_APOSTROPHE_FILTER.sub(\"'\", result)\n",
    "        result = RE_LEFT_PARENTH_FILTER.sub(\"(\", result)\n",
    "        result = RE_RIGHT_PARENTH_FILTER.sub(\")\", result)\n",
    "        result = RE_BASIC_CLEANER.sub('', result)\n",
    "        return result\n",
    "\n",
    "    if (os.path.isfile(NEWS_FILE_NAME_CLEAN)):\n",
    "        print(\"Data file is already clean.\")\n",
    "    else:    \n",
    "        print(\"Clean data file:\")\n",
    "        number_lines = file_len(NEWS_FILE_NAME)\n",
    "        with open(NEWS_FILE_NAME_CLEAN, \"wb\") as clean_data:\n",
    "            processed = percentage = 0\n",
    "            for line in open(NEWS_FILE_NAME, encoding=\"utf8\"):\n",
    "                processed += 1\n",
    "                # decoded_line = line.decode('utf-8') # https://stackoverflow.com/a/28583969/852795\n",
    "                cleaned_line = clean_text(line)\n",
    "                encoded_line = cleaned_line.encode(\"utf-8\")\n",
    "                clean_data.write(encoded_line + b\"\\n\")\n",
    "                new_percentage = 100 * processed // number_lines\n",
    "                if (new_percentage > percentage):\n",
    "                    print(\"{0:2d}\".format(new_percentage), \"%: \", line, end=\"\")\n",
    "                    percentage = new_percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the charaters\n",
    "\n",
    "Get counts of all of the characters and select the top ones for processing and filter only sentences with the right charcters. Eliminate any sentences that are too small or too long.\n",
    "\n",
    "Takes `news.2013.en.shuffled` as input and produces `news.2013.en.filtered` and `news.2013.en.char_frequency.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyze_characters():\n",
    "    \n",
    "    from collections import Counter\n",
    "    import json\n",
    "\n",
    "    NUMBER_OF_CHARS = 75 # Quantity of most popular characters to keep. Was 100 in original code.\n",
    "    CHAR_FREQUENCY_FILE_NAME = os.path.join(os.path.expanduser(\"data\"), \"news.2013.en.char_frequency.json\")\n",
    "\n",
    "    # create character frequency file\n",
    "    if (os.path.isfile(CHAR_FREQUENCY_FILE_NAME)):\n",
    "        print(\"Character frequency file already created.\")\n",
    "    else:\n",
    "        counter = Counter()\n",
    "        print(\"Reading data file:\")\n",
    "        for line in open(NEWS_FILE_NAME_CLEAN, encoding=\"utf8\"):\n",
    "            counter.update(line)\n",
    "        print(\"Done. Writing to file:\")\n",
    "        with open(CHAR_FREQUENCY_FILE_NAME, 'wb') as output_file:\n",
    "            output_file.write(json.dumps(counter).encode(\"utf-8\"))\n",
    "        most_popular_chars = {key for key, _value in counter.most_common(NUMBER_OF_CHARS)}\n",
    "\n",
    "    # Read top characters that were saved to file\n",
    "    chars = json.loads(open(CHAR_FREQUENCY_FILE_NAME).read())\n",
    "    counter = Counter(chars)\n",
    "    most_popular_chars = {key for key, _value in counter.most_common(NUMBER_OF_CHARS)}\n",
    "    print(\"The top %s chars are:\", NUMBER_OF_CHARS)\n",
    "    print(\"\".join(sorted(most_popular_chars)))\n",
    "\n",
    "    # Filter only sentences with the right chars\n",
    "    if (os.path.isfile(NEWS_FILE_NAME_FILTERED)):\n",
    "        print(\"\\nFiltered file already created.\")\n",
    "    else:\n",
    "        print(\"\\nReading and filtering data:\")\n",
    "        num_lines = 0\n",
    "        with open(NEWS_FILE_NAME_FILTERED, \"wb\") as output_file:\n",
    "            for line in open(NEWS_FILE_NAME_CLEAN, encoding=\"utf8\"):\n",
    "                if line and (not bool(set(line) - most_popular_chars)) and (MAX_INPUT_LEN >= len(line) > MIN_INPUT_LEN):\n",
    "                    output_file.write(line.encode(\"utf8\"))\n",
    "                    num_lines += 1\n",
    "                    if (num_lines % 1000000 == 0):\n",
    "                        print(\"{0:10,d}\".format(num_lines), \": \", line, end=\"\")\n",
    "        print(\"Done. Filtered file contains {:,} lines.\".format(num_lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into training and validation sets\n",
    "Takes `news.2013.en.filtered` as input and produces `news.2013.en.train` and `news.2013.en.validate`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data():\n",
    "    \n",
    "    from numpy.random import shuffle as random_shuffle\n",
    "\n",
    "    if (os.path.isfile(NEWS_FILE_NAME_TRAIN)):\n",
    "        print(\"Training and Validation files already created.\")\n",
    "    else:\n",
    "        answers = open(NEWS_FILE_NAME_FILTERED, encoding=\"utf8\").read().split(\"\\n\")\n",
    "        print('shuffle', end=\" \")\n",
    "        random_shuffle(answers)\n",
    "        print(\"Done\")\n",
    "        # Explicitly set apart 10% for validation data that we never train over\n",
    "        # TODO skip if files already exist\n",
    "        split_at = len(answers) - len(answers) // 10\n",
    "        with open(NEWS_FILE_NAME_TRAIN, \"wb\") as output_file:\n",
    "            output_file.write(\"\\n\".join(answers[:split_at]).encode('utf-8'))\n",
    "        with open(NEWS_FILE_NAME_VALIDATE, \"wb\") as output_file:\n",
    "            output_file.write(\"\\n\".join(answers[split_at:]).encode('utf-8'))\n",
    "        print(\"\\nTraining and Validation files written.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the target data and generate source data by injecting mistakes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_noise_to_string(a_string, amount_of_noise): # Add artificial spelling mistakes to string    \n",
    "    \n",
    "    from numpy.random import choice as random_choice, randint as random_randint, seed as random_seed, rand\n",
    "\n",
    "    CHARS = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .\")\n",
    "\n",
    "    if rand() < amount_of_noise * len(a_string):\n",
    "        # Replace a character with a random character\n",
    "        random_char_position = random_randint(len(a_string))\n",
    "        a_string = a_string[:random_char_position] + random_choice(CHARS[:-1]) + a_string[random_char_position + 1:]\n",
    "    if rand() < amount_of_noise * len(a_string):\n",
    "        # Delete a character\n",
    "        random_char_position = random_randint(len(a_string))\n",
    "        a_string = a_string[:random_char_position] + a_string[random_char_position + 1:]\n",
    "    if len(a_string) < MAX_INPUT_LEN and rand() < amount_of_noise * len(a_string):\n",
    "        # Add a random character\n",
    "        random_char_position = random_randint(len(a_string))\n",
    "        a_string = a_string[:random_char_position] + random_choice(CHARS[:-1]) + a_string[random_char_position:]\n",
    "    if rand() < amount_of_noise * len(a_string):\n",
    "        # Transpose 2 characters\n",
    "        random_char_position = random_randint(len(a_string) - 1)\n",
    "        a_string = (a_string[:random_char_position] + a_string[random_char_position + 1] + \n",
    "                    a_string[random_char_position] + a_string[random_char_position + 2:])\n",
    "    return a_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_big_data():\n",
    "\n",
    "    AMOUNT_OF_NOISE = 0.2 / MAX_INPUT_LEN\n",
    "\n",
    "    #TODO save file with source_sentences so no need to recompute\n",
    "    target_sentences = open(NEWS_FILE_NAME_TRAIN, encoding=\"utf8\").read().split(\"\\n\")    \n",
    "    source_sentences = open(NEWS_FILE_NAME_TRAIN, encoding=\"utf8\").read().split(\"\\n\")\n",
    "    for i in range(len(source_sentences)):\n",
    "        source_sentences[i] = add_noise_to_string(source_sentences[i], AMOUNT_OF_NOISE)\n",
    "\n",
    "    print('\\nFirst 10 sentence:')\n",
    "    for i in range (0, 10):\n",
    "        print(\"\\nSource --> \" + source_sentences[i])\n",
    "        print(\"Target --> \" + target_sentences[i])\n",
    "        \n",
    "    # Datasets\n",
    "    # Take a look at the initial source of target datasets\n",
    "    print(\"\\nThe source is comprised of {:,} sentences. Here are the first 10.\".format(len(source_sentences)))\n",
    "    print(\"\\n\".join(source_sentences[:10]))\n",
    "    \n",
    "    print(\"\\nThe target is comprised of {:,} sentences. Here are the first 10.\".format(len(target_sentences)))\n",
    "    print(\"\\n\".join(target_sentences[:10]))\n",
    "    \n",
    "    return source_sentences, target_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this cell to get the tiny data set\n",
    "**Start here if you are going to run with the small dataset** If you are using the big dataset, make sure to skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Run this cell to grab the small data sets that came with this model. Otherwise skip it.\n",
    "# The dataset lives in the /data/ folder. At the moment, it is made up of the following files:\n",
    "# letters_source.txt: The list of input letter sequences. Each sequence is its own line. \n",
    "# letters_target.txt: The list of target sequences we'll use in the training process.\n",
    "# Each sequence here is a response to the input sequence in letters_source.txt with the same line number.\n",
    "\n",
    "def load_small_data():\n",
    "    \n",
    "    import helper\n",
    "\n",
    "    source_path = 'data/letters_source.txt'\n",
    "    target_path = 'data/letters_target.txt'\n",
    "\n",
    "    source_sentences = helper.load_data(source_path).split('\\n') # added .split('\\n) to be consistent with big data\n",
    "    target_sentences = helper.load_data(target_path).split('\\n')\n",
    "\n",
    "    # source_sentences contains the entire input sequence file as text delimited by newline symbols.\n",
    "    print(\"Source: {}\".format(source_sentences[:10]))\n",
    "    # target_sentences contains the entire output sequence file as text delimited by newline symbols.\n",
    "    # Each line corresponds to the line from source_sentences. target_sentences contains sorted characters of the line.\n",
    "    print(\"Target: {}\".format(target_sentences[:10]))\n",
    "\n",
    "    print(\"\\nThe source is comprised of {:,} sentences.\".format(len(source_sentences)))\n",
    "    \n",
    "    return source_sentences, target_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data - Big of Small\n",
    "If the command line switch \"small\" is set then load the small data. Otherwise load the big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load up the big data.\n",
      "Local copy of compressed data file is up to date.\n",
      "Data file is already uncompressed.\n",
      "Data file is already clean.\n",
      "Reading data file:\n",
      "Done. Writing to file:\n",
      "The top %s chars are: 75\n",
      "\n",
      " \"$'(),-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWYabcdefghijklmnopqrstuvwxyzé\n",
      "\n",
      "Reading and filtering data:\n",
      " 1,000,000 :  It's on to Rooty Hill for Team Abbott.\n",
      " 2,000,000 :  PMMA pills are slower to take effect.\n",
      " 3,000,000 :  It compliments hydropower, which is seasonal.\n",
      " 4,000,000 :  The High Line's Next Neighbor\n",
      "Done. Filtered file contains 4,615,704 lines.\n",
      "shuffle Done\n",
      "\n",
      "Training and Validation files written.\n",
      "\n",
      "First 10 sentence:\n",
      "\n",
      "Source --> \"Men We Reaped\" is that larger story.\n",
      "Target --> \"Men We Reaped\" is that larger story.\n",
      "\n",
      "Source --> Theer's a far bigger and more complex picture.\n",
      "Target --> There's a far bigger and more complex picture.\n",
      "\n",
      "Source --> Shanghai Pudong Development Bank Co.\n",
      "Target --> Shanghai Pudong Development Bank Co.\n",
      "\n",
      "Source --> De André IRUKOFF (AFP) - Il y a 42 minutes\n",
      "Target --> De André BIRUKOFF (AFP) - Il y a 42 minutes\n",
      "\n",
      "Source --> Y chwilio am Ben Thompson yn parhau\n",
      "Target --> Y chwilio am Ben Thompson yn parhau\n",
      "\n",
      "Source --> January 31, 2013 -- Updated 002 6GMT (0826 HKT)\n",
      "Target --> January 31, 2013 -- Updated 0026 GMT (0826 HKT)\n",
      "\n",
      "Source --> Movie Review: 'The Wolverine'\n",
      "Target --> Movie Review: 'The Wolverine'\n",
      "\n",
      "Source --> Libya crisis deepens as rebel groups expand demands\n",
      "Target --> Libya crisis deepens as rebel groups expand demands\n",
      "\n",
      "Source --> He then resigned.\n",
      "Target --> He then resigned.\n",
      "\n",
      "Source --> Would this be her that I'm getting a phone call about?\n",
      "Target --> Would this be her that I'm getting a phone call about?\n",
      "\n",
      "The source is comprised of 4,154,135 sentences. Here are the first 10.\n",
      "\"Men We Reaped\" is that larger story.\n",
      "Theer's a far bigger and more complex picture.\n",
      "Shanghai Pudong Development Bank Co.\n",
      "De André IRUKOFF (AFP) - Il y a 42 minutes\n",
      "Y chwilio am Ben Thompson yn parhau\n",
      "January 31, 2013 -- Updated 002 6GMT (0826 HKT)\n",
      "Movie Review: 'The Wolverine'\n",
      "Libya crisis deepens as rebel groups expand demands\n",
      "He then resigned.\n",
      "Would this be her that I'm getting a phone call about?\n",
      "\n",
      "The target is comprised of 4,154,135 sentences. Here are the first 10.\n",
      "\"Men We Reaped\" is that larger story.\n",
      "There's a far bigger and more complex picture.\n",
      "Shanghai Pudong Development Bank Co.\n",
      "De André BIRUKOFF (AFP) - Il y a 42 minutes\n",
      "Y chwilio am Ben Thompson yn parhau\n",
      "January 31, 2013 -- Updated 0026 GMT (0826 HKT)\n",
      "Movie Review: 'The Wolverine'\n",
      "Libya crisis deepens as rebel groups expand demands\n",
      "He then resigned.\n",
      "Would this be her that I'm getting a phone call about?\n"
     ]
    }
   ],
   "source": [
    "if (small):\n",
    "    print(\"Load up the small data.\")\n",
    "    source_sentences, target_sentences = load_small_data()\n",
    "else:\n",
    "    print(\"Load up the big data.\")\n",
    "    download_raw_datafile()\n",
    "    clean_data()\n",
    "    analyze_characters()\n",
    "    split_data()\n",
    "    source_sentences, target_sentences = load_big_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "To do anything useful with it, turn the each string into a list of characters. Then convert the characters to their int values as declared in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define global variables\n",
    "source_int_to_letter = []\n",
    "target_int_to_letter = []\n",
    "source_letter_to_int = []\n",
    "target_letter_to_int = []\n",
    "\n",
    "def extract_character_vocab(data):\n",
    "    special_words = ['<PAD>', '<UNK>', '<GO>',  '<EOS>']\n",
    "\n",
    "    #set_words = set([character for line in data.split('\\n') for character in line])\n",
    "    set_words = set([character for line in data for character in line])\n",
    "    int_to_vocab = {word_i: word for word_i, word in enumerate(special_words + list(set_words))}\n",
    "    vocab_to_int = {word: word_i for word_i, word in int_to_vocab.items()}\n",
    "\n",
    "    return int_to_vocab, vocab_to_int\n",
    "    \n",
    "def load_int_letter_translations(source, target):\n",
    "    \n",
    "    global source_int_to_letter, target_int_to_letter, source_letter_to_int, target_letter_to_int\n",
    "    \n",
    "    # Check to see if conversion files have already been created\n",
    "    if (os.path.isfile(SOURCE_INT_TO_LETTER)):\n",
    "\n",
    "        print()\n",
    "        # Load up all of the conversion files\n",
    "        with open(SOURCE_INT_TO_LETTER, 'r') as file:\n",
    "            try:\n",
    "                source_int_to_letter = json.load(file)\n",
    "                print(\"Read {} data from file.\".format(SOURCE_INT_TO_LETTER))\n",
    "            except ValueError: # if the file is empty the ValueError will be thrown\n",
    "                data = {}\n",
    "        source_int_to_letter = {int(k):v for k,v in source_int_to_letter.items()}\n",
    "        with open(TARGET_INT_TO_LETTER, 'r') as file:\n",
    "            try:\n",
    "                target_int_to_letter = json.load(file)\n",
    "                print(\"Read {} data from file.\".format(TARGET_INT_TO_LETTER))\n",
    "            except ValueError: # if the file is empty the ValueError will be thrown\n",
    "                data = {}\n",
    "        target_int_to_letter = {int(k):v for k,v in target_int_to_letter.items()}\n",
    "        with open(SOURCE_LETTER_TO_INT, 'r') as file:\n",
    "            try:\n",
    "                source_letter_to_int = json.load(file)\n",
    "                print(\"Read {} data from file.\".format(SOURCE_LETTER_TO_INT))\n",
    "            except ValueError: # if the file is empty the ValueError will be thrown\n",
    "                data = {}\n",
    "        source_letter_to_int = {k:int(v) for k,v in source_letter_to_int.items()}\n",
    "        with open(TARGET_LETTER_TO_INT, 'r') as file:\n",
    "            try:\n",
    "                target_letter_to_int = json.load(file)\n",
    "                print(\"Read {} data from file.\".format(TARGET_LETTER_TO_INT))\n",
    "            except ValueError: # if the file is empty the ValueError will be thrown\n",
    "                data = {}\n",
    "        target_letter_to_int = {k:int(v) for k,v in target_letter_to_int.items()}\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Build int2letter and letter2int dicts\n",
    "        source_int_to_letter, source_letter_to_int = extract_character_vocab(source)\n",
    "        target_int_to_letter, target_letter_to_int = extract_character_vocab(target)\n",
    "        print(\"Source INT to letter: {}\".format(source_int_to_letter))\n",
    "        print(\"Target INT to letter: {}\\n\".format(target_int_to_letter))\n",
    "\n",
    "        # Save source_int_to_letter, target_int_to_letter & source_letter_to_int for loading later after graph is saved\n",
    "        with open(SOURCE_INT_TO_LETTER, 'w') as output_file:\n",
    "            json.dump(source_int_to_letter, output_file)\n",
    "        print(\"Wrote {} data to file.\".format(SOURCE_INT_TO_LETTER))\n",
    "        with open(TARGET_INT_TO_LETTER, 'w') as output_file:\n",
    "            json.dump(target_int_to_letter, output_file)\n",
    "        print(\"Wrote {} data to file.\".format(TARGET_INT_TO_LETTER))\n",
    "        with open(SOURCE_LETTER_TO_INT, 'w') as output_file:\n",
    "            json.dump(source_letter_to_int, output_file)\n",
    "        print(\"Wrote {} data to file.\".format(SOURCE_LETTER_TO_INT))\n",
    "        with open(TARGET_LETTER_TO_INT, 'w') as output_file:\n",
    "            json.dump(target_letter_to_int, output_file)\n",
    "        print(\"Wrote {} data to file.\".format(TARGET_LETTER_TO_INT))\n",
    "\n",
    "def produce_letter_ids(source, target):\n",
    "    \n",
    "    if (not source_int_to_letter):\n",
    "        load_int_letter_translations(source, target)\n",
    "    \n",
    "    # Convert characters to ids\n",
    "    source_ids = [[source_letter_to_int.get(letter, source_letter_to_int['<UNK>']) for letter in line] \\\n",
    "                         for line in source]\n",
    "    target_ids = [[target_letter_to_int.get(letter, target_letter_to_int['<UNK>']) for letter in line] \\\n",
    "                         + [target_letter_to_int['<EOS>']] for line in target]\n",
    "    \n",
    "    return source_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Read large_graph/sourceinttoletter.json data from file.\n",
      "Read large_graph/targetinttoletter.json data from file.\n",
      "Read large_graph/sourcelettertoint.json data from file.\n",
      "Read large_graph/targetlettertoint.json data from file.\n",
      "\n",
      "Example source sequences\n",
      "[[81, 14, 44, 8, 70, 7, 44, 70, 58, 44, 76, 53, 44, 48, 81, 70, 52, 86, 70, 98, 21, 76, 98, 70, 46, 76, 74, 32, 44, 74, 70, 86, 98, 12, 74, 100, 83], [34, 21, 44, 44, 74, 87, 86, 70, 76, 70, 41, 76, 74, 70, 38, 52, 32, 32, 44, 74, 70, 76, 8, 48, 70, 59, 12, 74, 44, 70, 73, 12, 59, 53, 46, 44, 71, 70, 53, 52, 73, 98, 57, 74, 44, 83], [72, 21, 76, 8, 32, 21, 76, 52, 70, 60, 57, 48, 12, 8, 32, 70, 47, 44, 55, 44, 46, 12, 53, 59, 44, 8, 98, 70, 97, 76, 8, 65, 70, 89, 12, 83]]\n",
      "\n",
      "Example target sequences\n",
      "[[81, 14, 44, 8, 70, 7, 44, 70, 58, 44, 76, 53, 44, 48, 81, 70, 52, 86, 70, 98, 21, 76, 98, 70, 46, 76, 74, 32, 44, 74, 70, 86, 98, 12, 74, 100, 83, 3], [34, 21, 44, 74, 44, 87, 86, 70, 76, 70, 41, 76, 74, 70, 38, 52, 32, 32, 44, 74, 70, 76, 8, 48, 70, 59, 12, 74, 44, 70, 73, 12, 59, 53, 46, 44, 71, 70, 53, 52, 73, 98, 57, 74, 44, 83, 3], [72, 21, 76, 8, 32, 21, 76, 52, 70, 60, 57, 48, 12, 8, 32, 70, 47, 44, 55, 44, 46, 12, 53, 59, 44, 8, 98, 70, 97, 76, 8, 65, 70, 89, 12, 83, 3]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert source and target sentences into IDs\n",
    "source_letter_ids, target_letter_ids = produce_letter_ids(source_sentences, target_sentences)\n",
    "\n",
    "print(\"\\nExample source sequences\")\n",
    "print(source_letter_ids[:3])\n",
    "print(\"\\nExample target sequences\")\n",
    "print(target_letter_ids[:3])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First 10 sentence:\n",
      "\n",
      "Source --> [81, 14, 44, 8, 70, 7, 44, 70, 58, 44, 76, 53, 44, 48, 81, 70, 52, 86, 70, 98, 21, 76, 98, 70, 46, 76, 74, 32, 44, 74, 70, 86, 98, 12, 74, 100, 83]\n",
      "Target --> [81, 14, 44, 8, 70, 7, 44, 70, 58, 44, 76, 53, 44, 48, 81, 70, 52, 86, 70, 98, 21, 76, 98, 70, 46, 76, 74, 32, 44, 74, 70, 86, 98, 12, 74, 100, 83, 3]\n",
      "\n",
      "Source --> [34, 21, 44, 44, 74, 87, 86, 70, 76, 70, 41, 76, 74, 70, 38, 52, 32, 32, 44, 74, 70, 76, 8, 48, 70, 59, 12, 74, 44, 70, 73, 12, 59, 53, 46, 44, 71, 70, 53, 52, 73, 98, 57, 74, 44, 83]\n",
      "Target --> [34, 21, 44, 74, 44, 87, 86, 70, 76, 70, 41, 76, 74, 70, 38, 52, 32, 32, 44, 74, 70, 76, 8, 48, 70, 59, 12, 74, 44, 70, 73, 12, 59, 53, 46, 44, 71, 70, 53, 52, 73, 98, 57, 74, 44, 83, 3]\n",
      "\n",
      "Source --> [72, 21, 76, 8, 32, 21, 76, 52, 70, 60, 57, 48, 12, 8, 32, 70, 47, 44, 55, 44, 46, 12, 53, 59, 44, 8, 98, 70, 97, 76, 8, 65, 70, 89, 12, 83]\n",
      "Target --> [72, 21, 76, 8, 32, 21, 76, 52, 70, 60, 57, 48, 12, 8, 32, 70, 47, 44, 55, 44, 46, 12, 53, 59, 44, 8, 98, 70, 97, 76, 8, 65, 70, 89, 12, 83, 3]\n",
      "\n",
      "Source --> [47, 44, 70, 91, 8, 48, 74, 78, 70, 18, 58, 6, 42, 88, 33, 33, 70, 10, 91, 33, 60, 56, 70, 75, 70, 18, 46, 70, 100, 70, 76, 70, 43, 51, 70, 59, 52, 8, 57, 98, 44, 86]\n",
      "Target --> [47, 44, 70, 91, 8, 48, 74, 78, 70, 97, 18, 58, 6, 42, 88, 33, 33, 70, 10, 91, 33, 60, 56, 70, 75, 70, 18, 46, 70, 100, 70, 76, 70, 43, 51, 70, 59, 52, 8, 57, 98, 44, 86, 3]\n",
      "\n",
      "Source --> [68, 70, 73, 21, 11, 52, 46, 52, 12, 70, 76, 59, 70, 97, 44, 8, 70, 34, 21, 12, 59, 53, 86, 12, 8, 70, 100, 8, 70, 53, 76, 74, 21, 76, 57]\n",
      "Target --> [68, 70, 73, 21, 11, 52, 46, 52, 12, 70, 76, 59, 70, 97, 44, 8, 70, 34, 21, 12, 59, 53, 86, 12, 8, 70, 100, 8, 70, 53, 76, 74, 21, 76, 57, 3]\n",
      "\n",
      "Source --> [29, 76, 8, 57, 76, 74, 100, 70, 69, 36, 45, 70, 51, 25, 36, 69, 70, 75, 75, 70, 6, 53, 48, 76, 98, 44, 48, 70, 25, 25, 51, 70, 4, 20, 14, 34, 70, 10, 25, 82, 51, 4, 70, 63, 42, 34, 56]\n",
      "Target --> [29, 76, 8, 57, 76, 74, 100, 70, 69, 36, 45, 70, 51, 25, 36, 69, 70, 75, 75, 70, 6, 53, 48, 76, 98, 44, 48, 70, 25, 25, 51, 4, 70, 20, 14, 34, 70, 10, 25, 82, 51, 4, 70, 63, 42, 34, 56, 3]\n",
      "\n",
      "Source --> [14, 12, 55, 52, 44, 70, 58, 44, 55, 52, 44, 11, 77, 70, 87, 34, 21, 44, 70, 7, 12, 46, 55, 44, 74, 52, 8, 44, 87]\n",
      "Target --> [14, 12, 55, 52, 44, 70, 58, 44, 55, 52, 44, 11, 77, 70, 87, 34, 21, 44, 70, 7, 12, 46, 55, 44, 74, 52, 8, 44, 87, 3]\n",
      "\n",
      "Source --> [16, 52, 38, 100, 76, 70, 73, 74, 52, 86, 52, 86, 70, 48, 44, 44, 53, 44, 8, 86, 70, 76, 86, 70, 74, 44, 38, 44, 46, 70, 32, 74, 12, 57, 53, 86, 70, 44, 71, 53, 76, 8, 48, 70, 48, 44, 59, 76, 8, 48, 86]\n",
      "Target --> [16, 52, 38, 100, 76, 70, 73, 74, 52, 86, 52, 86, 70, 48, 44, 44, 53, 44, 8, 86, 70, 76, 86, 70, 74, 44, 38, 44, 46, 70, 32, 74, 12, 57, 53, 86, 70, 44, 71, 53, 76, 8, 48, 70, 48, 44, 59, 76, 8, 48, 86, 3]\n",
      "\n",
      "Source --> [63, 44, 70, 98, 21, 44, 8, 70, 74, 44, 86, 52, 32, 8, 44, 48, 83]\n",
      "Target --> [63, 44, 70, 98, 21, 44, 8, 70, 74, 44, 86, 52, 32, 8, 44, 48, 83, 3]\n",
      "\n",
      "Source --> [7, 12, 57, 46, 48, 70, 98, 21, 52, 86, 70, 38, 44, 70, 21, 44, 74, 70, 98, 21, 76, 98, 70, 18, 87, 59, 70, 32, 44, 98, 98, 52, 8, 32, 70, 76, 70, 53, 21, 12, 8, 44, 70, 73, 76, 46, 46, 70, 76, 38, 12, 57, 98, 96]\n",
      "Target --> [7, 12, 57, 46, 48, 70, 98, 21, 52, 86, 70, 38, 44, 70, 21, 44, 74, 70, 98, 21, 76, 98, 70, 18, 87, 59, 70, 32, 44, 98, 98, 52, 8, 32, 70, 76, 70, 53, 21, 12, 8, 44, 70, 73, 76, 46, 46, 70, 76, 38, 12, 57, 98, 96, 3]\n"
     ]
    }
   ],
   "source": [
    "print('\\nFirst 10 sentence:')\n",
    "for i in range (0, 10):\n",
    "    print(\"\\nSource --> {}\".format(source_letter_ids[i]))\n",
    "    print(\"Target --> {}\".format(target_letter_ids[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character Sequence to Sequence Model\n",
    "This model was updated to work with TensorFlow 1.1 and builds on the work of Dave Currie. Check out Dave's post [Text Summarization with Amazon Reviews](https://medium.com/towards-data-science/text-summarization-with-amazon-reviews-41801c2210b).\n",
    "<img src=\"images/sequence-to-sequence.jpg\"/>\n",
    "#### Check the Version of TensorFlow and wether or not there's a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.3.0\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using hyperparameters for the big data with 4,154,135 source sentences.\n"
     ]
    }
   ],
   "source": [
    "if (len(source_sentences) > 10000):\n",
    "    \n",
    "    # We are using the big data\n",
    "    print(\"Using hyperparameters for the big data with {:,} source sentences.\".format(len(source_sentences)))\n",
    "    epochs = 4       # Number of Epochs\n",
    "    batch_size = 128 # Batch Size\n",
    "\n",
    "    rnn_size = 512   # RNN Size\n",
    "    num_layers = 2   # Number of Layers\n",
    "    encoding_embedding_size = 512 # Encoding embedding Size\n",
    "    decoding_embedding_size = 512 # Decoding embedding Size\n",
    "    keep_probability = 0.7 # keep probability\n",
    "\n",
    "    learning_rate = 0.001 # Learning Rate\n",
    "    \n",
    "else:\n",
    "    \n",
    "    # We are using the small data\n",
    "    print(\"Using hyperparameters for the small data with {:,} source sentences.\".format(len(source_sentences)))\n",
    "    epochs = 60 # Number of Epochs (normally 60 but reduced to test retraining model)\n",
    "    batch_size = 128 # Batch Size\n",
    "    rnn_size = 50 # RNN Size    \n",
    "    num_layers = 2 # Number of Layers    \n",
    "    encoding_embedding_size = 15 # Embedding Size\n",
    "    decoding_embedding_size = 15 # Embedding Size\n",
    "    keep_probability = 0.7 # keep probability\n",
    "    learning_rate = 0.001 # Learning Rate\n",
    "\n",
    "def get_hyperparameters_message():\n",
    "    message  = \"Batch size: {}\\n\".format(batch_size)\n",
    "    message += \"RNN size  : {}\\n\".format(rnn_size)\n",
    "    message += \"Num layers: {}\\n\".format(num_layers)\n",
    "    message += \"Enc. size : {}\\n\".format(encoding_embedding_size)\n",
    "    message += \"Dec. size : {}\\n\".format(decoding_embedding_size)\n",
    "    message += \"Keep prob.: {}\\n\".format(keep_probability)\n",
    "    message += \"Learn rate: {}\\n\\n\".format(learning_rate)\n",
    "    return message\n",
    "\n",
    "# Write batch_size to file for loading after graph has been saved\n",
    "with open(GRAPH_PARAMETERS, 'w') as file:\n",
    "  file.write('%d' % batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    keep_probability = tf.placeholder(tf.float32,name='keep_prob')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "    target_sequence_length = tf.placeholder(tf.int32, (None,), name='target_sequence_length')\n",
    "    max_target_sequence_length = tf.reduce_max(target_sequence_length, name='max_target_len')\n",
    "    source_sequence_length = tf.placeholder(tf.int32, (None,), name='source_sequence_length')\n",
    "    \n",
    "    return input_data, targets, keep_probability, lr, target_sequence_length, max_target_sequence_length, source_sequence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence to Sequence Model\n",
    "\n",
    "We can now start defining the functions that will build the seq2seq model. We are building it from the bottom up with the following components:\n",
    "\n",
    "    2.1 Encoder\n",
    "        - Embedding\n",
    "        - Encoder cell\n",
    "    2.2 Decoder\n",
    "        1- Process decoder inputs\n",
    "        2- Set up the decoder\n",
    "            - Embedding\n",
    "            - Decoder cell\n",
    "            - Dense output layer\n",
    "            - Training decoder\n",
    "            - Inference decoder\n",
    "    2.3 Seq2seq model connecting the encoder and decoder\n",
    "    2.4 Build the training graph hooking up the model with the \n",
    "        optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Encoder\n",
    "\n",
    "The first bit of the model we'll build is the encoder. Here, we'll embed the input data, construct our encoder, then pass the embedded data to the encoder.\n",
    "\n",
    "- Embed the input data using [`tf.contrib.layers.embed_sequence`](https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embed_sequence)\n",
    "<img src=\"images/embed_sequence.png\" />\n",
    "\n",
    "- Pass the embedded input into a stack of RNNs.  Save the RNN state and ignore the output.\n",
    "<img src=\"images/encoder.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(input_data, rnn_size, num_layers, keep_prob, source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "\n",
    "    # Encoder embedding\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(input_data, source_vocab_size, encoding_embedding_size)\n",
    "\n",
    "    # RNN cell\n",
    "    def make_cell(rnn_size):\n",
    "        enc_cell = tf.contrib.rnn.LSTMCell(rnn_size, initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        enc_cell = tf.contrib.rnn.DropoutWrapper(enc_cell, output_keep_prob=keep_prob)\n",
    "        return enc_cell\n",
    "\n",
    "    enc_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    enc_output, enc_state = tf.nn.dynamic_rnn(enc_cell, enc_embed_input, \n",
    "                                              sequence_length=source_sequence_length, dtype=tf.float32)\n",
    "    \n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Decoder\n",
    "\n",
    "The decoder is probably the most involved part of this model. The following steps are needed to create it:\n",
    "\n",
    "    1- Process decoder inputs\n",
    "    2- Set up the decoder components\n",
    "        - Embedding\n",
    "        - Decoder cell\n",
    "        - Dense output layer\n",
    "        - Training decoder\n",
    "        - Inference decoder\n",
    "\n",
    "\n",
    "### Process Decoder Input\n",
    "\n",
    "\n",
    "In the training process, the target sequences will be used in two different places:\n",
    "\n",
    " 1. Using them to calculate the loss\n",
    " 2. Feeding them to the decoder during training to make the model more robust.\n",
    "\n",
    "Now we need to address the second point. Let's assume our targets look like this in their letter/word form (we're doing this for readibility. At this point in the code, these sequences would be in int form):\n",
    "\n",
    "\n",
    "<img src=\"images/targets_1.png\"/>\n",
    "\n",
    "We need to do a simple transformation on the tensor before feeding it to the decoder:\n",
    "\n",
    "1- We will feed an item of the sequence to the decoder at each time step. Think about the last timestep -- where the decoder outputs the final word in its output. The input to that step is the item before last from the target sequence. The decoder has no use for the last item in the target sequence in this scenario. So we'll need to remove the last item. \n",
    "\n",
    "We do that using tensorflow's tf.strided_slice() method. We hand it the tensor, and the index of where to start and where to end the cutting.\n",
    "\n",
    "<img src=\"images/strided_slice_1.png\"/>\n",
    "\n",
    "2- The first item in each sequence we feed to the decoder has to be GO symbol. So We'll add that to the beginning.\n",
    "\n",
    "\n",
    "<img src=\"images/targets_add_go.png\"/>\n",
    "\n",
    "\n",
    "Now the tensor is ready to be fed to the decoder. It looks like this (if we convert from ints to letters/symbols):\n",
    "\n",
    "<img src=\"images/targets_after_processing_1.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process the input we'll feed to the decoder\n",
    "def process_decoder_input(target_data, vocab_to_int, batch_size):\n",
    "    '''Remove the last word id from each batch and concat the <GO> to the begining of each batch'''\n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Set up the decoder components\n",
    "\n",
    "        - Embedding\n",
    "        - Decoder cell\n",
    "        - Dense output layer\n",
    "        - Training decoder\n",
    "        - Inference decoder\n",
    "\n",
    "#### 1- Embedding\n",
    "Now that we have prepared the inputs to the training decoder, we need to embed them so they can be ready to be passed to the decoder. \n",
    "\n",
    "We'll create an embedding matrix like the following then have tf.nn.embedding_lookup convert our input to its embedded equivalent:\n",
    "<img src=\"images/embeddings.png\" />\n",
    "\n",
    "#### 2- Decoder Cell\n",
    "Then we declare our decoder cell. Just like the encoder, we'll use an tf.contrib.rnn.LSTMCell here as well.\n",
    "\n",
    "We need to declare a decoder for the training process, and a decoder for the inference/prediction process. These two decoders will share their parameters (so that all the weights and biases that are set during the training phase can be used when we deploy the model).\n",
    "\n",
    "First, we'll need to define the type of cell we'll be using for our decoder RNNs. We opted for LSTM.\n",
    "\n",
    "#### 3- Dense output layer\n",
    "Before we move to declaring our decoders, we'll need to create the output layer, which will be a tensorflow.python.layers.core.Dense layer that translates the outputs of the decoder to logits that tell us which element of the decoder vocabulary the decoder is choosing to output at each time step.\n",
    "\n",
    "#### 4- Training decoder\n",
    "Essentially, we'll be creating two decoders which share their parameters. One for training and one for inference. The two are similar in that both created using tf.contrib.seq2seq.**BasicDecoder** and tf.contrib.seq2seq.**dynamic_decode**. They differ, however, in that we feed the the target sequences as inputs to the training decoder at each time step to make it more robust.\n",
    "\n",
    "We can think of the training decoder as looking like this (except that it works with sequences in batches):\n",
    "<img src=\"images/sequence-to-sequence-training-decoder.png\"/>\n",
    "\n",
    "The training decoder **does not** feed the output of each time step to the next. Rather, the inputs to the decoder time steps are the target sequence from the training dataset (the orange letters).\n",
    "\n",
    "#### 5- Inference decoder\n",
    "The inference decoder is the one we'll use when we deploy our model to the wild.\n",
    "\n",
    "<img src=\"images/sequence-to-sequence-inference-decoder.png\"/>\n",
    "\n",
    "We'll hand our encoder hidden state to both the training and inference decoders and have it process its output. TensorFlow handles most of the logic for us. We just have to use the appropriate methods from tf.contrib.seq2seq and supply them with the appropriate inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(target_letter_to_int, decoding_embedding_size, num_layers, rnn_size, keep_prob,\n",
    "                   target_sequence_length, max_target_sequence_length, enc_state, dec_input):\n",
    "    \n",
    "    # 1. Decoder Embedding\n",
    "    target_vocab_size = len(target_letter_to_int)\n",
    "    dec_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, decoding_embedding_size]))\n",
    "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
    "\n",
    "    # 2. Construct the decoder cell\n",
    "    def make_cell(rnn_size):\n",
    "        dec_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.contrib.layers.variance_scaling_initializer())\n",
    "        dec_cell = tf.contrib.rnn.DropoutWrapper(dec_cell, output_keep_prob=keep_prob)\n",
    "        return dec_cell\n",
    "\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "     \n",
    "    # 3. Dense layer to translate the decoder's output at each time \n",
    "    # step into a choice from the target vocabulary\n",
    "    output_layer = Dense(target_vocab_size,\n",
    "                         kernel_initializer = tf.contrib.layers.variance_scaling_initializer())\n",
    "\n",
    "    # 4. Set up a training decoder and an inference decoder\n",
    "    # Training Decoder\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "\n",
    "        # Helper for the training process. Used by BasicDecoder to read inputs.\n",
    "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                            sequence_length=target_sequence_length,\n",
    "                                                            time_major=False)\n",
    "\n",
    "        # Basic decoder\n",
    "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, training_helper, enc_state, output_layer) \n",
    "        \n",
    "        # Perform dynamic decoding using the decoder\n",
    "        training_decoder_output = tf.contrib.seq2seq.dynamic_decode(training_decoder, impute_finished=True, \n",
    "                                                                    maximum_iterations=max_target_sequence_length)[0]\n",
    "\n",
    "    # 5. Inference Decoder\n",
    "    # Reuses the same parameters trained by the training process\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        start_tokens = tf.tile(tf.constant([target_letter_to_int['<GO>']], dtype=tf.int32), \n",
    "                               [batch_size], name='start_tokens')\n",
    "\n",
    "        # Helper for the inference process.\n",
    "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(dec_embeddings, \n",
    "                                                                    start_tokens, \n",
    "                                                                    target_letter_to_int['<EOS>'])\n",
    "\n",
    "        # Basic decoder\n",
    "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell, inference_helper, enc_state, output_layer)\n",
    "        \n",
    "        # Perform dynamic decoding using the decoder\n",
    "        inference_decoder_output = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_target_sequence_length)[0]\n",
    "\n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Seq2seq model \n",
    "Let's now go a step above, and hook up the encoder and decoder using the methods we just declared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, targets, lr, target_sequence_length, max_target_sequence_length, source_sequence_length,\n",
    "                  source_vocab_size, target_vocab_size, enc_embedding_size, dec_embedding_size, rnn_size, num_layers, \n",
    "                  keep_prob):\n",
    "    \n",
    "    # Pass the input data through the encoder. We'll ignore the encoder output, but use the state\n",
    "    _, enc_state = encoding_layer(input_data, \n",
    "                                  rnn_size, \n",
    "                                  num_layers,\n",
    "                                  keep_prob,\n",
    "                                  source_sequence_length,\n",
    "                                  source_vocab_size, \n",
    "                                  encoding_embedding_size)\n",
    "\n",
    "    # Prepare the target sequences we'll feed to the decoder in training mode\n",
    "    dec_input = process_decoder_input(targets, target_letter_to_int, batch_size)\n",
    "    \n",
    "    # Pass encoder state and decoder inputs to the decoders\n",
    "    training_decoder_output, inference_decoder_output = decoding_layer(target_letter_to_int, \n",
    "                                                                       decoding_embedding_size, \n",
    "                                                                       num_layers, \n",
    "                                                                       rnn_size,\n",
    "                                                                       keep_prob,\n",
    "                                                                       target_sequence_length,\n",
    "                                                                       max_target_sequence_length,\n",
    "                                                                       enc_state, \n",
    "                                                                       dec_input) \n",
    "    \n",
    "    return training_decoder_output, inference_decoder_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model outputs *training_decoder_output* and *inference_decoder_output* both contain a 'rnn_output' logits tensor that looks like this:\n",
    "\n",
    "<img src=\"images/logits.png\"/>\n",
    "\n",
    "The logits we get from the training tensor we'll pass to tf.contrib.seq2seq.**sequence_loss()** to calculate the loss and ultimately the gradient.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, keep_prob, lr, target_sequence_length, max_target_sequence_length, source_sequence_length \\\n",
    "    = get_model_inputs()\n",
    "    \n",
    "    # Create the training and inference logits\n",
    "    training_decoder_output, inference_decoder_output = seq2seq_model(input_data, \n",
    "                                                                      targets, \n",
    "                                                                      lr, \n",
    "                                                                      target_sequence_length, \n",
    "                                                                      max_target_sequence_length, \n",
    "                                                                      source_sequence_length,\n",
    "                                                                      len(source_letter_to_int),\n",
    "                                                                      len(target_letter_to_int),\n",
    "                                                                      encoding_embedding_size, \n",
    "                                                                      decoding_embedding_size, \n",
    "                                                                      rnn_size, \n",
    "                                                                      num_layers,\n",
    "                                                                      keep_prob)    \n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_decoder_output.rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_decoder_output.sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss\n",
    "    masks = tf.sequence_mask(target_sequence_length, max_target_sequence_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        \n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, targets, masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "        \n",
    "        # Add variables to collection in order to load them up when retraining a saved graph\n",
    "        tf.add_to_collection(\"cost\", cost)\n",
    "        tf.add_to_collection(\"train_op\", train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Batches\n",
    "\n",
    "There's little processing involved when we retreive the batches. This is a simple example assuming batch_size = 2\n",
    "\n",
    "Target sequences (it's actually in int form, we're showing the characters for clarity):\n",
    "\n",
    "<img src=\"images/source_batch.png\" />\n",
    "\n",
    "Source sequences (also in int, but showing letters for clarity):\n",
    "\n",
    "<img src=\"images/target_batch.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pad_sentence_batch(sentence_batch, pad_int):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [pad_int] * (max_sentence - len(sentence)) for sentence in sentence_batch]\n",
    "\n",
    "def get_batches(targets, sources, batch_size, source_pad_int, target_pad_int):\n",
    "    \"\"\"Batch targets, sources, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(sources)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        sources_batch = sources[start_i:start_i + batch_size]\n",
    "        targets_batch = targets[start_i:start_i + batch_size]\n",
    "        pad_sources_batch = np.array(pad_sentence_batch(sources_batch, source_pad_int))\n",
    "        pad_targets_batch = np.array(pad_sentence_batch(targets_batch, target_pad_int))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_targets_lengths = []\n",
    "        for target in pad_targets_batch:\n",
    "            pad_targets_lengths.append(len(target))\n",
    "        \n",
    "        pad_source_lengths = []\n",
    "        for source in pad_sources_batch:\n",
    "            pad_source_lengths.append(len(source))\n",
    "        \n",
    "        yield pad_targets_batch, pad_sources_batch, pad_targets_lengths, pad_source_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training function\n",
    "We're now ready to train our model. If you run into OOM (out of memory) issues during training, try to decrease the batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split data to training and validation sets\n",
    "train_source = source_letter_ids[batch_size:]\n",
    "train_target = target_letter_ids[batch_size:]\n",
    "valid_source = source_letter_ids[:batch_size]\n",
    "valid_target = target_letter_ids[:batch_size]\n",
    "(valid_targets_batch, valid_sources_batch, valid_targets_lengths, valid_sources_lengths) \\\n",
    "= next(get_batches(valid_target, valid_source, batch_size, \n",
    "                   source_letter_to_int['<PAD>'], target_letter_to_int['<PAD>']))\n",
    "\n",
    "if (len(source_sentences) > 10000):\n",
    "    display_step = 100 # Check training loss after each of this many batches with large data\n",
    "else:\n",
    "    display_step = 20 # Check training loss after each of this many batches with small data\n",
    "\n",
    "def train(epoch_i):\n",
    "    \n",
    "    global train_graph, train_op, cost, input_data, targets, lr\n",
    "    global source_sequence_length, target_sequence_length, keep_prob\n",
    "    \n",
    "    # Test to see if graph already exists\n",
    "    if os.path.exists(checkpoint + \".meta\"):\n",
    "        print(\"Reloading existing graph to continue training.\")\n",
    "        reloading = True    \n",
    "        train_graph = tf.Graph()\n",
    "    else:\n",
    "        print(\"Starting with new graph.\")\n",
    "        reloading = False\n",
    "        with train_graph.as_default():\n",
    "            saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session(graph=train_graph) as sess:    \n",
    "\n",
    "        if reloading:\n",
    "            saver = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "            saver.restore(sess, checkpoint) \n",
    "\n",
    "            # Restore variables\n",
    "            input_data = train_graph.get_tensor_by_name('input:0')\n",
    "            targets = train_graph.get_tensor_by_name('targets:0')\n",
    "            lr = train_graph.get_tensor_by_name('learning_rate:0')\n",
    "            source_sequence_length = train_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "            target_sequence_length = train_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "            keep_prob = train_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "            # Grab the optimizer variables that were added to the collection during build\n",
    "            cost = tf.get_collection(\"cost\")[0]\n",
    "            train_op = tf.get_collection(\"train_op\")[0]\n",
    "\n",
    "        else:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        message = \"\" # Clear message to be sent in body of email\n",
    "        \n",
    "        for batch_i, (targets_batch, sources_batch, targets_lengths, sources_lengths) in enumerate(\n",
    "                get_batches(train_target, train_source, batch_size,\n",
    "                           source_letter_to_int['<PAD>'],\n",
    "                           target_letter_to_int['<PAD>'])):\n",
    "\n",
    "            # Training step\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: sources_batch,\n",
    "                 targets: targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: targets_lengths,\n",
    "                 source_sequence_length: sources_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch = batch_i + 1 # batch_i starts at zero so batch is the batch number\n",
    "            \n",
    "            # Debug message updating us on the status of the training\n",
    "            if (batch % display_step == 0 and batch > 0) or batch == (len(train_source) // batch_size):\n",
    "\n",
    "                # Calculate validation cost\n",
    "                validation_loss = sess.run(\n",
    "                [cost],\n",
    "                {input_data: valid_sources_batch,\n",
    "                 targets: valid_targets_batch,\n",
    "                 lr: learning_rate,\n",
    "                 target_sequence_length: valid_targets_lengths,\n",
    "                 source_sequence_length: valid_sources_lengths,\n",
    "                 keep_prob: 1.0})\n",
    "\n",
    "                line = 'Epoch {:>3}/{} Batch {:>6}/{} Inputs (000) {:>7} - Loss: {:>6.3f} - Validation loss: {:>6.3f}'\\\n",
    "                .format(epoch_i, epochs, batch, len(train_source) // batch_size, \n",
    "                        (((epoch_i - 1) * len(train_source)) + batch_i * batch_size) // 1000, loss, validation_loss[0])\n",
    "                print(line)\n",
    "                message += line + \"\\n\"\n",
    "\n",
    "        # Save model at the end of each epoch\n",
    "        print(\"Saving graph...\")\n",
    "        saver.save(sess, checkpoint)\n",
    "        \n",
    "        return message # return message to be sent in body of email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "**Start here to use a saved and pre-trained graph.** Load the saved graph and compute some preditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded batch_size = 128\n",
      "There are 461,570 validation sentences and 3,606 batches.\n",
      "\n",
      "First 10 sentence:\n",
      "\n",
      "Source --> How could this happen?\" he said, fighting back tears.\n",
      "Target --> How could this happen?\" he said, fighting back tears.\n",
      "\n",
      "Source --> It's going to stay a little sore.\n",
      "Target --> It's going to stay a little sore.\n",
      "\n",
      "Source --> Food is always the best way to discover a country.\n",
      "Target --> Food is always the best way to discover a country.\n",
      "\n",
      "Source --> Elearly not, Mark.\n",
      "Target --> Clearly not, Mark.\n",
      "\n",
      "Source --> 1/2 small red onion,\n",
      "Target --> 1/2 small red onion,\n",
      "\n",
      "Source --> And I've never been the saze since.\n",
      "Target --> And I've never been the same since.\n",
      "\n",
      "Source --> Shared room\n",
      "Target --> Shared room\n",
      "\n",
      "Source --> Freedom for Midnight's Dauggters - NYTimes.com\n",
      "Target --> Freedom for Midnight's Daughters - NYTimes.com\n",
      "\n",
      "Source --> It's certainly very ambitious.\n",
      "Target --> It's certainly very ambitious.\n",
      "\n",
      "Source --> My older brother was living in Sweden.\n",
      "Target --> My older brother was living in Sweden.\n"
     ]
    }
   ],
   "source": [
    "# Read batch_size from file\n",
    "with open(GRAPH_PARAMETERS, 'r') as file:\n",
    "    try:\n",
    "        batch_size = int(file.read())\n",
    "        print(\"Loaded batch_size = {}\".format(batch_size))\n",
    "    except ValueError:\n",
    "        batch_size = 128\n",
    "        print(\"Unable to load batch_size from file so using default 128.\")\n",
    "            \n",
    "if (small):\n",
    "    # There is no validation data for the small set, so just load up the data\n",
    "    print(\"Load up the small data.\")\n",
    "    validation_source_sentences, validation_target_sentences = load_small_data()\n",
    "else:\n",
    "    \n",
    "    # Load the validation set and construct the source sentences\n",
    "    AMOUNT_OF_NOISE = 0.2 / MAX_INPUT_LEN\n",
    "\n",
    "    validation_target_sentences = open(NEWS_FILE_NAME_VALIDATE, encoding=\"utf8\").read().split(\"\\n\")\n",
    "    validation_source_sentences = open(NEWS_FILE_NAME_VALIDATE, encoding=\"utf8\").read().split(\"\\n\")\n",
    "    # Reduce workload by grabbing first batches only\n",
    "    # target_sentences = target_sentences[:5*batch_size]\n",
    "    # source_sentences = source_sentences[:5*batch_size]\n",
    "    \n",
    "    # Add the random noise to the source\n",
    "    for i in range(len(validation_source_sentences)):\n",
    "        validation_source_sentences[i] = add_noise_to_string(validation_source_sentences[i], AMOUNT_OF_NOISE)\n",
    "    \n",
    "print(\"There are {:,d} validation sentences and {:,.0f} batches.\".format(len(validation_source_sentences), \n",
    "                                                                         len(validation_source_sentences)//batch_size))\n",
    "    \n",
    "print('\\nFirst 10 sentence:')\n",
    "for i in range (0, 10):\n",
    "    print(\"\\nSource --> \" + validation_source_sentences[i])\n",
    "    print(\"Target --> \" + validation_target_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def source_to_seq(text, length):\n",
    "#     '''Prepare the text for the model'''\n",
    "# #     sequence_length = 7 # don't understand why set to 7\n",
    "# #     sequence_length = 60\n",
    "#     return [source_letter_to_int.get(word, source_letter_to_int['<UNK>']) for word in text] \\\n",
    "# + [source_letter_to_int['<PAD>']]*(length-len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(source_sentences, target_sentences):\n",
    "    \n",
    "    # Convert sentences to IDs\n",
    "    source_letter_ids, target_letter_ids = produce_letter_ids(source_sentences, target_sentences)\n",
    "\n",
    "    pad = source_letter_to_int[\"<PAD>\"]\n",
    "    eos = source_letter_to_int[\"<EOS>\"]\n",
    "    matches = 0\n",
    "    total = 0\n",
    "    display_step = 10\n",
    "\n",
    "    loaded_graph = tf.Graph()\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "\n",
    "        # Load saved model\n",
    "        loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "        loader.restore(sess, checkpoint)\n",
    "\n",
    "        # Load graph variables\n",
    "        input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "        logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "        source_sequence_length = loaded_graph.get_tensor_by_name('source_sequence_length:0')\n",
    "        target_sequence_length = loaded_graph.get_tensor_by_name('target_sequence_length:0')\n",
    "        keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "        for batch_i,(targets_batch, sources_batch, targets_lengths, sources_lengths) \\\n",
    "        in enumerate(get_batches(target_letter_ids, source_letter_ids, batch_size, \n",
    "                                 source_letter_to_int['<PAD>'], target_letter_to_int['<PAD>'])):\n",
    "\n",
    "            # Multiply by batch_size to match the model's input parameters\n",
    "            answer_logits = sess.run(logits, {input_data: sources_batch, \n",
    "                                              target_sequence_length: targets_lengths, \n",
    "                                              source_sequence_length: sources_lengths,\n",
    "                                              keep_prob: 1.0})\n",
    "\n",
    "            for n in range(batch_size):\n",
    "                answer = \"\".join([target_int_to_letter[i] for i in answer_logits[n] if (i != pad and i != eos)])\n",
    "                target = target_sentences[batch_i * batch_size + n]\n",
    "                total += 1\n",
    "                if (answer == target):\n",
    "                    matches += 1\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Batch {:>6}/{} - Accuracy: {:.1%}'.format(batch_i, \n",
    "                                                                 len(source_sentences)//batch_size, \n",
    "                                                                 matches/total))\n",
    "\n",
    "        print(\"Final accuracy = {:.1%}\\n\".format(matches/total))\n",
    "        \n",
    "        return matches/total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train graph by looping through epochs\n",
    "Compute accuracy after each epoch and return in email"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with new graph.\n",
      "Epoch   1/4 Batch    100/32453 Inputs (000)      12 - Loss:  1.848 - Validation loss:  1.803\n",
      "Epoch   1/4 Batch    200/32453 Inputs (000)      25 - Loss:  1.588 - Validation loss:  1.585\n",
      "Epoch   1/4 Batch    300/32453 Inputs (000)      38 - Loss:  1.526 - Validation loss:  1.478\n",
      "Epoch   1/4 Batch    400/32453 Inputs (000)      51 - Loss:  1.409 - Validation loss:  1.408\n",
      "Epoch   1/4 Batch    500/32453 Inputs (000)      63 - Loss:  1.400 - Validation loss:  1.349\n",
      "Epoch   1/4 Batch    600/32453 Inputs (000)      76 - Loss:  1.341 - Validation loss:  1.304\n",
      "Epoch   1/4 Batch    700/32453 Inputs (000)      89 - Loss:  1.278 - Validation loss:  1.272\n",
      "Epoch   1/4 Batch    800/32453 Inputs (000)     102 - Loss:  1.274 - Validation loss:  1.236\n",
      "Epoch   1/4 Batch    900/32453 Inputs (000)     115 - Loss:  1.216 - Validation loss:  1.212\n",
      "Epoch   1/4 Batch   1000/32453 Inputs (000)     127 - Loss:  1.305 - Validation loss:  1.188\n",
      "Epoch   1/4 Batch   1100/32453 Inputs (000)     140 - Loss:  1.071 - Validation loss:  1.163\n",
      "Epoch   1/4 Batch   1200/32453 Inputs (000)     153 - Loss:  1.173 - Validation loss:  1.141\n",
      "Epoch   1/4 Batch   1300/32453 Inputs (000)     166 - Loss:  1.078 - Validation loss:  1.135\n",
      "Epoch   1/4 Batch   1400/32453 Inputs (000)     179 - Loss:  1.092 - Validation loss:  1.114\n",
      "Epoch   1/4 Batch   1500/32453 Inputs (000)     191 - Loss:  1.084 - Validation loss:  1.091\n",
      "Epoch   1/4 Batch   1600/32453 Inputs (000)     204 - Loss:  0.996 - Validation loss:  1.067\n",
      "Epoch   1/4 Batch   1700/32453 Inputs (000)     217 - Loss:  1.026 - Validation loss:  1.051\n",
      "Epoch   1/4 Batch   1800/32453 Inputs (000)     230 - Loss:  1.004 - Validation loss:  1.031\n",
      "Epoch   1/4 Batch   1900/32453 Inputs (000)     243 - Loss:  0.988 - Validation loss:  1.017\n",
      "Epoch   1/4 Batch   2000/32453 Inputs (000)     255 - Loss:  0.970 - Validation loss:  1.001\n",
      "Epoch   1/4 Batch   2100/32453 Inputs (000)     268 - Loss:  0.920 - Validation loss:  0.986\n",
      "Epoch   1/4 Batch   2200/32453 Inputs (000)     281 - Loss:  0.993 - Validation loss:  0.968\n",
      "Epoch   1/4 Batch   2300/32453 Inputs (000)     294 - Loss:  0.948 - Validation loss:  0.958\n",
      "Epoch   1/4 Batch   2400/32453 Inputs (000)     307 - Loss:  0.973 - Validation loss:  0.940\n",
      "Epoch   1/4 Batch   2500/32453 Inputs (000)     319 - Loss:  0.916 - Validation loss:  0.924\n",
      "Epoch   1/4 Batch   2600/32453 Inputs (000)     332 - Loss:  0.919 - Validation loss:  0.909\n",
      "Epoch   1/4 Batch   2700/32453 Inputs (000)     345 - Loss:  0.921 - Validation loss:  0.894\n",
      "Epoch   1/4 Batch   2800/32453 Inputs (000)     358 - Loss:  0.903 - Validation loss:  0.882\n",
      "Epoch   1/4 Batch   2900/32453 Inputs (000)     371 - Loss:  0.829 - Validation loss:  0.848\n",
      "Epoch   1/4 Batch   3000/32453 Inputs (000)     383 - Loss:  0.893 - Validation loss:  0.829\n",
      "Epoch   1/4 Batch   3100/32453 Inputs (000)     396 - Loss:  0.863 - Validation loss:  0.799\n",
      "Epoch   1/4 Batch   3200/32453 Inputs (000)     409 - Loss:  0.877 - Validation loss:  0.770\n",
      "Epoch   1/4 Batch   3300/32453 Inputs (000)     422 - Loss:  0.839 - Validation loss:  0.742\n",
      "Epoch   1/4 Batch   3400/32453 Inputs (000)     435 - Loss:  0.797 - Validation loss:  0.710\n",
      "Epoch   1/4 Batch   3500/32453 Inputs (000)     447 - Loss:  0.642 - Validation loss:  0.669\n",
      "Epoch   1/4 Batch   3600/32453 Inputs (000)     460 - Loss:  0.688 - Validation loss:  0.636\n",
      "Epoch   1/4 Batch   3700/32453 Inputs (000)     473 - Loss:  0.628 - Validation loss:  0.605\n",
      "Epoch   1/4 Batch   3800/32453 Inputs (000)     486 - Loss:  0.602 - Validation loss:  0.569\n",
      "Epoch   1/4 Batch   3900/32453 Inputs (000)     499 - Loss:  0.599 - Validation loss:  0.556\n",
      "Epoch   1/4 Batch   4000/32453 Inputs (000)     511 - Loss:  0.585 - Validation loss:  0.519\n",
      "Epoch   1/4 Batch   4100/32453 Inputs (000)     524 - Loss:  0.628 - Validation loss:  0.496\n",
      "Epoch   1/4 Batch   4200/32453 Inputs (000)     537 - Loss:  0.554 - Validation loss:  0.474\n",
      "Epoch   1/4 Batch   4300/32453 Inputs (000)     550 - Loss:  0.579 - Validation loss:  0.456\n",
      "Epoch   1/4 Batch   4400/32453 Inputs (000)     563 - Loss:  0.486 - Validation loss:  0.442\n",
      "Epoch   1/4 Batch   4500/32453 Inputs (000)     575 - Loss:  0.530 - Validation loss:  0.414\n",
      "Epoch   1/4 Batch   4600/32453 Inputs (000)     588 - Loss:  0.463 - Validation loss:  0.407\n",
      "Epoch   1/4 Batch   4700/32453 Inputs (000)     601 - Loss:  0.427 - Validation loss:  0.386\n",
      "Epoch   1/4 Batch   4800/32453 Inputs (000)     614 - Loss:  0.423 - Validation loss:  0.383\n",
      "Epoch   1/4 Batch   4900/32453 Inputs (000)     627 - Loss:  0.457 - Validation loss:  0.357\n",
      "Epoch   1/4 Batch   5000/32453 Inputs (000)     639 - Loss:  0.398 - Validation loss:  0.348\n",
      "Epoch   1/4 Batch   5100/32453 Inputs (000)     652 - Loss:  0.404 - Validation loss:  0.343\n",
      "Epoch   1/4 Batch   5200/32453 Inputs (000)     665 - Loss:  0.377 - Validation loss:  0.325\n",
      "Epoch   1/4 Batch   5300/32453 Inputs (000)     678 - Loss:  0.384 - Validation loss:  0.321\n",
      "Epoch   1/4 Batch   5400/32453 Inputs (000)     691 - Loss:  0.315 - Validation loss:  0.312\n",
      "Epoch   1/4 Batch   5500/32453 Inputs (000)     703 - Loss:  0.405 - Validation loss:  0.301\n",
      "Epoch   1/4 Batch   5600/32453 Inputs (000)     716 - Loss:  0.340 - Validation loss:  0.287\n",
      "Epoch   1/4 Batch   5700/32453 Inputs (000)     729 - Loss:  0.319 - Validation loss:  0.284\n",
      "Epoch   1/4 Batch   5800/32453 Inputs (000)     742 - Loss:  0.306 - Validation loss:  0.274\n",
      "Epoch   1/4 Batch   5900/32453 Inputs (000)     755 - Loss:  0.285 - Validation loss:  0.266\n",
      "Epoch   1/4 Batch   6000/32453 Inputs (000)     767 - Loss:  0.313 - Validation loss:  0.271\n",
      "Epoch   1/4 Batch   6100/32453 Inputs (000)     780 - Loss:  0.325 - Validation loss:  0.262\n",
      "Epoch   1/4 Batch   6200/32453 Inputs (000)     793 - Loss:  0.297 - Validation loss:  0.256\n",
      "Epoch   1/4 Batch   6300/32453 Inputs (000)     806 - Loss:  0.314 - Validation loss:  0.252\n",
      "Epoch   1/4 Batch   6400/32453 Inputs (000)     819 - Loss:  0.267 - Validation loss:  0.243\n",
      "Epoch   1/4 Batch   6500/32453 Inputs (000)     831 - Loss:  0.327 - Validation loss:  0.237\n",
      "Epoch   1/4 Batch   6600/32453 Inputs (000)     844 - Loss:  0.306 - Validation loss:  0.232\n",
      "Epoch   1/4 Batch   6700/32453 Inputs (000)     857 - Loss:  0.312 - Validation loss:  0.232\n",
      "Epoch   1/4 Batch   6800/32453 Inputs (000)     870 - Loss:  0.298 - Validation loss:  0.222\n",
      "Epoch   1/4 Batch   6900/32453 Inputs (000)     883 - Loss:  0.276 - Validation loss:  0.218\n",
      "Epoch   1/4 Batch   7000/32453 Inputs (000)     895 - Loss:  0.256 - Validation loss:  0.220\n",
      "Epoch   1/4 Batch   7100/32453 Inputs (000)     908 - Loss:  0.285 - Validation loss:  0.210\n",
      "Epoch   1/4 Batch   7200/32453 Inputs (000)     921 - Loss:  0.249 - Validation loss:  0.221\n",
      "Epoch   1/4 Batch   7300/32453 Inputs (000)     934 - Loss:  0.304 - Validation loss:  0.205\n",
      "Epoch   1/4 Batch   7400/32453 Inputs (000)     947 - Loss:  0.232 - Validation loss:  0.201\n",
      "Epoch   1/4 Batch   7500/32453 Inputs (000)     959 - Loss:  0.256 - Validation loss:  0.203\n",
      "Epoch   1/4 Batch   7600/32453 Inputs (000)     972 - Loss:  0.289 - Validation loss:  0.193\n",
      "Epoch   1/4 Batch   7700/32453 Inputs (000)     985 - Loss:  0.252 - Validation loss:  0.193\n",
      "Epoch   1/4 Batch   7800/32453 Inputs (000)     998 - Loss:  0.255 - Validation loss:  0.191\n",
      "Epoch   1/4 Batch   7900/32453 Inputs (000)    1011 - Loss:  0.223 - Validation loss:  0.186\n",
      "Epoch   1/4 Batch   8000/32453 Inputs (000)    1023 - Loss:  0.239 - Validation loss:  0.189\n",
      "Epoch   1/4 Batch   8100/32453 Inputs (000)    1036 - Loss:  0.285 - Validation loss:  0.187\n",
      "Epoch   1/4 Batch   8200/32453 Inputs (000)    1049 - Loss:  0.230 - Validation loss:  0.180\n",
      "Epoch   1/4 Batch   8300/32453 Inputs (000)    1062 - Loss:  0.249 - Validation loss:  0.175\n",
      "Epoch   1/4 Batch   8400/32453 Inputs (000)    1075 - Loss:  0.216 - Validation loss:  0.174\n",
      "Epoch   1/4 Batch   8500/32453 Inputs (000)    1087 - Loss:  0.212 - Validation loss:  0.170\n",
      "Epoch   1/4 Batch   8600/32453 Inputs (000)    1100 - Loss:  0.244 - Validation loss:  0.188\n",
      "Epoch   1/4 Batch   8700/32453 Inputs (000)    1113 - Loss:  0.234 - Validation loss:  0.168\n",
      "Epoch   1/4 Batch   8800/32453 Inputs (000)    1126 - Loss:  0.226 - Validation loss:  0.172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/4 Batch   8900/32453 Inputs (000)    1139 - Loss:  0.224 - Validation loss:  0.167\n",
      "Epoch   1/4 Batch   9000/32453 Inputs (000)    1151 - Loss:  0.191 - Validation loss:  0.168\n",
      "Epoch   1/4 Batch   9100/32453 Inputs (000)    1164 - Loss:  0.219 - Validation loss:  0.164\n",
      "Epoch   1/4 Batch   9200/32453 Inputs (000)    1177 - Loss:  0.240 - Validation loss:  0.159\n",
      "Epoch   1/4 Batch   9300/32453 Inputs (000)    1190 - Loss:  0.214 - Validation loss:  0.154\n",
      "Epoch   1/4 Batch   9400/32453 Inputs (000)    1203 - Loss:  0.234 - Validation loss:  0.154\n",
      "Epoch   1/4 Batch   9500/32453 Inputs (000)    1215 - Loss:  0.232 - Validation loss:  0.151\n",
      "Epoch   1/4 Batch   9600/32453 Inputs (000)    1228 - Loss:  0.215 - Validation loss:  0.153\n",
      "Epoch   1/4 Batch   9700/32453 Inputs (000)    1241 - Loss:  0.182 - Validation loss:  0.149\n",
      "Epoch   1/4 Batch   9800/32453 Inputs (000)    1254 - Loss:  0.190 - Validation loss:  0.153\n",
      "Epoch   1/4 Batch   9900/32453 Inputs (000)    1267 - Loss:  0.171 - Validation loss:  0.148\n",
      "Epoch   1/4 Batch  10000/32453 Inputs (000)    1279 - Loss:  0.183 - Validation loss:  0.143\n",
      "Epoch   1/4 Batch  10100/32453 Inputs (000)    1292 - Loss:  0.181 - Validation loss:  0.147\n",
      "Epoch   1/4 Batch  10200/32453 Inputs (000)    1305 - Loss:  0.216 - Validation loss:  0.147\n",
      "Epoch   1/4 Batch  10300/32453 Inputs (000)    1318 - Loss:  0.213 - Validation loss:  0.141\n",
      "Epoch   1/4 Batch  10400/32453 Inputs (000)    1331 - Loss:  0.197 - Validation loss:  0.140\n",
      "Epoch   1/4 Batch  10500/32453 Inputs (000)    1343 - Loss:  0.195 - Validation loss:  0.141\n",
      "Epoch   1/4 Batch  10600/32453 Inputs (000)    1356 - Loss:  0.201 - Validation loss:  0.143\n",
      "Epoch   1/4 Batch  10700/32453 Inputs (000)    1369 - Loss:  0.217 - Validation loss:  0.139\n",
      "Epoch   1/4 Batch  10800/32453 Inputs (000)    1382 - Loss:  0.191 - Validation loss:  0.136\n",
      "Epoch   1/4 Batch  10900/32453 Inputs (000)    1395 - Loss:  0.168 - Validation loss:  0.136\n",
      "Epoch   1/4 Batch  11000/32453 Inputs (000)    1407 - Loss:  0.191 - Validation loss:  0.134\n",
      "Epoch   1/4 Batch  11100/32453 Inputs (000)    1420 - Loss:  0.185 - Validation loss:  0.132\n",
      "Epoch   1/4 Batch  11200/32453 Inputs (000)    1433 - Loss:  0.207 - Validation loss:  0.133\n",
      "Epoch   1/4 Batch  11300/32453 Inputs (000)    1446 - Loss:  0.203 - Validation loss:  0.139\n",
      "Epoch   1/4 Batch  11400/32453 Inputs (000)    1459 - Loss:  0.180 - Validation loss:  0.134\n",
      "Epoch   1/4 Batch  11500/32453 Inputs (000)    1471 - Loss:  0.168 - Validation loss:  0.135\n",
      "Epoch   1/4 Batch  11600/32453 Inputs (000)    1484 - Loss:  0.183 - Validation loss:  0.126\n",
      "Epoch   1/4 Batch  11700/32453 Inputs (000)    1497 - Loss:  0.162 - Validation loss:  0.128\n",
      "Epoch   1/4 Batch  11800/32453 Inputs (000)    1510 - Loss:  0.150 - Validation loss:  0.130\n",
      "Epoch   1/4 Batch  11900/32453 Inputs (000)    1523 - Loss:  0.187 - Validation loss:  0.130\n",
      "Epoch   1/4 Batch  12000/32453 Inputs (000)    1535 - Loss:  0.192 - Validation loss:  0.122\n",
      "Epoch   1/4 Batch  12100/32453 Inputs (000)    1548 - Loss:  0.169 - Validation loss:  0.130\n",
      "Epoch   1/4 Batch  12200/32453 Inputs (000)    1561 - Loss:  0.143 - Validation loss:  0.126\n",
      "Epoch   1/4 Batch  12300/32453 Inputs (000)    1574 - Loss:  0.142 - Validation loss:  0.123\n",
      "Epoch   1/4 Batch  12400/32453 Inputs (000)    1587 - Loss:  0.166 - Validation loss:  0.123\n",
      "Epoch   1/4 Batch  12500/32453 Inputs (000)    1599 - Loss:  0.173 - Validation loss:  0.126\n",
      "Epoch   1/4 Batch  12600/32453 Inputs (000)    1612 - Loss:  0.174 - Validation loss:  0.115\n",
      "Epoch   1/4 Batch  12700/32453 Inputs (000)    1625 - Loss:  0.168 - Validation loss:  0.122\n",
      "Epoch   1/4 Batch  12800/32453 Inputs (000)    1638 - Loss:  0.159 - Validation loss:  0.136\n",
      "Epoch   1/4 Batch  12900/32453 Inputs (000)    1651 - Loss:  0.154 - Validation loss:  0.121\n",
      "Epoch   1/4 Batch  13000/32453 Inputs (000)    1663 - Loss:  0.141 - Validation loss:  0.112\n",
      "Epoch   1/4 Batch  13100/32453 Inputs (000)    1676 - Loss:  0.142 - Validation loss:  0.122\n",
      "Epoch   1/4 Batch  13200/32453 Inputs (000)    1689 - Loss:  0.169 - Validation loss:  0.114\n",
      "Epoch   1/4 Batch  13300/32453 Inputs (000)    1702 - Loss:  0.155 - Validation loss:  0.119\n",
      "Epoch   1/4 Batch  13400/32453 Inputs (000)    1715 - Loss:  0.127 - Validation loss:  0.117\n",
      "Epoch   1/4 Batch  13500/32453 Inputs (000)    1727 - Loss:  0.169 - Validation loss:  0.114\n",
      "Epoch   1/4 Batch  13600/32453 Inputs (000)    1740 - Loss:  0.144 - Validation loss:  0.115\n",
      "Epoch   1/4 Batch  13700/32453 Inputs (000)    1753 - Loss:  0.159 - Validation loss:  0.117\n",
      "Epoch   1/4 Batch  13800/32453 Inputs (000)    1766 - Loss:  0.158 - Validation loss:  0.116\n",
      "Epoch   1/4 Batch  13900/32453 Inputs (000)    1779 - Loss:  0.130 - Validation loss:  0.111\n",
      "Epoch   1/4 Batch  14000/32453 Inputs (000)    1791 - Loss:  0.160 - Validation loss:  0.109\n",
      "Epoch   1/4 Batch  14100/32453 Inputs (000)    1804 - Loss:  0.161 - Validation loss:  0.115\n",
      "Epoch   1/4 Batch  14200/32453 Inputs (000)    1817 - Loss:  0.159 - Validation loss:  0.112\n",
      "Epoch   1/4 Batch  14300/32453 Inputs (000)    1830 - Loss:  0.161 - Validation loss:  0.108\n",
      "Epoch   1/4 Batch  14400/32453 Inputs (000)    1843 - Loss:  0.176 - Validation loss:  0.106\n",
      "Epoch   1/4 Batch  14500/32453 Inputs (000)    1855 - Loss:  0.150 - Validation loss:  0.109\n",
      "Epoch   1/4 Batch  14600/32453 Inputs (000)    1868 - Loss:  0.132 - Validation loss:  0.106\n",
      "Epoch   1/4 Batch  14700/32453 Inputs (000)    1881 - Loss:  0.138 - Validation loss:  0.104\n",
      "Epoch   1/4 Batch  14800/32453 Inputs (000)    1894 - Loss:  0.128 - Validation loss:  0.106\n",
      "Epoch   1/4 Batch  14900/32453 Inputs (000)    1907 - Loss:  0.154 - Validation loss:  0.104\n",
      "Epoch   1/4 Batch  15000/32453 Inputs (000)    1919 - Loss:  0.144 - Validation loss:  0.103\n",
      "Epoch   1/4 Batch  15100/32453 Inputs (000)    1932 - Loss:  0.144 - Validation loss:  0.101\n",
      "Epoch   1/4 Batch  15200/32453 Inputs (000)    1945 - Loss:  0.155 - Validation loss:  0.103\n",
      "Epoch   1/4 Batch  15300/32453 Inputs (000)    1958 - Loss:  0.156 - Validation loss:  0.104\n",
      "Epoch   1/4 Batch  15400/32453 Inputs (000)    1971 - Loss:  0.135 - Validation loss:  0.109\n",
      "Epoch   1/4 Batch  15500/32453 Inputs (000)    1983 - Loss:  0.135 - Validation loss:  0.100\n",
      "Epoch   1/4 Batch  15600/32453 Inputs (000)    1996 - Loss:  0.130 - Validation loss:  0.099\n",
      "Epoch   1/4 Batch  15700/32453 Inputs (000)    2009 - Loss:  0.149 - Validation loss:  0.102\n",
      "Epoch   1/4 Batch  15800/32453 Inputs (000)    2022 - Loss:  0.127 - Validation loss:  0.102\n",
      "Epoch   1/4 Batch  15900/32453 Inputs (000)    2035 - Loss:  0.126 - Validation loss:  0.099\n",
      "Epoch   1/4 Batch  16000/32453 Inputs (000)    2047 - Loss:  0.146 - Validation loss:  0.103\n",
      "Epoch   1/4 Batch  16100/32453 Inputs (000)    2060 - Loss:  0.109 - Validation loss:  0.102\n",
      "Epoch   1/4 Batch  16200/32453 Inputs (000)    2073 - Loss:  0.134 - Validation loss:  0.098\n",
      "Epoch   1/4 Batch  16300/32453 Inputs (000)    2086 - Loss:  0.136 - Validation loss:  0.101\n",
      "Epoch   1/4 Batch  16400/32453 Inputs (000)    2099 - Loss:  0.138 - Validation loss:  0.102\n",
      "Epoch   1/4 Batch  16500/32453 Inputs (000)    2111 - Loss:  0.147 - Validation loss:  0.094\n",
      "Epoch   1/4 Batch  16600/32453 Inputs (000)    2124 - Loss:  0.117 - Validation loss:  0.098\n",
      "Epoch   1/4 Batch  16700/32453 Inputs (000)    2137 - Loss:  0.132 - Validation loss:  0.094\n",
      "Epoch   1/4 Batch  16800/32453 Inputs (000)    2150 - Loss:  0.115 - Validation loss:  0.101\n",
      "Epoch   1/4 Batch  16900/32453 Inputs (000)    2163 - Loss:  0.156 - Validation loss:  0.094\n",
      "Epoch   1/4 Batch  17000/32453 Inputs (000)    2175 - Loss:  0.128 - Validation loss:  0.098\n",
      "Epoch   1/4 Batch  17100/32453 Inputs (000)    2188 - Loss:  0.139 - Validation loss:  0.094\n",
      "Epoch   1/4 Batch  17200/32453 Inputs (000)    2201 - Loss:  0.112 - Validation loss:  0.095\n",
      "Epoch   1/4 Batch  17300/32453 Inputs (000)    2214 - Loss:  0.110 - Validation loss:  0.095\n",
      "Epoch   1/4 Batch  17400/32453 Inputs (000)    2227 - Loss:  0.122 - Validation loss:  0.090\n",
      "Epoch   1/4 Batch  17500/32453 Inputs (000)    2239 - Loss:  0.132 - Validation loss:  0.092\n",
      "Epoch   1/4 Batch  17600/32453 Inputs (000)    2252 - Loss:  0.143 - Validation loss:  0.098\n",
      "Epoch   1/4 Batch  17700/32453 Inputs (000)    2265 - Loss:  0.110 - Validation loss:  0.091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/4 Batch  17800/32453 Inputs (000)    2278 - Loss:  0.125 - Validation loss:  0.106\n",
      "Epoch   1/4 Batch  17900/32453 Inputs (000)    2291 - Loss:  0.135 - Validation loss:  0.094\n",
      "Epoch   1/4 Batch  18000/32453 Inputs (000)    2303 - Loss:  0.202 - Validation loss:  0.130\n",
      "Epoch   1/4 Batch  18100/32453 Inputs (000)    2316 - Loss:  0.178 - Validation loss:  0.097\n",
      "Epoch   1/4 Batch  18200/32453 Inputs (000)    2329 - Loss:  0.136 - Validation loss:  0.098\n",
      "Epoch   1/4 Batch  18300/32453 Inputs (000)    2342 - Loss:  0.121 - Validation loss:  0.096\n",
      "Epoch   1/4 Batch  18400/32453 Inputs (000)    2355 - Loss:  0.098 - Validation loss:  0.093\n",
      "Epoch   1/4 Batch  18500/32453 Inputs (000)    2367 - Loss:  0.121 - Validation loss:  0.093\n",
      "Epoch   1/4 Batch  18600/32453 Inputs (000)    2380 - Loss:  0.128 - Validation loss:  0.093\n",
      "Epoch   1/4 Batch  18700/32453 Inputs (000)    2393 - Loss:  0.138 - Validation loss:  0.090\n",
      "Epoch   1/4 Batch  18800/32453 Inputs (000)    2406 - Loss:  0.108 - Validation loss:  0.090\n",
      "Epoch   1/4 Batch  18900/32453 Inputs (000)    2419 - Loss:  0.118 - Validation loss:  0.088\n",
      "Epoch   1/4 Batch  19000/32453 Inputs (000)    2431 - Loss:  0.110 - Validation loss:  0.088\n",
      "Epoch   1/4 Batch  19100/32453 Inputs (000)    2444 - Loss:  0.106 - Validation loss:  0.087\n",
      "Epoch   1/4 Batch  19200/32453 Inputs (000)    2457 - Loss:  0.136 - Validation loss:  0.087\n",
      "Epoch   1/4 Batch  19300/32453 Inputs (000)    2470 - Loss:  0.105 - Validation loss:  0.087\n",
      "Epoch   1/4 Batch  19400/32453 Inputs (000)    2483 - Loss:  0.111 - Validation loss:  0.089\n",
      "Epoch   1/4 Batch  19500/32453 Inputs (000)    2495 - Loss:  0.112 - Validation loss:  0.084\n",
      "Epoch   1/4 Batch  19600/32453 Inputs (000)    2508 - Loss:  0.125 - Validation loss:  0.086\n",
      "Epoch   1/4 Batch  19700/32453 Inputs (000)    2521 - Loss:  0.118 - Validation loss:  0.087\n",
      "Epoch   1/4 Batch  19800/32453 Inputs (000)    2534 - Loss:  0.125 - Validation loss:  0.089\n",
      "Epoch   1/4 Batch  19900/32453 Inputs (000)    2547 - Loss:  0.111 - Validation loss:  0.090\n",
      "Epoch   1/4 Batch  20000/32453 Inputs (000)    2559 - Loss:  0.155 - Validation loss:  0.087\n",
      "Epoch   1/4 Batch  20100/32453 Inputs (000)    2572 - Loss:  0.121 - Validation loss:  0.087\n",
      "Epoch   1/4 Batch  20200/32453 Inputs (000)    2585 - Loss:  0.106 - Validation loss:  0.088\n",
      "Epoch   1/4 Batch  20300/32453 Inputs (000)    2598 - Loss:  0.133 - Validation loss:  0.090\n",
      "Epoch   1/4 Batch  20400/32453 Inputs (000)    2611 - Loss:  0.089 - Validation loss:  0.089\n",
      "Epoch   1/4 Batch  20500/32453 Inputs (000)    2623 - Loss:  0.091 - Validation loss:  0.090\n",
      "Epoch   1/4 Batch  20600/32453 Inputs (000)    2636 - Loss:  0.106 - Validation loss:  0.087\n",
      "Epoch   1/4 Batch  20700/32453 Inputs (000)    2649 - Loss:  0.126 - Validation loss:  0.088\n",
      "Epoch   1/4 Batch  20800/32453 Inputs (000)    2662 - Loss:  0.094 - Validation loss:  0.088\n",
      "Epoch   1/4 Batch  20900/32453 Inputs (000)    2675 - Loss:  0.117 - Validation loss:  0.084\n",
      "Epoch   1/4 Batch  21000/32453 Inputs (000)    2687 - Loss:  0.108 - Validation loss:  0.087\n",
      "Epoch   1/4 Batch  21100/32453 Inputs (000)    2700 - Loss:  0.133 - Validation loss:  0.088\n",
      "Epoch   1/4 Batch  21200/32453 Inputs (000)    2713 - Loss:  0.114 - Validation loss:  0.083\n",
      "Epoch   1/4 Batch  21300/32453 Inputs (000)    2726 - Loss:  0.103 - Validation loss:  0.081\n",
      "Epoch   1/4 Batch  21400/32453 Inputs (000)    2739 - Loss:  0.108 - Validation loss:  0.083\n",
      "Epoch   1/4 Batch  21500/32453 Inputs (000)    2751 - Loss:  0.120 - Validation loss:  0.079\n",
      "Epoch   1/4 Batch  21600/32453 Inputs (000)    2764 - Loss:  0.134 - Validation loss:  0.089\n",
      "Epoch   1/4 Batch  21700/32453 Inputs (000)    2777 - Loss:  0.131 - Validation loss:  0.085\n",
      "Epoch   1/4 Batch  21800/32453 Inputs (000)    2790 - Loss:  0.105 - Validation loss:  0.089\n",
      "Epoch   1/4 Batch  21900/32453 Inputs (000)    2803 - Loss:  0.112 - Validation loss:  0.083\n",
      "Epoch   1/4 Batch  22000/32453 Inputs (000)    2815 - Loss:  0.100 - Validation loss:  0.083\n",
      "Epoch   1/4 Batch  22100/32453 Inputs (000)    2828 - Loss:  0.098 - Validation loss:  0.083\n",
      "Epoch   1/4 Batch  22200/32453 Inputs (000)    2841 - Loss:  0.112 - Validation loss:  0.084\n",
      "Epoch   1/4 Batch  22300/32453 Inputs (000)    2854 - Loss:  0.131 - Validation loss:  0.084\n",
      "Epoch   1/4 Batch  22400/32453 Inputs (000)    2867 - Loss:  0.123 - Validation loss:  0.081\n",
      "Epoch   1/4 Batch  22500/32453 Inputs (000)    2879 - Loss:  0.109 - Validation loss:  0.082\n",
      "Epoch   1/4 Batch  22600/32453 Inputs (000)    2892 - Loss:  0.113 - Validation loss:  0.081\n",
      "Epoch   1/4 Batch  22700/32453 Inputs (000)    2905 - Loss:  0.102 - Validation loss:  0.081\n",
      "Epoch   1/4 Batch  22800/32453 Inputs (000)    2918 - Loss:  0.112 - Validation loss:  0.079\n",
      "Epoch   1/4 Batch  22900/32453 Inputs (000)    2931 - Loss:  0.096 - Validation loss:  0.082\n",
      "Epoch   1/4 Batch  23000/32453 Inputs (000)    2943 - Loss:  0.122 - Validation loss:  0.078\n",
      "Epoch   1/4 Batch  23100/32453 Inputs (000)    2956 - Loss:  0.102 - Validation loss:  0.083\n",
      "Epoch   1/4 Batch  23200/32453 Inputs (000)    2969 - Loss:  0.122 - Validation loss:  0.080\n",
      "Epoch   1/4 Batch  23300/32453 Inputs (000)    2982 - Loss:  0.110 - Validation loss:  0.075\n",
      "Epoch   1/4 Batch  23400/32453 Inputs (000)    2995 - Loss:  0.127 - Validation loss:  0.083\n",
      "Epoch   1/4 Batch  23500/32453 Inputs (000)    3007 - Loss:  0.098 - Validation loss:  0.081\n",
      "Epoch   1/4 Batch  23600/32453 Inputs (000)    3020 - Loss:  0.087 - Validation loss:  0.081\n",
      "Epoch   1/4 Batch  23700/32453 Inputs (000)    3033 - Loss:  0.118 - Validation loss:  0.077\n",
      "Epoch   1/4 Batch  23800/32453 Inputs (000)    3046 - Loss:  0.113 - Validation loss:  0.077\n",
      "Epoch   1/4 Batch  23900/32453 Inputs (000)    3059 - Loss:  0.103 - Validation loss:  0.079\n",
      "Epoch   1/4 Batch  24000/32453 Inputs (000)    3071 - Loss:  0.115 - Validation loss:  0.081\n",
      "Epoch   1/4 Batch  24100/32453 Inputs (000)    3084 - Loss:  0.099 - Validation loss:  0.077\n",
      "Epoch   1/4 Batch  24200/32453 Inputs (000)    3097 - Loss:  0.102 - Validation loss:  0.077\n",
      "Epoch   1/4 Batch  24300/32453 Inputs (000)    3110 - Loss:  0.088 - Validation loss:  0.079\n",
      "Epoch   1/4 Batch  24400/32453 Inputs (000)    3123 - Loss:  0.121 - Validation loss:  0.076\n",
      "Epoch   1/4 Batch  24500/32453 Inputs (000)    3135 - Loss:  0.117 - Validation loss:  0.074\n",
      "Epoch   1/4 Batch  24600/32453 Inputs (000)    3148 - Loss:  0.095 - Validation loss:  0.075\n",
      "Epoch   1/4 Batch  24700/32453 Inputs (000)    3161 - Loss:  0.119 - Validation loss:  0.078\n",
      "Epoch   1/4 Batch  24800/32453 Inputs (000)    3174 - Loss:  0.085 - Validation loss:  0.080\n",
      "Epoch   1/4 Batch  24900/32453 Inputs (000)    3187 - Loss:  0.126 - Validation loss:  0.090\n",
      "Epoch   1/4 Batch  25000/32453 Inputs (000)    3199 - Loss:  0.100 - Validation loss:  0.075\n",
      "Epoch   1/4 Batch  25100/32453 Inputs (000)    3212 - Loss:  0.103 - Validation loss:  0.077\n",
      "Epoch   1/4 Batch  25200/32453 Inputs (000)    3225 - Loss:  0.095 - Validation loss:  0.072\n",
      "Epoch   1/4 Batch  25300/32453 Inputs (000)    3238 - Loss:  0.102 - Validation loss:  0.074\n",
      "Epoch   1/4 Batch  25400/32453 Inputs (000)    3251 - Loss:  0.107 - Validation loss:  0.071\n",
      "Epoch   1/4 Batch  25500/32453 Inputs (000)    3263 - Loss:  0.099 - Validation loss:  0.075\n",
      "Epoch   1/4 Batch  25600/32453 Inputs (000)    3276 - Loss:  0.083 - Validation loss:  0.072\n",
      "Epoch   1/4 Batch  25700/32453 Inputs (000)    3289 - Loss:  0.118 - Validation loss:  0.072\n",
      "Epoch   1/4 Batch  25800/32453 Inputs (000)    3302 - Loss:  0.096 - Validation loss:  0.073\n",
      "Epoch   1/4 Batch  25900/32453 Inputs (000)    3315 - Loss:  0.101 - Validation loss:  0.075\n",
      "Epoch   1/4 Batch  26000/32453 Inputs (000)    3327 - Loss:  0.104 - Validation loss:  0.072\n",
      "Epoch   1/4 Batch  26100/32453 Inputs (000)    3340 - Loss:  0.092 - Validation loss:  0.079\n",
      "Epoch   1/4 Batch  26200/32453 Inputs (000)    3353 - Loss:  0.091 - Validation loss:  0.073\n",
      "Epoch   1/4 Batch  26300/32453 Inputs (000)    3366 - Loss:  0.144 - Validation loss:  0.112\n",
      "Epoch   1/4 Batch  26400/32453 Inputs (000)    3379 - Loss:  0.118 - Validation loss:  0.083\n",
      "Epoch   1/4 Batch  26500/32453 Inputs (000)    3391 - Loss:  0.085 - Validation loss:  0.076\n",
      "Epoch   1/4 Batch  26600/32453 Inputs (000)    3404 - Loss:  0.094 - Validation loss:  0.076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/4 Batch  26700/32453 Inputs (000)    3417 - Loss:  0.095 - Validation loss:  0.073\n",
      "Epoch   1/4 Batch  26800/32453 Inputs (000)    3430 - Loss:  0.122 - Validation loss:  0.073\n",
      "Epoch   1/4 Batch  26900/32453 Inputs (000)    3443 - Loss:  0.092 - Validation loss:  0.074\n",
      "Epoch   1/4 Batch  27000/32453 Inputs (000)    3455 - Loss:  0.107 - Validation loss:  0.078\n",
      "Epoch   1/4 Batch  27100/32453 Inputs (000)    3468 - Loss:  0.088 - Validation loss:  0.072\n",
      "Epoch   1/4 Batch  27200/32453 Inputs (000)    3481 - Loss:  0.107 - Validation loss:  0.083\n",
      "Epoch   1/4 Batch  27300/32453 Inputs (000)    3494 - Loss:  0.098 - Validation loss:  0.071\n",
      "Epoch   1/4 Batch  27400/32453 Inputs (000)    3507 - Loss:  0.096 - Validation loss:  0.072\n",
      "Epoch   1/4 Batch  27500/32453 Inputs (000)    3519 - Loss:  0.106 - Validation loss:  0.074\n",
      "Epoch   1/4 Batch  27600/32453 Inputs (000)    3532 - Loss:  0.088 - Validation loss:  0.071\n",
      "Epoch   1/4 Batch  27700/32453 Inputs (000)    3545 - Loss:  0.101 - Validation loss:  0.081\n",
      "Epoch   1/4 Batch  27800/32453 Inputs (000)    3558 - Loss:  0.096 - Validation loss:  0.078\n",
      "Epoch   1/4 Batch  27900/32453 Inputs (000)    3571 - Loss:  0.107 - Validation loss:  0.072\n",
      "Epoch   1/4 Batch  28000/32453 Inputs (000)    3583 - Loss:  0.105 - Validation loss:  0.070\n",
      "Epoch   1/4 Batch  28100/32453 Inputs (000)    3596 - Loss:  0.091 - Validation loss:  0.071\n",
      "Epoch   1/4 Batch  28200/32453 Inputs (000)    3609 - Loss:  0.089 - Validation loss:  0.072\n",
      "Epoch   1/4 Batch  28300/32453 Inputs (000)    3622 - Loss:  0.108 - Validation loss:  0.072\n",
      "Epoch   1/4 Batch  28400/32453 Inputs (000)    3635 - Loss:  0.087 - Validation loss:  0.070\n",
      "Epoch   1/4 Batch  28500/32453 Inputs (000)    3647 - Loss:  0.098 - Validation loss:  0.071\n",
      "Epoch   1/4 Batch  28600/32453 Inputs (000)    3660 - Loss:  0.082 - Validation loss:  0.071\n",
      "Epoch   1/4 Batch  28700/32453 Inputs (000)    3673 - Loss:  0.099 - Validation loss:  0.069\n",
      "Epoch   1/4 Batch  28800/32453 Inputs (000)    3686 - Loss:  0.123 - Validation loss:  0.075\n",
      "Epoch   1/4 Batch  28900/32453 Inputs (000)    3699 - Loss:  0.108 - Validation loss:  0.078\n",
      "Epoch   1/4 Batch  29000/32453 Inputs (000)    3711 - Loss:  0.113 - Validation loss:  0.081\n",
      "Epoch   1/4 Batch  29100/32453 Inputs (000)    3724 - Loss:  0.091 - Validation loss:  0.070\n",
      "Epoch   1/4 Batch  29200/32453 Inputs (000)    3737 - Loss:  0.109 - Validation loss:  0.074\n",
      "Epoch   1/4 Batch  29300/32453 Inputs (000)    3750 - Loss:  0.111 - Validation loss:  0.070\n",
      "Epoch   1/4 Batch  29400/32453 Inputs (000)    3763 - Loss:  0.100 - Validation loss:  0.072\n",
      "Epoch   1/4 Batch  29500/32453 Inputs (000)    3775 - Loss:  0.100 - Validation loss:  0.067\n",
      "Epoch   1/4 Batch  29600/32453 Inputs (000)    3788 - Loss:  0.086 - Validation loss:  0.068\n",
      "Epoch   1/4 Batch  29700/32453 Inputs (000)    3801 - Loss:  0.101 - Validation loss:  0.067\n",
      "Epoch   1/4 Batch  29800/32453 Inputs (000)    3814 - Loss:  0.104 - Validation loss:  0.069\n",
      "Epoch   1/4 Batch  29900/32453 Inputs (000)    3827 - Loss:  0.111 - Validation loss:  0.066\n",
      "Epoch   1/4 Batch  30000/32453 Inputs (000)    3839 - Loss:  0.090 - Validation loss:  0.066\n",
      "Epoch   1/4 Batch  30100/32453 Inputs (000)    3852 - Loss:  0.077 - Validation loss:  0.070\n",
      "Epoch   1/4 Batch  30200/32453 Inputs (000)    3865 - Loss:  0.106 - Validation loss:  0.065\n",
      "Epoch   1/4 Batch  30300/32453 Inputs (000)    3878 - Loss:  0.087 - Validation loss:  0.067\n",
      "Epoch   1/4 Batch  30400/32453 Inputs (000)    3891 - Loss:  0.097 - Validation loss:  0.064\n",
      "Epoch   1/4 Batch  30500/32453 Inputs (000)    3903 - Loss:  0.109 - Validation loss:  0.064\n",
      "Epoch   1/4 Batch  30600/32453 Inputs (000)    3916 - Loss:  0.083 - Validation loss:  0.066\n",
      "Epoch   1/4 Batch  30700/32453 Inputs (000)    3929 - Loss:  0.104 - Validation loss:  0.066\n",
      "Epoch   1/4 Batch  30800/32453 Inputs (000)    3942 - Loss:  0.088 - Validation loss:  0.064\n",
      "Epoch   1/4 Batch  30900/32453 Inputs (000)    3955 - Loss:  0.094 - Validation loss:  0.063\n",
      "Epoch   1/4 Batch  31000/32453 Inputs (000)    3967 - Loss:  0.081 - Validation loss:  0.063\n",
      "Epoch   1/4 Batch  31100/32453 Inputs (000)    3980 - Loss:  0.083 - Validation loss:  0.062\n",
      "Epoch   1/4 Batch  31200/32453 Inputs (000)    3993 - Loss:  0.082 - Validation loss:  0.064\n",
      "Epoch   1/4 Batch  31300/32453 Inputs (000)    4006 - Loss:  0.091 - Validation loss:  0.064\n",
      "Epoch   1/4 Batch  31400/32453 Inputs (000)    4019 - Loss:  0.070 - Validation loss:  0.065\n",
      "Epoch   1/4 Batch  31500/32453 Inputs (000)    4031 - Loss:  0.126 - Validation loss:  0.090\n",
      "Epoch   1/4 Batch  31600/32453 Inputs (000)    4044 - Loss:  0.097 - Validation loss:  0.073\n",
      "Epoch   1/4 Batch  31700/32453 Inputs (000)    4057 - Loss:  0.106 - Validation loss:  0.061\n",
      "Epoch   1/4 Batch  31800/32453 Inputs (000)    4070 - Loss:  0.106 - Validation loss:  0.069\n",
      "Epoch   1/4 Batch  31900/32453 Inputs (000)    4083 - Loss:  0.085 - Validation loss:  0.066\n",
      "Epoch   1/4 Batch  32000/32453 Inputs (000)    4095 - Loss:  0.087 - Validation loss:  0.063\n",
      "Epoch   1/4 Batch  32100/32453 Inputs (000)    4108 - Loss:  0.096 - Validation loss:  0.064\n",
      "Epoch   1/4 Batch  32200/32453 Inputs (000)    4121 - Loss:  0.085 - Validation loss:  0.062\n",
      "Epoch   1/4 Batch  32300/32453 Inputs (000)    4134 - Loss:  0.091 - Validation loss:  0.063\n",
      "Epoch   1/4 Batch  32400/32453 Inputs (000)    4147 - Loss:  0.115 - Validation loss:  0.065\n",
      "Epoch   1/4 Batch  32453/32453 Inputs (000)    4153 - Loss:  0.083 - Validation loss:  0.062\n",
      "Saving graph...\n",
      "Model Trained in 6h:19m:34s and Saved\n",
      "INFO:tensorflow:Restoring parameters from ./large_graph/best_model.ckpt\n",
      "Batch     10/3606 - Accuracy: 55.3%\n",
      "Batch     20/3606 - Accuracy: 53.5%\n",
      "Batch     30/3606 - Accuracy: 52.3%\n",
      "Batch     40/3606 - Accuracy: 52.7%\n",
      "Batch     50/3606 - Accuracy: 53.1%\n",
      "Batch     60/3606 - Accuracy: 53.3%\n",
      "Batch     70/3606 - Accuracy: 53.2%\n",
      "Batch     80/3606 - Accuracy: 53.2%\n",
      "Batch     90/3606 - Accuracy: 53.0%\n",
      "Batch    100/3606 - Accuracy: 53.2%\n",
      "Batch    110/3606 - Accuracy: 53.2%\n",
      "Batch    120/3606 - Accuracy: 53.4%\n",
      "Batch    130/3606 - Accuracy: 53.3%\n",
      "Batch    140/3606 - Accuracy: 53.2%\n",
      "Batch    150/3606 - Accuracy: 53.2%\n",
      "Batch    160/3606 - Accuracy: 53.3%\n",
      "Batch    170/3606 - Accuracy: 53.2%\n",
      "Batch    180/3606 - Accuracy: 53.2%\n",
      "Batch    190/3606 - Accuracy: 53.3%\n",
      "Batch    200/3606 - Accuracy: 53.3%\n",
      "Batch    210/3606 - Accuracy: 53.3%\n",
      "Batch    220/3606 - Accuracy: 53.2%\n",
      "Batch    230/3606 - Accuracy: 53.1%\n",
      "Batch    240/3606 - Accuracy: 53.2%\n",
      "Batch    250/3606 - Accuracy: 53.2%\n",
      "Batch    260/3606 - Accuracy: 53.2%\n",
      "Batch    270/3606 - Accuracy: 53.3%\n",
      "Batch    280/3606 - Accuracy: 53.3%\n",
      "Batch    290/3606 - Accuracy: 53.3%\n",
      "Batch    300/3606 - Accuracy: 53.3%\n",
      "Batch    310/3606 - Accuracy: 53.3%\n",
      "Batch    320/3606 - Accuracy: 53.3%\n",
      "Batch    330/3606 - Accuracy: 53.3%\n",
      "Batch    340/3606 - Accuracy: 53.3%\n",
      "Batch    350/3606 - Accuracy: 53.3%\n",
      "Batch    360/3606 - Accuracy: 53.3%\n",
      "Batch    370/3606 - Accuracy: 53.3%\n",
      "Batch    380/3606 - Accuracy: 53.3%\n",
      "Batch    390/3606 - Accuracy: 53.4%\n",
      "Batch    400/3606 - Accuracy: 53.3%\n",
      "Batch    410/3606 - Accuracy: 53.3%\n",
      "Batch    420/3606 - Accuracy: 53.3%\n",
      "Batch    430/3606 - Accuracy: 53.3%\n",
      "Batch    440/3606 - Accuracy: 53.3%\n",
      "Batch    450/3606 - Accuracy: 53.3%\n",
      "Batch    460/3606 - Accuracy: 53.3%\n",
      "Batch    470/3606 - Accuracy: 53.3%\n",
      "Batch    480/3606 - Accuracy: 53.3%\n",
      "Batch    490/3606 - Accuracy: 53.3%\n",
      "Batch    500/3606 - Accuracy: 53.3%\n",
      "Batch    510/3606 - Accuracy: 53.3%\n",
      "Batch    520/3606 - Accuracy: 53.3%\n",
      "Batch    530/3606 - Accuracy: 53.3%\n",
      "Batch    540/3606 - Accuracy: 53.4%\n",
      "Batch    550/3606 - Accuracy: 53.4%\n",
      "Batch    560/3606 - Accuracy: 53.4%\n",
      "Batch    570/3606 - Accuracy: 53.4%\n",
      "Batch    580/3606 - Accuracy: 53.4%\n",
      "Batch    590/3606 - Accuracy: 53.4%\n",
      "Batch    600/3606 - Accuracy: 53.4%\n",
      "Batch    610/3606 - Accuracy: 53.4%\n",
      "Batch    620/3606 - Accuracy: 53.4%\n",
      "Batch    630/3606 - Accuracy: 53.4%\n",
      "Batch    640/3606 - Accuracy: 53.4%\n",
      "Batch    650/3606 - Accuracy: 53.4%\n",
      "Batch    660/3606 - Accuracy: 53.4%\n",
      "Batch    670/3606 - Accuracy: 53.4%\n",
      "Batch    680/3606 - Accuracy: 53.4%\n",
      "Batch    690/3606 - Accuracy: 53.5%\n",
      "Batch    700/3606 - Accuracy: 53.5%\n",
      "Batch    710/3606 - Accuracy: 53.6%\n",
      "Batch    720/3606 - Accuracy: 53.6%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    730/3606 - Accuracy: 53.6%\n",
      "Batch    740/3606 - Accuracy: 53.6%\n",
      "Batch    750/3606 - Accuracy: 53.6%\n",
      "Batch    760/3606 - Accuracy: 53.6%\n",
      "Batch    770/3606 - Accuracy: 53.6%\n",
      "Batch    780/3606 - Accuracy: 53.6%\n",
      "Batch    790/3606 - Accuracy: 53.6%\n",
      "Batch    800/3606 - Accuracy: 53.6%\n",
      "Batch    810/3606 - Accuracy: 53.6%\n",
      "Batch    820/3606 - Accuracy: 53.6%\n",
      "Batch    830/3606 - Accuracy: 53.6%\n",
      "Batch    840/3606 - Accuracy: 53.6%\n",
      "Batch    850/3606 - Accuracy: 53.6%\n",
      "Batch    860/3606 - Accuracy: 53.6%\n",
      "Batch    870/3606 - Accuracy: 53.6%\n",
      "Batch    880/3606 - Accuracy: 53.6%\n",
      "Batch    890/3606 - Accuracy: 53.6%\n",
      "Batch    900/3606 - Accuracy: 53.6%\n",
      "Batch    910/3606 - Accuracy: 53.6%\n",
      "Batch    920/3606 - Accuracy: 53.6%\n",
      "Batch    930/3606 - Accuracy: 53.6%\n",
      "Batch    940/3606 - Accuracy: 53.6%\n",
      "Batch    950/3606 - Accuracy: 53.6%\n",
      "Batch    960/3606 - Accuracy: 53.6%\n",
      "Batch    970/3606 - Accuracy: 53.6%\n",
      "Batch    980/3606 - Accuracy: 53.6%\n",
      "Batch    990/3606 - Accuracy: 53.6%\n",
      "Batch   1000/3606 - Accuracy: 53.6%\n",
      "Batch   1010/3606 - Accuracy: 53.6%\n",
      "Batch   1020/3606 - Accuracy: 53.6%\n",
      "Batch   1030/3606 - Accuracy: 53.6%\n",
      "Batch   1040/3606 - Accuracy: 53.6%\n",
      "Batch   1050/3606 - Accuracy: 53.6%\n",
      "Batch   1060/3606 - Accuracy: 53.6%\n",
      "Batch   1070/3606 - Accuracy: 53.6%\n",
      "Batch   1080/3606 - Accuracy: 53.6%\n",
      "Batch   1090/3606 - Accuracy: 53.6%\n",
      "Batch   1100/3606 - Accuracy: 53.6%\n",
      "Batch   1110/3606 - Accuracy: 53.6%\n",
      "Batch   1120/3606 - Accuracy: 53.6%\n",
      "Batch   1130/3606 - Accuracy: 53.6%\n",
      "Batch   1140/3606 - Accuracy: 53.6%\n",
      "Batch   1150/3606 - Accuracy: 53.6%\n",
      "Batch   1160/3606 - Accuracy: 53.5%\n",
      "Batch   1170/3606 - Accuracy: 53.5%\n",
      "Batch   1180/3606 - Accuracy: 53.5%\n",
      "Batch   1190/3606 - Accuracy: 53.5%\n",
      "Batch   1200/3606 - Accuracy: 53.5%\n",
      "Batch   1210/3606 - Accuracy: 53.5%\n",
      "Batch   1220/3606 - Accuracy: 53.5%\n",
      "Batch   1230/3606 - Accuracy: 53.5%\n",
      "Batch   1240/3606 - Accuracy: 53.5%\n",
      "Batch   1250/3606 - Accuracy: 53.5%\n",
      "Batch   1260/3606 - Accuracy: 53.5%\n",
      "Batch   1270/3606 - Accuracy: 53.6%\n",
      "Batch   1280/3606 - Accuracy: 53.6%\n",
      "Batch   1290/3606 - Accuracy: 53.6%\n",
      "Batch   1300/3606 - Accuracy: 53.6%\n",
      "Batch   1310/3606 - Accuracy: 53.6%\n",
      "Batch   1320/3606 - Accuracy: 53.5%\n",
      "Batch   1330/3606 - Accuracy: 53.6%\n",
      "Batch   1340/3606 - Accuracy: 53.6%\n",
      "Batch   1350/3606 - Accuracy: 53.6%\n",
      "Batch   1360/3606 - Accuracy: 53.6%\n",
      "Batch   1370/3606 - Accuracy: 53.6%\n",
      "Batch   1380/3606 - Accuracy: 53.6%\n",
      "Batch   1390/3606 - Accuracy: 53.6%\n",
      "Batch   1400/3606 - Accuracy: 53.6%\n",
      "Batch   1410/3606 - Accuracy: 53.6%\n",
      "Batch   1420/3606 - Accuracy: 53.6%\n",
      "Batch   1430/3606 - Accuracy: 53.6%\n",
      "Batch   1440/3606 - Accuracy: 53.6%\n",
      "Batch   1450/3606 - Accuracy: 53.6%\n",
      "Batch   1460/3606 - Accuracy: 53.6%\n",
      "Batch   1470/3606 - Accuracy: 53.6%\n",
      "Batch   1480/3606 - Accuracy: 53.6%\n",
      "Batch   1490/3606 - Accuracy: 53.5%\n",
      "Batch   1500/3606 - Accuracy: 53.6%\n",
      "Batch   1510/3606 - Accuracy: 53.6%\n",
      "Batch   1520/3606 - Accuracy: 53.6%\n",
      "Batch   1530/3606 - Accuracy: 53.6%\n",
      "Batch   1540/3606 - Accuracy: 53.6%\n",
      "Batch   1550/3606 - Accuracy: 53.6%\n",
      "Batch   1560/3606 - Accuracy: 53.6%\n",
      "Batch   1570/3606 - Accuracy: 53.6%\n",
      "Batch   1580/3606 - Accuracy: 53.6%\n",
      "Batch   1590/3606 - Accuracy: 53.6%\n",
      "Batch   1600/3606 - Accuracy: 53.6%\n",
      "Batch   1610/3606 - Accuracy: 53.6%\n",
      "Batch   1620/3606 - Accuracy: 53.6%\n",
      "Batch   1630/3606 - Accuracy: 53.6%\n",
      "Batch   1640/3606 - Accuracy: 53.6%\n",
      "Batch   1650/3606 - Accuracy: 53.6%\n",
      "Batch   1660/3606 - Accuracy: 53.6%\n",
      "Batch   1670/3606 - Accuracy: 53.6%\n",
      "Batch   1680/3606 - Accuracy: 53.6%\n",
      "Batch   1690/3606 - Accuracy: 53.6%\n",
      "Batch   1700/3606 - Accuracy: 53.6%\n",
      "Batch   1710/3606 - Accuracy: 53.6%\n",
      "Batch   1720/3606 - Accuracy: 53.6%\n",
      "Batch   1730/3606 - Accuracy: 53.6%\n",
      "Batch   1740/3606 - Accuracy: 53.6%\n",
      "Batch   1750/3606 - Accuracy: 53.6%\n",
      "Batch   1760/3606 - Accuracy: 53.6%\n",
      "Batch   1770/3606 - Accuracy: 53.6%\n",
      "Batch   1780/3606 - Accuracy: 53.7%\n",
      "Batch   1790/3606 - Accuracy: 53.7%\n",
      "Batch   1800/3606 - Accuracy: 53.7%\n",
      "Batch   1810/3606 - Accuracy: 53.7%\n",
      "Batch   1820/3606 - Accuracy: 53.7%\n",
      "Batch   1830/3606 - Accuracy: 53.7%\n",
      "Batch   1840/3606 - Accuracy: 53.7%\n",
      "Batch   1850/3606 - Accuracy: 53.7%\n",
      "Batch   1860/3606 - Accuracy: 53.7%\n",
      "Batch   1870/3606 - Accuracy: 53.7%\n",
      "Batch   1880/3606 - Accuracy: 53.7%\n",
      "Batch   1890/3606 - Accuracy: 53.7%\n",
      "Batch   1900/3606 - Accuracy: 53.7%\n",
      "Batch   1910/3606 - Accuracy: 53.7%\n",
      "Batch   1920/3606 - Accuracy: 53.7%\n",
      "Batch   1930/3606 - Accuracy: 53.7%\n",
      "Batch   1940/3606 - Accuracy: 53.7%\n",
      "Batch   1950/3606 - Accuracy: 53.7%\n",
      "Batch   1960/3606 - Accuracy: 53.7%\n",
      "Batch   1970/3606 - Accuracy: 53.7%\n",
      "Batch   1980/3606 - Accuracy: 53.7%\n",
      "Batch   1990/3606 - Accuracy: 53.7%\n",
      "Batch   2000/3606 - Accuracy: 53.7%\n",
      "Batch   2010/3606 - Accuracy: 53.7%\n",
      "Batch   2020/3606 - Accuracy: 53.7%\n",
      "Batch   2030/3606 - Accuracy: 53.7%\n",
      "Batch   2040/3606 - Accuracy: 53.7%\n",
      "Batch   2050/3606 - Accuracy: 53.7%\n",
      "Batch   2060/3606 - Accuracy: 53.7%\n",
      "Batch   2070/3606 - Accuracy: 53.7%\n",
      "Batch   2080/3606 - Accuracy: 53.7%\n",
      "Batch   2090/3606 - Accuracy: 53.7%\n",
      "Batch   2100/3606 - Accuracy: 53.7%\n",
      "Batch   2110/3606 - Accuracy: 53.7%\n",
      "Batch   2120/3606 - Accuracy: 53.7%\n",
      "Batch   2130/3606 - Accuracy: 53.7%\n",
      "Batch   2140/3606 - Accuracy: 53.7%\n",
      "Batch   2150/3606 - Accuracy: 53.7%\n",
      "Batch   2160/3606 - Accuracy: 53.7%\n",
      "Batch   2170/3606 - Accuracy: 53.7%\n",
      "Batch   2180/3606 - Accuracy: 53.7%\n",
      "Batch   2190/3606 - Accuracy: 53.7%\n",
      "Batch   2200/3606 - Accuracy: 53.7%\n",
      "Batch   2210/3606 - Accuracy: 53.7%\n",
      "Batch   2220/3606 - Accuracy: 53.7%\n",
      "Batch   2230/3606 - Accuracy: 53.7%\n",
      "Batch   2240/3606 - Accuracy: 53.7%\n",
      "Batch   2250/3606 - Accuracy: 53.7%\n",
      "Batch   2260/3606 - Accuracy: 53.7%\n",
      "Batch   2270/3606 - Accuracy: 53.7%\n",
      "Batch   2280/3606 - Accuracy: 53.7%\n",
      "Batch   2290/3606 - Accuracy: 53.7%\n",
      "Batch   2300/3606 - Accuracy: 53.7%\n",
      "Batch   2310/3606 - Accuracy: 53.7%\n",
      "Batch   2320/3606 - Accuracy: 53.7%\n",
      "Batch   2330/3606 - Accuracy: 53.7%\n",
      "Batch   2340/3606 - Accuracy: 53.7%\n",
      "Batch   2350/3606 - Accuracy: 53.7%\n",
      "Batch   2360/3606 - Accuracy: 53.7%\n",
      "Batch   2370/3606 - Accuracy: 53.7%\n",
      "Batch   2380/3606 - Accuracy: 53.7%\n",
      "Batch   2390/3606 - Accuracy: 53.7%\n",
      "Batch   2400/3606 - Accuracy: 53.7%\n",
      "Batch   2410/3606 - Accuracy: 53.7%\n",
      "Batch   2420/3606 - Accuracy: 53.7%\n",
      "Batch   2430/3606 - Accuracy: 53.7%\n",
      "Batch   2440/3606 - Accuracy: 53.7%\n",
      "Batch   2450/3606 - Accuracy: 53.7%\n",
      "Batch   2460/3606 - Accuracy: 53.7%\n",
      "Batch   2470/3606 - Accuracy: 53.7%\n",
      "Batch   2480/3606 - Accuracy: 53.7%\n",
      "Batch   2490/3606 - Accuracy: 53.7%\n",
      "Batch   2500/3606 - Accuracy: 53.7%\n",
      "Batch   2510/3606 - Accuracy: 53.7%\n",
      "Batch   2520/3606 - Accuracy: 53.7%\n",
      "Batch   2530/3606 - Accuracy: 53.7%\n",
      "Batch   2540/3606 - Accuracy: 53.7%\n",
      "Batch   2550/3606 - Accuracy: 53.7%\n",
      "Batch   2560/3606 - Accuracy: 53.7%\n",
      "Batch   2570/3606 - Accuracy: 53.7%\n",
      "Batch   2580/3606 - Accuracy: 53.7%\n",
      "Batch   2590/3606 - Accuracy: 53.7%\n",
      "Batch   2600/3606 - Accuracy: 53.7%\n",
      "Batch   2610/3606 - Accuracy: 53.7%\n",
      "Batch   2620/3606 - Accuracy: 53.7%\n",
      "Batch   2630/3606 - Accuracy: 53.7%\n",
      "Batch   2640/3606 - Accuracy: 53.7%\n",
      "Batch   2650/3606 - Accuracy: 53.7%\n",
      "Batch   2660/3606 - Accuracy: 53.7%\n",
      "Batch   2670/3606 - Accuracy: 53.7%\n",
      "Batch   2680/3606 - Accuracy: 53.7%\n",
      "Batch   2690/3606 - Accuracy: 53.7%\n",
      "Batch   2700/3606 - Accuracy: 53.7%\n",
      "Batch   2710/3606 - Accuracy: 53.7%\n",
      "Batch   2720/3606 - Accuracy: 53.7%\n",
      "Batch   2730/3606 - Accuracy: 53.7%\n",
      "Batch   2740/3606 - Accuracy: 53.7%\n",
      "Batch   2750/3606 - Accuracy: 53.7%\n",
      "Batch   2760/3606 - Accuracy: 53.7%\n",
      "Batch   2770/3606 - Accuracy: 53.7%\n",
      "Batch   2780/3606 - Accuracy: 53.7%\n",
      "Batch   2790/3606 - Accuracy: 53.7%\n",
      "Batch   2800/3606 - Accuracy: 53.7%\n",
      "Batch   2810/3606 - Accuracy: 53.7%\n",
      "Batch   2820/3606 - Accuracy: 53.7%\n",
      "Batch   2830/3606 - Accuracy: 53.7%\n",
      "Batch   2840/3606 - Accuracy: 53.7%\n",
      "Batch   2850/3606 - Accuracy: 53.7%\n",
      "Batch   2860/3606 - Accuracy: 53.7%\n",
      "Batch   2870/3606 - Accuracy: 53.7%\n",
      "Batch   2880/3606 - Accuracy: 53.7%\n",
      "Batch   2890/3606 - Accuracy: 53.7%\n",
      "Batch   2900/3606 - Accuracy: 53.7%\n",
      "Batch   2910/3606 - Accuracy: 53.7%\n",
      "Batch   2920/3606 - Accuracy: 53.7%\n",
      "Batch   2930/3606 - Accuracy: 53.7%\n",
      "Batch   2940/3606 - Accuracy: 53.7%\n",
      "Batch   2950/3606 - Accuracy: 53.7%\n",
      "Batch   2960/3606 - Accuracy: 53.7%\n",
      "Batch   2970/3606 - Accuracy: 53.7%\n",
      "Batch   2980/3606 - Accuracy: 53.7%\n",
      "Batch   2990/3606 - Accuracy: 53.7%\n",
      "Batch   3000/3606 - Accuracy: 53.7%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch   3010/3606 - Accuracy: 53.7%\n",
      "Batch   3020/3606 - Accuracy: 53.7%\n",
      "Batch   3030/3606 - Accuracy: 53.7%\n",
      "Batch   3040/3606 - Accuracy: 53.7%\n",
      "Batch   3050/3606 - Accuracy: 53.6%\n",
      "Batch   3060/3606 - Accuracy: 53.7%\n",
      "Batch   3070/3606 - Accuracy: 53.6%\n",
      "Batch   3080/3606 - Accuracy: 53.7%\n",
      "Batch   3090/3606 - Accuracy: 53.7%\n",
      "Batch   3100/3606 - Accuracy: 53.7%\n",
      "Batch   3110/3606 - Accuracy: 53.6%\n",
      "Batch   3120/3606 - Accuracy: 53.7%\n",
      "Batch   3130/3606 - Accuracy: 53.7%\n",
      "Batch   3140/3606 - Accuracy: 53.7%\n",
      "Batch   3150/3606 - Accuracy: 53.7%\n",
      "Batch   3160/3606 - Accuracy: 53.6%\n",
      "Batch   3170/3606 - Accuracy: 53.6%\n",
      "Batch   3180/3606 - Accuracy: 53.6%\n",
      "Batch   3190/3606 - Accuracy: 53.6%\n",
      "Batch   3200/3606 - Accuracy: 53.6%\n",
      "Batch   3210/3606 - Accuracy: 53.6%\n",
      "Batch   3220/3606 - Accuracy: 53.6%\n",
      "Batch   3230/3606 - Accuracy: 53.6%\n",
      "Batch   3240/3606 - Accuracy: 53.6%\n",
      "Batch   3250/3606 - Accuracy: 53.6%\n",
      "Batch   3260/3606 - Accuracy: 53.6%\n",
      "Batch   3270/3606 - Accuracy: 53.7%\n",
      "Batch   3280/3606 - Accuracy: 53.7%\n",
      "Batch   3290/3606 - Accuracy: 53.7%\n",
      "Batch   3300/3606 - Accuracy: 53.7%\n",
      "Batch   3310/3606 - Accuracy: 53.7%\n",
      "Batch   3320/3606 - Accuracy: 53.7%\n",
      "Batch   3330/3606 - Accuracy: 53.7%\n",
      "Batch   3340/3606 - Accuracy: 53.7%\n",
      "Batch   3350/3606 - Accuracy: 53.7%\n",
      "Batch   3360/3606 - Accuracy: 53.7%\n",
      "Batch   3370/3606 - Accuracy: 53.7%\n",
      "Batch   3380/3606 - Accuracy: 53.7%\n",
      "Batch   3390/3606 - Accuracy: 53.7%\n",
      "Batch   3400/3606 - Accuracy: 53.7%\n",
      "Batch   3410/3606 - Accuracy: 53.7%\n",
      "Batch   3420/3606 - Accuracy: 53.7%\n",
      "Batch   3430/3606 - Accuracy: 53.7%\n",
      "Batch   3440/3606 - Accuracy: 53.7%\n",
      "Batch   3450/3606 - Accuracy: 53.7%\n",
      "Batch   3460/3606 - Accuracy: 53.7%\n",
      "Batch   3470/3606 - Accuracy: 53.7%\n",
      "Batch   3480/3606 - Accuracy: 53.7%\n",
      "Batch   3490/3606 - Accuracy: 53.7%\n",
      "Batch   3500/3606 - Accuracy: 53.7%\n",
      "Batch   3510/3606 - Accuracy: 53.7%\n",
      "Batch   3520/3606 - Accuracy: 53.7%\n",
      "Batch   3530/3606 - Accuracy: 53.7%\n",
      "Batch   3540/3606 - Accuracy: 53.7%\n",
      "Batch   3550/3606 - Accuracy: 53.7%\n",
      "Batch   3560/3606 - Accuracy: 53.7%\n",
      "Batch   3570/3606 - Accuracy: 53.7%\n",
      "Batch   3580/3606 - Accuracy: 53.7%\n",
      "Batch   3590/3606 - Accuracy: 53.7%\n",
      "Batch   3600/3606 - Accuracy: 53.7%\n",
      "Final accuracy = 53.7%\n",
      "\n",
      "Reloading existing graph to continue training.\n",
      "INFO:tensorflow:Restoring parameters from ./large_graph/best_model.ckpt\n",
      "Epoch   2/4 Batch    100/32453 Inputs (000)    4166 - Loss:  0.095 - Validation loss:  0.063\n",
      "Epoch   2/4 Batch    200/32453 Inputs (000)    4179 - Loss:  0.152 - Validation loss:  0.095\n",
      "Epoch   2/4 Batch    300/32453 Inputs (000)    4192 - Loss:  0.106 - Validation loss:  0.066\n",
      "Epoch   2/4 Batch    400/32453 Inputs (000)    4205 - Loss:  0.090 - Validation loss:  0.067\n",
      "Epoch   2/4 Batch    500/32453 Inputs (000)    4217 - Loss:  0.097 - Validation loss:  0.065\n",
      "Epoch   2/4 Batch    600/32453 Inputs (000)    4230 - Loss:  0.099 - Validation loss:  0.063\n",
      "Epoch   2/4 Batch    700/32453 Inputs (000)    4243 - Loss:  0.093 - Validation loss:  0.064\n",
      "Epoch   2/4 Batch    800/32453 Inputs (000)    4256 - Loss:  0.087 - Validation loss:  0.064\n",
      "Epoch   2/4 Batch    900/32453 Inputs (000)    4269 - Loss:  0.077 - Validation loss:  0.068\n",
      "Epoch   2/4 Batch   1000/32453 Inputs (000)    4281 - Loss:  0.099 - Validation loss:  0.065\n",
      "Epoch   2/4 Batch   1100/32453 Inputs (000)    4294 - Loss:  0.076 - Validation loss:  0.068\n",
      "Epoch   2/4 Batch   1200/32453 Inputs (000)    4307 - Loss:  0.089 - Validation loss:  0.065\n",
      "Epoch   2/4 Batch   1300/32453 Inputs (000)    4320 - Loss:  0.079 - Validation loss:  0.068\n",
      "Epoch   2/4 Batch   1400/32453 Inputs (000)    4333 - Loss:  0.084 - Validation loss:  0.062\n",
      "Epoch   2/4 Batch   1500/32453 Inputs (000)    4345 - Loss:  0.075 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch   1600/32453 Inputs (000)    4358 - Loss:  0.066 - Validation loss:  0.063\n",
      "Epoch   2/4 Batch   1700/32453 Inputs (000)    4371 - Loss:  0.094 - Validation loss:  0.064\n",
      "Epoch   2/4 Batch   1800/32453 Inputs (000)    4384 - Loss:  0.086 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch   1900/32453 Inputs (000)    4397 - Loss:  0.079 - Validation loss:  0.062\n",
      "Epoch   2/4 Batch   2000/32453 Inputs (000)    4409 - Loss:  0.073 - Validation loss:  0.064\n",
      "Epoch   2/4 Batch   2100/32453 Inputs (000)    4422 - Loss:  0.078 - Validation loss:  0.066\n",
      "Epoch   2/4 Batch   2200/32453 Inputs (000)    4435 - Loss:  0.080 - Validation loss:  0.064\n",
      "Epoch   2/4 Batch   2300/32453 Inputs (000)    4448 - Loss:  0.072 - Validation loss:  0.062\n",
      "Epoch   2/4 Batch   2400/32453 Inputs (000)    4461 - Loss:  0.088 - Validation loss:  0.059\n",
      "Epoch   2/4 Batch   2500/32453 Inputs (000)    4473 - Loss:  0.073 - Validation loss:  0.060\n",
      "Epoch   2/4 Batch   2600/32453 Inputs (000)    4486 - Loss:  0.073 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch   2700/32453 Inputs (000)    4499 - Loss:  0.085 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch   2800/32453 Inputs (000)    4512 - Loss:  0.081 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch   2900/32453 Inputs (000)    4525 - Loss:  0.069 - Validation loss:  0.060\n",
      "Epoch   2/4 Batch   3000/32453 Inputs (000)    4537 - Loss:  0.078 - Validation loss:  0.060\n",
      "Epoch   2/4 Batch   3100/32453 Inputs (000)    4550 - Loss:  0.088 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch   3200/32453 Inputs (000)    4563 - Loss:  0.116 - Validation loss:  0.066\n",
      "Epoch   2/4 Batch   3300/32453 Inputs (000)    4576 - Loss:  0.103 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch   3400/32453 Inputs (000)    4589 - Loss:  0.087 - Validation loss:  0.059\n",
      "Epoch   2/4 Batch   3500/32453 Inputs (000)    4601 - Loss:  0.081 - Validation loss:  0.064\n",
      "Epoch   2/4 Batch   3600/32453 Inputs (000)    4614 - Loss:  0.097 - Validation loss:  0.066\n",
      "Epoch   2/4 Batch   3700/32453 Inputs (000)    4627 - Loss:  0.090 - Validation loss:  0.063\n",
      "Epoch   2/4 Batch   3800/32453 Inputs (000)    4640 - Loss:  0.071 - Validation loss:  0.063\n",
      "Epoch   2/4 Batch   3900/32453 Inputs (000)    4653 - Loss:  0.093 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch   4000/32453 Inputs (000)    4665 - Loss:  0.101 - Validation loss:  0.065\n",
      "Epoch   2/4 Batch   4100/32453 Inputs (000)    4678 - Loss:  0.098 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch   4200/32453 Inputs (000)    4691 - Loss:  0.087 - Validation loss:  0.060\n",
      "Epoch   2/4 Batch   4300/32453 Inputs (000)    4704 - Loss:  0.115 - Validation loss:  0.060\n",
      "Epoch   2/4 Batch   4400/32453 Inputs (000)    4717 - Loss:  0.100 - Validation loss:  0.073\n",
      "Epoch   2/4 Batch   4500/32453 Inputs (000)    4729 - Loss:  0.089 - Validation loss:  0.064\n",
      "Epoch   2/4 Batch   4600/32453 Inputs (000)    4742 - Loss:  0.093 - Validation loss:  0.062\n",
      "Epoch   2/4 Batch   4700/32453 Inputs (000)    4755 - Loss:  0.083 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch   4800/32453 Inputs (000)    4768 - Loss:  0.068 - Validation loss:  0.062\n",
      "Epoch   2/4 Batch   4900/32453 Inputs (000)    4781 - Loss:  0.119 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch   5000/32453 Inputs (000)    4793 - Loss:  0.080 - Validation loss:  0.060\n",
      "Epoch   2/4 Batch   5100/32453 Inputs (000)    4806 - Loss:  0.077 - Validation loss:  0.059\n",
      "Epoch   2/4 Batch   5200/32453 Inputs (000)    4819 - Loss:  0.089 - Validation loss:  0.058\n",
      "Epoch   2/4 Batch   5300/32453 Inputs (000)    4832 - Loss:  0.085 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch   5400/32453 Inputs (000)    4845 - Loss:  0.074 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch   5500/32453 Inputs (000)    4857 - Loss:  0.089 - Validation loss:  0.058\n",
      "Epoch   2/4 Batch   5600/32453 Inputs (000)    4870 - Loss:  0.069 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch   5700/32453 Inputs (000)    4883 - Loss:  0.072 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch   5800/32453 Inputs (000)    4896 - Loss:  0.074 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch   5900/32453 Inputs (000)    4909 - Loss:  0.070 - Validation loss:  0.060\n",
      "Epoch   2/4 Batch   6000/32453 Inputs (000)    4921 - Loss:  0.094 - Validation loss:  0.062\n",
      "Epoch   2/4 Batch   6100/32453 Inputs (000)    4934 - Loss:  0.088 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch   6200/32453 Inputs (000)    4947 - Loss:  0.066 - Validation loss:  0.059\n",
      "Epoch   2/4 Batch   6300/32453 Inputs (000)    4960 - Loss:  0.080 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch   6400/32453 Inputs (000)    4973 - Loss:  0.069 - Validation loss:  0.056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/4 Batch   6500/32453 Inputs (000)    4985 - Loss:  0.092 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch   6600/32453 Inputs (000)    4998 - Loss:  0.079 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch   6700/32453 Inputs (000)    5011 - Loss:  0.086 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch   6800/32453 Inputs (000)    5024 - Loss:  0.089 - Validation loss:  0.058\n",
      "Epoch   2/4 Batch   6900/32453 Inputs (000)    5037 - Loss:  0.084 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch   7000/32453 Inputs (000)    5049 - Loss:  0.062 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch   7100/32453 Inputs (000)    5062 - Loss:  0.087 - Validation loss:  0.058\n",
      "Epoch   2/4 Batch   7200/32453 Inputs (000)    5075 - Loss:  0.079 - Validation loss:  0.059\n",
      "Epoch   2/4 Batch   7300/32453 Inputs (000)    5088 - Loss:  0.096 - Validation loss:  0.059\n",
      "Epoch   2/4 Batch   7400/32453 Inputs (000)    5101 - Loss:  0.067 - Validation loss:  0.058\n",
      "Epoch   2/4 Batch   7500/32453 Inputs (000)    5113 - Loss:  0.081 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch   7600/32453 Inputs (000)    5126 - Loss:  0.086 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch   7700/32453 Inputs (000)    5139 - Loss:  0.086 - Validation loss:  0.060\n",
      "Epoch   2/4 Batch   7800/32453 Inputs (000)    5152 - Loss:  0.087 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch   7900/32453 Inputs (000)    5165 - Loss:  0.072 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch   8000/32453 Inputs (000)    5177 - Loss:  0.084 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch   8100/32453 Inputs (000)    5190 - Loss:  0.111 - Validation loss:  0.066\n",
      "Epoch   2/4 Batch   8200/32453 Inputs (000)    5203 - Loss:  0.085 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch   8300/32453 Inputs (000)    5216 - Loss:  0.083 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch   8400/32453 Inputs (000)    5229 - Loss:  0.074 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch   8500/32453 Inputs (000)    5241 - Loss:  0.062 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch   8600/32453 Inputs (000)    5254 - Loss:  0.080 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch   8700/32453 Inputs (000)    5267 - Loss:  0.079 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch   8800/32453 Inputs (000)    5280 - Loss:  0.077 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch   8900/32453 Inputs (000)    5293 - Loss:  0.092 - Validation loss:  0.060\n",
      "Epoch   2/4 Batch   9000/32453 Inputs (000)    5305 - Loss:  0.063 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch   9100/32453 Inputs (000)    5318 - Loss:  0.073 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch   9200/32453 Inputs (000)    5331 - Loss:  0.333 - Validation loss:  0.233\n",
      "Epoch   2/4 Batch   9300/32453 Inputs (000)    5344 - Loss:  0.104 - Validation loss:  0.072\n",
      "Epoch   2/4 Batch   9400/32453 Inputs (000)    5357 - Loss:  0.102 - Validation loss:  0.065\n",
      "Epoch   2/4 Batch   9500/32453 Inputs (000)    5369 - Loss:  0.110 - Validation loss:  0.064\n",
      "Epoch   2/4 Batch   9600/32453 Inputs (000)    5382 - Loss:  0.089 - Validation loss:  0.059\n",
      "Epoch   2/4 Batch   9700/32453 Inputs (000)    5395 - Loss:  0.072 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch   9800/32453 Inputs (000)    5408 - Loss:  0.080 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch   9900/32453 Inputs (000)    5421 - Loss:  0.118 - Validation loss:  0.091\n",
      "Epoch   2/4 Batch  10000/32453 Inputs (000)    5433 - Loss:  0.092 - Validation loss:  0.068\n",
      "Epoch   2/4 Batch  10100/32453 Inputs (000)    5446 - Loss:  0.084 - Validation loss:  0.068\n",
      "Epoch   2/4 Batch  10200/32453 Inputs (000)    5459 - Loss:  0.092 - Validation loss:  0.065\n",
      "Epoch   2/4 Batch  10300/32453 Inputs (000)    5472 - Loss:  0.094 - Validation loss:  0.060\n",
      "Epoch   2/4 Batch  10400/32453 Inputs (000)    5485 - Loss:  0.087 - Validation loss:  0.064\n",
      "Epoch   2/4 Batch  10500/32453 Inputs (000)    5497 - Loss:  0.078 - Validation loss:  0.059\n",
      "Epoch   2/4 Batch  10600/32453 Inputs (000)    5510 - Loss:  0.083 - Validation loss:  0.062\n",
      "Epoch   2/4 Batch  10700/32453 Inputs (000)    5523 - Loss:  0.097 - Validation loss:  0.059\n",
      "Epoch   2/4 Batch  10800/32453 Inputs (000)    5536 - Loss:  0.084 - Validation loss:  0.058\n",
      "Epoch   2/4 Batch  10900/32453 Inputs (000)    5549 - Loss:  0.068 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch  11000/32453 Inputs (000)    5561 - Loss:  0.086 - Validation loss:  0.060\n",
      "Epoch   2/4 Batch  11100/32453 Inputs (000)    5574 - Loss:  0.074 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch  11200/32453 Inputs (000)    5587 - Loss:  0.083 - Validation loss:  0.060\n",
      "Epoch   2/4 Batch  11300/32453 Inputs (000)    5600 - Loss:  0.103 - Validation loss:  0.067\n",
      "Epoch   2/4 Batch  11400/32453 Inputs (000)    5613 - Loss:  0.071 - Validation loss:  0.062\n",
      "Epoch   2/4 Batch  11500/32453 Inputs (000)    5625 - Loss:  0.068 - Validation loss:  0.058\n",
      "Epoch   2/4 Batch  11600/32453 Inputs (000)    5638 - Loss:  0.080 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch  11700/32453 Inputs (000)    5651 - Loss:  0.069 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch  11800/32453 Inputs (000)    5664 - Loss:  0.072 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch  11900/32453 Inputs (000)    5677 - Loss:  0.070 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch  12000/32453 Inputs (000)    5689 - Loss:  0.090 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch  12100/32453 Inputs (000)    5702 - Loss:  0.083 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch  12200/32453 Inputs (000)    5715 - Loss:  0.068 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch  12300/32453 Inputs (000)    5728 - Loss:  0.054 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch  12400/32453 Inputs (000)    5741 - Loss:  0.073 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch  12500/32453 Inputs (000)    5753 - Loss:  0.090 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch  12600/32453 Inputs (000)    5766 - Loss:  0.106 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch  12700/32453 Inputs (000)    5779 - Loss:  0.072 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch  12800/32453 Inputs (000)    5792 - Loss:  0.080 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  12900/32453 Inputs (000)    5805 - Loss:  0.074 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch  13000/32453 Inputs (000)    5817 - Loss:  0.076 - Validation loss:  0.071\n",
      "Epoch   2/4 Batch  13100/32453 Inputs (000)    5830 - Loss:  0.068 - Validation loss:  0.059\n",
      "Epoch   2/4 Batch  13200/32453 Inputs (000)    5843 - Loss:  0.089 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch  13300/32453 Inputs (000)    5856 - Loss:  0.075 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  13400/32453 Inputs (000)    5869 - Loss:  0.052 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch  13500/32453 Inputs (000)    5881 - Loss:  0.077 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch  13600/32453 Inputs (000)    5894 - Loss:  0.083 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch  13700/32453 Inputs (000)    5907 - Loss:  0.070 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch  13800/32453 Inputs (000)    5920 - Loss:  0.079 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  13900/32453 Inputs (000)    5933 - Loss:  0.065 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  14000/32453 Inputs (000)    5945 - Loss:  0.085 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  14100/32453 Inputs (000)    5958 - Loss:  0.079 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  14200/32453 Inputs (000)    5971 - Loss:  0.086 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  14300/32453 Inputs (000)    5984 - Loss:  0.073 - Validation loss:  0.058\n",
      "Epoch   2/4 Batch  14400/32453 Inputs (000)    5997 - Loss:  0.096 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch  14500/32453 Inputs (000)    6009 - Loss:  0.087 - Validation loss:  0.062\n",
      "Epoch   2/4 Batch  14600/32453 Inputs (000)    6022 - Loss:  0.070 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch  14700/32453 Inputs (000)    6035 - Loss:  0.068 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch  14800/32453 Inputs (000)    6048 - Loss:  0.064 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch  14900/32453 Inputs (000)    6061 - Loss:  0.087 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch  15000/32453 Inputs (000)    6073 - Loss:  0.072 - Validation loss:  0.050\n",
      "Epoch   2/4 Batch  15100/32453 Inputs (000)    6086 - Loss:  0.072 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  15200/32453 Inputs (000)    6099 - Loss:  0.085 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch  15300/32453 Inputs (000)    6112 - Loss:  0.081 - Validation loss:  0.053\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/4 Batch  15400/32453 Inputs (000)    6125 - Loss:  0.074 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  15500/32453 Inputs (000)    6137 - Loss:  0.071 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch  15600/32453 Inputs (000)    6150 - Loss:  0.071 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  15700/32453 Inputs (000)    6163 - Loss:  0.075 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  15800/32453 Inputs (000)    6176 - Loss:  0.071 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch  15900/32453 Inputs (000)    6189 - Loss:  0.067 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch  16000/32453 Inputs (000)    6201 - Loss:  0.081 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch  16100/32453 Inputs (000)    6214 - Loss:  0.055 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  16200/32453 Inputs (000)    6227 - Loss:  0.069 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  16300/32453 Inputs (000)    6240 - Loss:  0.077 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch  16400/32453 Inputs (000)    6253 - Loss:  0.070 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch  16500/32453 Inputs (000)    6265 - Loss:  0.078 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch  16600/32453 Inputs (000)    6278 - Loss:  0.065 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch  16700/32453 Inputs (000)    6291 - Loss:  0.067 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch  16800/32453 Inputs (000)    6304 - Loss:  0.063 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch  16900/32453 Inputs (000)    6317 - Loss:  0.094 - Validation loss:  0.062\n",
      "Epoch   2/4 Batch  17000/32453 Inputs (000)    6329 - Loss:  0.080 - Validation loss:  0.058\n",
      "Epoch   2/4 Batch  17100/32453 Inputs (000)    6342 - Loss:  0.086 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch  17200/32453 Inputs (000)    6355 - Loss:  0.071 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch  17300/32453 Inputs (000)    6368 - Loss:  0.071 - Validation loss:  0.059\n",
      "Epoch   2/4 Batch  17400/32453 Inputs (000)    6381 - Loss:  0.076 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  17500/32453 Inputs (000)    6393 - Loss:  0.083 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch  17600/32453 Inputs (000)    6406 - Loss:  0.077 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  17700/32453 Inputs (000)    6419 - Loss:  0.065 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  17800/32453 Inputs (000)    6432 - Loss:  0.067 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  17900/32453 Inputs (000)    6445 - Loss:  0.075 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  18000/32453 Inputs (000)    6457 - Loss:  0.083 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  18100/32453 Inputs (000)    6470 - Loss:  0.105 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  18200/32453 Inputs (000)    6483 - Loss:  0.089 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch  18300/32453 Inputs (000)    6496 - Loss:  0.066 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  18400/32453 Inputs (000)    6509 - Loss:  0.058 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  18500/32453 Inputs (000)    6521 - Loss:  0.085 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch  18600/32453 Inputs (000)    6534 - Loss:  0.088 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  18700/32453 Inputs (000)    6547 - Loss:  0.094 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch  18800/32453 Inputs (000)    6560 - Loss:  0.071 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch  18900/32453 Inputs (000)    6573 - Loss:  0.070 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  19000/32453 Inputs (000)    6585 - Loss:  0.071 - Validation loss:  0.059\n",
      "Epoch   2/4 Batch  19100/32453 Inputs (000)    6598 - Loss:  0.070 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  19200/32453 Inputs (000)    6611 - Loss:  0.088 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  19300/32453 Inputs (000)    6624 - Loss:  0.061 - Validation loss:  0.049\n",
      "Epoch   2/4 Batch  19400/32453 Inputs (000)    6637 - Loss:  0.071 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch  19500/32453 Inputs (000)    6649 - Loss:  0.075 - Validation loss:  0.050\n",
      "Epoch   2/4 Batch  19600/32453 Inputs (000)    6662 - Loss:  0.071 - Validation loss:  0.046\n",
      "Epoch   2/4 Batch  19700/32453 Inputs (000)    6675 - Loss:  0.071 - Validation loss:  0.047\n",
      "Epoch   2/4 Batch  19800/32453 Inputs (000)    6688 - Loss:  0.068 - Validation loss:  0.049\n",
      "Epoch   2/4 Batch  19900/32453 Inputs (000)    6701 - Loss:  0.064 - Validation loss:  0.049\n",
      "Epoch   2/4 Batch  20000/32453 Inputs (000)    6713 - Loss:  0.115 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch  20100/32453 Inputs (000)    6726 - Loss:  0.070 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  20200/32453 Inputs (000)    6739 - Loss:  0.065 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch  20300/32453 Inputs (000)    6752 - Loss:  0.077 - Validation loss:  0.050\n",
      "Epoch   2/4 Batch  20400/32453 Inputs (000)    6765 - Loss:  0.051 - Validation loss:  0.050\n",
      "Epoch   2/4 Batch  20500/32453 Inputs (000)    6777 - Loss:  0.062 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  20600/32453 Inputs (000)    6790 - Loss:  0.059 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  20700/32453 Inputs (000)    6803 - Loss:  0.077 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  20800/32453 Inputs (000)    6816 - Loss:  0.057 - Validation loss:  0.049\n",
      "Epoch   2/4 Batch  20900/32453 Inputs (000)    6829 - Loss:  0.090 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  21000/32453 Inputs (000)    6841 - Loss:  0.068 - Validation loss:  0.049\n",
      "Epoch   2/4 Batch  21100/32453 Inputs (000)    6854 - Loss:  0.079 - Validation loss:  0.047\n",
      "Epoch   2/4 Batch  21200/32453 Inputs (000)    6867 - Loss:  0.077 - Validation loss:  0.049\n",
      "Epoch   2/4 Batch  21300/32453 Inputs (000)    6880 - Loss:  0.066 - Validation loss:  0.047\n",
      "Epoch   2/4 Batch  21400/32453 Inputs (000)    6893 - Loss:  0.065 - Validation loss:  0.044\n",
      "Epoch   2/4 Batch  21500/32453 Inputs (000)    6905 - Loss:  0.087 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch  21600/32453 Inputs (000)    6918 - Loss:  0.087 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  21700/32453 Inputs (000)    6931 - Loss:  0.094 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  21800/32453 Inputs (000)    6944 - Loss:  0.072 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  21900/32453 Inputs (000)    6957 - Loss:  0.072 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  22000/32453 Inputs (000)    6969 - Loss:  0.058 - Validation loss:  0.049\n",
      "Epoch   2/4 Batch  22100/32453 Inputs (000)    6982 - Loss:  0.058 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  22200/32453 Inputs (000)    6995 - Loss:  0.077 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  22300/32453 Inputs (000)    7008 - Loss:  0.078 - Validation loss:  0.050\n",
      "Epoch   2/4 Batch  22400/32453 Inputs (000)    7021 - Loss:  0.087 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch  22500/32453 Inputs (000)    7033 - Loss:  0.074 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  22600/32453 Inputs (000)    7046 - Loss:  0.071 - Validation loss:  0.050\n",
      "Epoch   2/4 Batch  22700/32453 Inputs (000)    7059 - Loss:  0.111 - Validation loss:  0.085\n",
      "Epoch   2/4 Batch  22800/32453 Inputs (000)    7072 - Loss:  0.101 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch  22900/32453 Inputs (000)    7085 - Loss:  0.078 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch  23000/32453 Inputs (000)    7097 - Loss:  0.085 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  23100/32453 Inputs (000)    7110 - Loss:  0.067 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  23200/32453 Inputs (000)    7123 - Loss:  0.090 - Validation loss:  0.056\n",
      "Epoch   2/4 Batch  23300/32453 Inputs (000)    7136 - Loss:  0.073 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  23400/32453 Inputs (000)    7149 - Loss:  0.076 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  23500/32453 Inputs (000)    7161 - Loss:  0.060 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  23600/32453 Inputs (000)    7174 - Loss:  0.056 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch  23700/32453 Inputs (000)    7187 - Loss:  0.076 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  23800/32453 Inputs (000)    7200 - Loss:  0.093 - Validation loss:  0.057\n",
      "Epoch   2/4 Batch  23900/32453 Inputs (000)    7213 - Loss:  0.071 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  24000/32453 Inputs (000)    7225 - Loss:  0.080 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  24100/32453 Inputs (000)    7238 - Loss:  0.072 - Validation loss:  0.046\n",
      "Epoch   2/4 Batch  24200/32453 Inputs (000)    7251 - Loss:  0.073 - Validation loss:  0.050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   2/4 Batch  24300/32453 Inputs (000)    7264 - Loss:  0.051 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  24400/32453 Inputs (000)    7277 - Loss:  0.078 - Validation loss:  0.046\n",
      "Epoch   2/4 Batch  24500/32453 Inputs (000)    7289 - Loss:  0.088 - Validation loss:  0.047\n",
      "Epoch   2/4 Batch  24600/32453 Inputs (000)    7302 - Loss:  0.063 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  24700/32453 Inputs (000)    7315 - Loss:  0.080 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  24800/32453 Inputs (000)    7328 - Loss:  0.058 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  24900/32453 Inputs (000)    7341 - Loss:  0.086 - Validation loss:  0.047\n",
      "Epoch   2/4 Batch  25000/32453 Inputs (000)    7353 - Loss:  0.058 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  25100/32453 Inputs (000)    7366 - Loss:  0.067 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  25200/32453 Inputs (000)    7379 - Loss:  0.066 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  25300/32453 Inputs (000)    7392 - Loss:  0.067 - Validation loss:  0.045\n",
      "Epoch   2/4 Batch  25400/32453 Inputs (000)    7405 - Loss:  0.077 - Validation loss:  0.046\n",
      "Epoch   2/4 Batch  25500/32453 Inputs (000)    7417 - Loss:  0.068 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  25600/32453 Inputs (000)    7430 - Loss:  0.098 - Validation loss:  0.082\n",
      "Epoch   2/4 Batch  25700/32453 Inputs (000)    7443 - Loss:  0.081 - Validation loss:  0.049\n",
      "Epoch   2/4 Batch  25800/32453 Inputs (000)    7456 - Loss:  0.066 - Validation loss:  0.047\n",
      "Epoch   2/4 Batch  25900/32453 Inputs (000)    7469 - Loss:  0.066 - Validation loss:  0.047\n",
      "Epoch   2/4 Batch  26000/32453 Inputs (000)    7481 - Loss:  0.079 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  26100/32453 Inputs (000)    7494 - Loss:  0.062 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  26200/32453 Inputs (000)    7507 - Loss:  0.113 - Validation loss:  0.094\n",
      "Epoch   2/4 Batch  26300/32453 Inputs (000)    7520 - Loss:  0.072 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  26400/32453 Inputs (000)    7533 - Loss:  0.085 - Validation loss:  0.049\n",
      "Epoch   2/4 Batch  26500/32453 Inputs (000)    7545 - Loss:  0.059 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  26600/32453 Inputs (000)    7558 - Loss:  0.068 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  26700/32453 Inputs (000)    7571 - Loss:  0.075 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  26800/32453 Inputs (000)    7584 - Loss:  0.073 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  26900/32453 Inputs (000)    7597 - Loss:  0.073 - Validation loss:  0.061\n",
      "Epoch   2/4 Batch  27000/32453 Inputs (000)    7609 - Loss:  0.078 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  27100/32453 Inputs (000)    7622 - Loss:  0.061 - Validation loss:  0.050\n",
      "Epoch   2/4 Batch  27200/32453 Inputs (000)    7635 - Loss:  0.071 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  27300/32453 Inputs (000)    7648 - Loss:  0.066 - Validation loss:  0.049\n",
      "Epoch   2/4 Batch  27400/32453 Inputs (000)    7661 - Loss:  0.072 - Validation loss:  0.054\n",
      "Epoch   2/4 Batch  27500/32453 Inputs (000)    7673 - Loss:  0.071 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  27600/32453 Inputs (000)    7686 - Loss:  0.060 - Validation loss:  0.047\n",
      "Epoch   2/4 Batch  27700/32453 Inputs (000)    7699 - Loss:  0.064 - Validation loss:  0.050\n",
      "Epoch   2/4 Batch  27800/32453 Inputs (000)    7712 - Loss:  0.067 - Validation loss:  0.049\n",
      "Epoch   2/4 Batch  27900/32453 Inputs (000)    7725 - Loss:  0.076 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  28000/32453 Inputs (000)    7737 - Loss:  0.085 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  28100/32453 Inputs (000)    7750 - Loss:  0.080 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch  28200/32453 Inputs (000)    7763 - Loss:  0.072 - Validation loss:  0.050\n",
      "Epoch   2/4 Batch  28300/32453 Inputs (000)    7776 - Loss:  0.069 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  28400/32453 Inputs (000)    7789 - Loss:  0.066 - Validation loss:  0.050\n",
      "Epoch   2/4 Batch  28500/32453 Inputs (000)    7801 - Loss:  0.062 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  28600/32453 Inputs (000)    7814 - Loss:  0.061 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  28700/32453 Inputs (000)    7827 - Loss:  0.076 - Validation loss:  0.049\n",
      "Epoch   2/4 Batch  28800/32453 Inputs (000)    7840 - Loss:  0.102 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  28900/32453 Inputs (000)    7853 - Loss:  0.093 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch  29000/32453 Inputs (000)    7865 - Loss:  0.086 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  29100/32453 Inputs (000)    7878 - Loss:  0.070 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  29200/32453 Inputs (000)    7891 - Loss:  0.081 - Validation loss:  0.053\n",
      "Epoch   2/4 Batch  29300/32453 Inputs (000)    7904 - Loss:  0.075 - Validation loss:  0.055\n",
      "Epoch   2/4 Batch  29400/32453 Inputs (000)    7917 - Loss:  0.068 - Validation loss:  0.052\n",
      "Epoch   2/4 Batch  29500/32453 Inputs (000)    7929 - Loss:  0.071 - Validation loss:  0.049\n",
      "Epoch   2/4 Batch  29600/32453 Inputs (000)    7942 - Loss:  0.058 - Validation loss:  0.049\n",
      "Epoch   2/4 Batch  29700/32453 Inputs (000)    7955 - Loss:  0.086 - Validation loss:  0.050\n",
      "Epoch   2/4 Batch  29800/32453 Inputs (000)    7968 - Loss:  0.070 - Validation loss:  0.051\n",
      "Epoch   2/4 Batch  29900/32453 Inputs (000)    7981 - Loss:  0.083 - Validation loss:  0.047\n",
      "Epoch   2/4 Batch  30000/32453 Inputs (000)    7993 - Loss:  0.060 - Validation loss:  0.047\n",
      "Epoch   2/4 Batch  30100/32453 Inputs (000)    8006 - Loss:  0.053 - Validation loss:  0.045\n",
      "Epoch   2/4 Batch  30200/32453 Inputs (000)    8019 - Loss:  0.071 - Validation loss:  0.043\n",
      "Epoch   2/4 Batch  30300/32453 Inputs (000)    8032 - Loss:  0.061 - Validation loss:  0.046\n",
      "Epoch   2/4 Batch  30400/32453 Inputs (000)    8045 - Loss:  0.058 - Validation loss:  0.044\n",
      "Epoch   2/4 Batch  30500/32453 Inputs (000)    8057 - Loss:  0.074 - Validation loss:  0.044\n",
      "Epoch   2/4 Batch  30600/32453 Inputs (000)    8070 - Loss:  0.064 - Validation loss:  0.049\n",
      "Epoch   2/4 Batch  30700/32453 Inputs (000)    8083 - Loss:  0.074 - Validation loss:  0.046\n",
      "Epoch   2/4 Batch  30800/32453 Inputs (000)    8096 - Loss:  0.067 - Validation loss:  0.050\n",
      "Epoch   2/4 Batch  30900/32453 Inputs (000)    8109 - Loss:  0.074 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  31000/32453 Inputs (000)    8121 - Loss:  0.052 - Validation loss:  0.047\n",
      "Epoch   2/4 Batch  31100/32453 Inputs (000)    8134 - Loss:  0.067 - Validation loss:  0.045\n",
      "Epoch   2/4 Batch  31200/32453 Inputs (000)    8147 - Loss:  0.060 - Validation loss:  0.045\n",
      "Epoch   2/4 Batch  31300/32453 Inputs (000)    8160 - Loss:  0.065 - Validation loss:  0.048\n",
      "Epoch   2/4 Batch  31400/32453 Inputs (000)    8173 - Loss:  0.048 - Validation loss:  0.047\n",
      "Epoch   2/4 Batch  31500/32453 Inputs (000)    8185 - Loss:  0.068 - Validation loss:  0.044\n",
      "Epoch   2/4 Batch  31600/32453 Inputs (000)    8198 - Loss:  0.058 - Validation loss:  0.047\n",
      "Epoch   2/4 Batch  31700/32453 Inputs (000)    8211 - Loss:  0.076 - Validation loss:  0.046\n",
      "Epoch   2/4 Batch  31800/32453 Inputs (000)    8224 - Loss:  0.068 - Validation loss:  0.045\n",
      "Epoch   2/4 Batch  31900/32453 Inputs (000)    8237 - Loss:  0.061 - Validation loss:  0.045\n",
      "Epoch   2/4 Batch  32000/32453 Inputs (000)    8249 - Loss:  0.064 - Validation loss:  0.047\n",
      "Epoch   2/4 Batch  32100/32453 Inputs (000)    8262 - Loss:  0.066 - Validation loss:  0.045\n",
      "Epoch   2/4 Batch  32200/32453 Inputs (000)    8275 - Loss:  0.058 - Validation loss:  0.043\n",
      "Epoch   2/4 Batch  32300/32453 Inputs (000)    8288 - Loss:  0.062 - Validation loss:  0.043\n",
      "Epoch   2/4 Batch  32400/32453 Inputs (000)    8301 - Loss:  0.176 - Validation loss:  0.108\n",
      "Epoch   2/4 Batch  32453/32453 Inputs (000)    8307 - Loss:  0.078 - Validation loss:  0.056\n",
      "Saving graph...\n",
      "Model Trained in 12h:55m:1s and Saved\n",
      "INFO:tensorflow:Restoring parameters from ./large_graph/best_model.ckpt\n",
      "Batch     10/3606 - Accuracy: 52.3%\n",
      "Batch     20/3606 - Accuracy: 51.7%\n",
      "Batch     30/3606 - Accuracy: 50.8%\n",
      "Batch     40/3606 - Accuracy: 51.2%\n",
      "Batch     50/3606 - Accuracy: 51.1%\n",
      "Batch     60/3606 - Accuracy: 51.3%\n",
      "Batch     70/3606 - Accuracy: 51.5%\n",
      "Batch     80/3606 - Accuracy: 51.8%\n",
      "Batch     90/3606 - Accuracy: 51.8%\n",
      "Batch    100/3606 - Accuracy: 51.9%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch    110/3606 - Accuracy: 52.0%\n",
      "Batch    120/3606 - Accuracy: 52.1%\n",
      "Batch    130/3606 - Accuracy: 51.9%\n",
      "Batch    140/3606 - Accuracy: 51.8%\n",
      "Batch    150/3606 - Accuracy: 51.8%\n",
      "Batch    160/3606 - Accuracy: 51.9%\n",
      "Batch    170/3606 - Accuracy: 51.8%\n",
      "Batch    180/3606 - Accuracy: 51.9%\n",
      "Batch    190/3606 - Accuracy: 51.9%\n",
      "Batch    200/3606 - Accuracy: 51.8%\n",
      "Batch    210/3606 - Accuracy: 51.9%\n",
      "Batch    220/3606 - Accuracy: 51.9%\n",
      "Batch    230/3606 - Accuracy: 51.9%\n",
      "Batch    240/3606 - Accuracy: 51.9%\n",
      "Batch    250/3606 - Accuracy: 51.9%\n",
      "Batch    260/3606 - Accuracy: 51.9%\n",
      "Batch    270/3606 - Accuracy: 51.9%\n",
      "Batch    280/3606 - Accuracy: 52.0%\n",
      "Batch    290/3606 - Accuracy: 52.1%\n",
      "Batch    300/3606 - Accuracy: 52.1%\n",
      "Batch    310/3606 - Accuracy: 52.1%\n",
      "Batch    320/3606 - Accuracy: 52.1%\n",
      "Batch    330/3606 - Accuracy: 52.1%\n",
      "Batch    340/3606 - Accuracy: 52.0%\n",
      "Batch    350/3606 - Accuracy: 52.1%\n",
      "Batch    360/3606 - Accuracy: 52.0%\n",
      "Batch    370/3606 - Accuracy: 52.0%\n",
      "Batch    380/3606 - Accuracy: 52.1%\n",
      "Batch    390/3606 - Accuracy: 52.0%\n",
      "Batch    400/3606 - Accuracy: 52.1%\n",
      "Batch    410/3606 - Accuracy: 52.0%\n",
      "Batch    420/3606 - Accuracy: 52.0%\n",
      "Batch    430/3606 - Accuracy: 52.0%\n",
      "Batch    440/3606 - Accuracy: 52.1%\n",
      "Batch    450/3606 - Accuracy: 52.0%\n",
      "Batch    460/3606 - Accuracy: 52.0%\n",
      "Batch    470/3606 - Accuracy: 52.1%\n",
      "Batch    480/3606 - Accuracy: 52.1%\n",
      "Batch    490/3606 - Accuracy: 52.1%\n",
      "Batch    500/3606 - Accuracy: 52.1%\n",
      "Batch    510/3606 - Accuracy: 52.2%\n",
      "Batch    520/3606 - Accuracy: 52.2%\n",
      "Batch    530/3606 - Accuracy: 52.2%\n",
      "Batch    540/3606 - Accuracy: 52.2%\n",
      "Batch    550/3606 - Accuracy: 52.3%\n",
      "Batch    560/3606 - Accuracy: 52.3%\n",
      "Batch    570/3606 - Accuracy: 52.2%\n",
      "Batch    580/3606 - Accuracy: 52.2%\n",
      "Batch    590/3606 - Accuracy: 52.2%\n",
      "Batch    600/3606 - Accuracy: 52.2%\n",
      "Batch    610/3606 - Accuracy: 52.3%\n",
      "Batch    620/3606 - Accuracy: 52.3%\n",
      "Batch    630/3606 - Accuracy: 52.3%\n",
      "Batch    640/3606 - Accuracy: 52.3%\n",
      "Batch    650/3606 - Accuracy: 52.2%\n",
      "Batch    660/3606 - Accuracy: 52.3%\n",
      "Batch    670/3606 - Accuracy: 52.3%\n",
      "Batch    680/3606 - Accuracy: 52.3%\n",
      "Batch    690/3606 - Accuracy: 52.3%\n",
      "Batch    700/3606 - Accuracy: 52.3%\n",
      "Batch    710/3606 - Accuracy: 52.3%\n",
      "Batch    720/3606 - Accuracy: 52.4%\n",
      "Batch    730/3606 - Accuracy: 52.4%\n",
      "Batch    740/3606 - Accuracy: 52.3%\n",
      "Batch    750/3606 - Accuracy: 52.3%\n",
      "Batch    760/3606 - Accuracy: 52.4%\n",
      "Batch    770/3606 - Accuracy: 52.4%\n",
      "Batch    780/3606 - Accuracy: 52.4%\n",
      "Batch    790/3606 - Accuracy: 52.4%\n",
      "Batch    800/3606 - Accuracy: 52.4%\n",
      "Batch    810/3606 - Accuracy: 52.3%\n",
      "Batch    820/3606 - Accuracy: 52.3%\n",
      "Batch    830/3606 - Accuracy: 52.3%\n",
      "Batch    840/3606 - Accuracy: 52.3%\n",
      "Batch    850/3606 - Accuracy: 52.3%\n",
      "Batch    860/3606 - Accuracy: 52.4%\n",
      "Batch    870/3606 - Accuracy: 52.3%\n",
      "Batch    880/3606 - Accuracy: 52.4%\n",
      "Batch    890/3606 - Accuracy: 52.3%\n",
      "Batch    900/3606 - Accuracy: 52.3%\n",
      "Batch    910/3606 - Accuracy: 52.4%\n",
      "Batch    920/3606 - Accuracy: 52.3%\n",
      "Batch    930/3606 - Accuracy: 52.4%\n",
      "Batch    940/3606 - Accuracy: 52.4%\n",
      "Batch    950/3606 - Accuracy: 52.3%\n",
      "Batch    960/3606 - Accuracy: 52.4%\n",
      "Batch    970/3606 - Accuracy: 52.4%\n",
      "Batch    980/3606 - Accuracy: 52.4%\n",
      "Batch    990/3606 - Accuracy: 52.4%\n",
      "Batch   1000/3606 - Accuracy: 52.4%\n",
      "Batch   1010/3606 - Accuracy: 52.4%\n",
      "Batch   1020/3606 - Accuracy: 52.4%\n",
      "Batch   1030/3606 - Accuracy: 52.4%\n",
      "Batch   1040/3606 - Accuracy: 52.4%\n",
      "Batch   1050/3606 - Accuracy: 52.4%\n",
      "Batch   1060/3606 - Accuracy: 52.4%\n",
      "Batch   1070/3606 - Accuracy: 52.4%\n",
      "Batch   1080/3606 - Accuracy: 52.4%\n",
      "Batch   1090/3606 - Accuracy: 52.4%\n",
      "Batch   1100/3606 - Accuracy: 52.4%\n",
      "Batch   1110/3606 - Accuracy: 52.3%\n",
      "Batch   1120/3606 - Accuracy: 52.3%\n",
      "Batch   1130/3606 - Accuracy: 52.3%\n",
      "Batch   1140/3606 - Accuracy: 52.3%\n",
      "Batch   1150/3606 - Accuracy: 52.3%\n",
      "Batch   1160/3606 - Accuracy: 52.3%\n",
      "Batch   1170/3606 - Accuracy: 52.3%\n",
      "Batch   1180/3606 - Accuracy: 52.3%\n",
      "Batch   1190/3606 - Accuracy: 52.3%\n",
      "Batch   1200/3606 - Accuracy: 52.3%\n",
      "Batch   1210/3606 - Accuracy: 52.3%\n",
      "Batch   1220/3606 - Accuracy: 52.4%\n",
      "Batch   1230/3606 - Accuracy: 52.4%\n",
      "Batch   1240/3606 - Accuracy: 52.4%\n",
      "Batch   1250/3606 - Accuracy: 52.4%\n",
      "Batch   1260/3606 - Accuracy: 52.4%\n",
      "Batch   1270/3606 - Accuracy: 52.4%\n",
      "Batch   1280/3606 - Accuracy: 52.4%\n",
      "Batch   1290/3606 - Accuracy: 52.4%\n",
      "Batch   1300/3606 - Accuracy: 52.4%\n",
      "Batch   1310/3606 - Accuracy: 52.4%\n",
      "Batch   1320/3606 - Accuracy: 52.4%\n",
      "Batch   1330/3606 - Accuracy: 52.4%\n",
      "Batch   1340/3606 - Accuracy: 52.4%\n",
      "Batch   1350/3606 - Accuracy: 52.4%\n",
      "Batch   1360/3606 - Accuracy: 52.4%\n",
      "Batch   1370/3606 - Accuracy: 52.4%\n",
      "Batch   1380/3606 - Accuracy: 52.4%\n",
      "Batch   1390/3606 - Accuracy: 52.4%\n",
      "Batch   1400/3606 - Accuracy: 52.4%\n",
      "Batch   1410/3606 - Accuracy: 52.4%\n",
      "Batch   1420/3606 - Accuracy: 52.4%\n",
      "Batch   1430/3606 - Accuracy: 52.3%\n",
      "Batch   1440/3606 - Accuracy: 52.3%\n",
      "Batch   1450/3606 - Accuracy: 52.3%\n",
      "Batch   1460/3606 - Accuracy: 52.3%\n",
      "Batch   1470/3606 - Accuracy: 52.4%\n",
      "Batch   1480/3606 - Accuracy: 52.3%\n",
      "Batch   1490/3606 - Accuracy: 52.3%\n",
      "Batch   1500/3606 - Accuracy: 52.3%\n",
      "Batch   1510/3606 - Accuracy: 52.3%\n",
      "Batch   1520/3606 - Accuracy: 52.3%\n",
      "Batch   1530/3606 - Accuracy: 52.3%\n",
      "Batch   1540/3606 - Accuracy: 52.3%\n",
      "Batch   1550/3606 - Accuracy: 52.3%\n",
      "Batch   1560/3606 - Accuracy: 52.3%\n",
      "Batch   1570/3606 - Accuracy: 52.3%\n",
      "Batch   1580/3606 - Accuracy: 52.3%\n",
      "Batch   1590/3606 - Accuracy: 52.3%\n",
      "Batch   1600/3606 - Accuracy: 52.3%\n",
      "Batch   1610/3606 - Accuracy: 52.3%\n",
      "Batch   1620/3606 - Accuracy: 52.3%\n",
      "Batch   1630/3606 - Accuracy: 52.3%\n",
      "Batch   1640/3606 - Accuracy: 52.3%\n",
      "Batch   1650/3606 - Accuracy: 52.3%\n",
      "Batch   1660/3606 - Accuracy: 52.3%\n",
      "Batch   1670/3606 - Accuracy: 52.3%\n",
      "Batch   1680/3606 - Accuracy: 52.3%\n",
      "Batch   1690/3606 - Accuracy: 52.3%\n",
      "Batch   1700/3606 - Accuracy: 52.3%\n",
      "Batch   1710/3606 - Accuracy: 52.3%\n",
      "Batch   1720/3606 - Accuracy: 52.3%\n",
      "Batch   1730/3606 - Accuracy: 52.3%\n",
      "Batch   1740/3606 - Accuracy: 52.3%\n",
      "Batch   1750/3606 - Accuracy: 52.3%\n",
      "Batch   1760/3606 - Accuracy: 52.3%\n",
      "Batch   1770/3606 - Accuracy: 52.3%\n",
      "Batch   1780/3606 - Accuracy: 52.3%\n",
      "Batch   1790/3606 - Accuracy: 52.4%\n",
      "Batch   1800/3606 - Accuracy: 52.4%\n",
      "Batch   1810/3606 - Accuracy: 52.4%\n",
      "Batch   1820/3606 - Accuracy: 52.4%\n",
      "Batch   1830/3606 - Accuracy: 52.4%\n",
      "Batch   1840/3606 - Accuracy: 52.4%\n",
      "Batch   1850/3606 - Accuracy: 52.4%\n",
      "Batch   1860/3606 - Accuracy: 52.4%\n",
      "Batch   1870/3606 - Accuracy: 52.4%\n",
      "Batch   1880/3606 - Accuracy: 52.4%\n",
      "Batch   1890/3606 - Accuracy: 52.4%\n",
      "Batch   1900/3606 - Accuracy: 52.4%\n",
      "Batch   1910/3606 - Accuracy: 52.4%\n",
      "Batch   1920/3606 - Accuracy: 52.4%\n",
      "Batch   1930/3606 - Accuracy: 52.4%\n",
      "Batch   1940/3606 - Accuracy: 52.4%\n",
      "Batch   1950/3606 - Accuracy: 52.4%\n",
      "Batch   1960/3606 - Accuracy: 52.4%\n",
      "Batch   1970/3606 - Accuracy: 52.4%\n",
      "Batch   1980/3606 - Accuracy: 52.4%\n",
      "Batch   1990/3606 - Accuracy: 52.4%\n",
      "Batch   2000/3606 - Accuracy: 52.4%\n",
      "Batch   2010/3606 - Accuracy: 52.4%\n",
      "Batch   2020/3606 - Accuracy: 52.4%\n",
      "Batch   2030/3606 - Accuracy: 52.4%\n",
      "Batch   2040/3606 - Accuracy: 52.4%\n",
      "Batch   2050/3606 - Accuracy: 52.4%\n",
      "Batch   2060/3606 - Accuracy: 52.4%\n",
      "Batch   2070/3606 - Accuracy: 52.4%\n",
      "Batch   2080/3606 - Accuracy: 52.4%\n",
      "Batch   2090/3606 - Accuracy: 52.4%\n",
      "Batch   2100/3606 - Accuracy: 52.4%\n",
      "Batch   2110/3606 - Accuracy: 52.4%\n",
      "Batch   2120/3606 - Accuracy: 52.4%\n",
      "Batch   2130/3606 - Accuracy: 52.4%\n",
      "Batch   2140/3606 - Accuracy: 52.4%\n",
      "Batch   2150/3606 - Accuracy: 52.4%\n",
      "Batch   2160/3606 - Accuracy: 52.4%\n",
      "Batch   2170/3606 - Accuracy: 52.4%\n",
      "Batch   2180/3606 - Accuracy: 52.4%\n",
      "Batch   2190/3606 - Accuracy: 52.4%\n",
      "Batch   2200/3606 - Accuracy: 52.4%\n",
      "Batch   2210/3606 - Accuracy: 52.4%\n",
      "Batch   2220/3606 - Accuracy: 52.4%\n",
      "Batch   2230/3606 - Accuracy: 52.4%\n",
      "Batch   2240/3606 - Accuracy: 52.4%\n",
      "Batch   2250/3606 - Accuracy: 52.4%\n",
      "Batch   2260/3606 - Accuracy: 52.4%\n",
      "Batch   2270/3606 - Accuracy: 52.4%\n",
      "Batch   2280/3606 - Accuracy: 52.4%\n",
      "Batch   2290/3606 - Accuracy: 52.4%\n",
      "Batch   2300/3606 - Accuracy: 52.4%\n",
      "Batch   2310/3606 - Accuracy: 52.4%\n",
      "Batch   2320/3606 - Accuracy: 52.4%\n",
      "Batch   2330/3606 - Accuracy: 52.4%\n",
      "Batch   2340/3606 - Accuracy: 52.4%\n",
      "Batch   2350/3606 - Accuracy: 52.4%\n",
      "Batch   2360/3606 - Accuracy: 52.4%\n",
      "Batch   2370/3606 - Accuracy: 52.4%\n",
      "Batch   2380/3606 - Accuracy: 52.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch   2390/3606 - Accuracy: 52.4%\n",
      "Batch   2400/3606 - Accuracy: 52.4%\n",
      "Batch   2410/3606 - Accuracy: 52.4%\n",
      "Batch   2420/3606 - Accuracy: 52.4%\n",
      "Batch   2430/3606 - Accuracy: 52.4%\n",
      "Batch   2440/3606 - Accuracy: 52.4%\n",
      "Batch   2450/3606 - Accuracy: 52.4%\n",
      "Batch   2460/3606 - Accuracy: 52.4%\n",
      "Batch   2470/3606 - Accuracy: 52.4%\n",
      "Batch   2480/3606 - Accuracy: 52.4%\n",
      "Batch   2490/3606 - Accuracy: 52.4%\n",
      "Batch   2500/3606 - Accuracy: 52.4%\n",
      "Batch   2510/3606 - Accuracy: 52.4%\n",
      "Batch   2520/3606 - Accuracy: 52.4%\n",
      "Batch   2530/3606 - Accuracy: 52.4%\n",
      "Batch   2540/3606 - Accuracy: 52.4%\n",
      "Batch   2550/3606 - Accuracy: 52.4%\n",
      "Batch   2560/3606 - Accuracy: 52.4%\n",
      "Batch   2570/3606 - Accuracy: 52.4%\n",
      "Batch   2580/3606 - Accuracy: 52.4%\n",
      "Batch   2590/3606 - Accuracy: 52.4%\n",
      "Batch   2600/3606 - Accuracy: 52.4%\n",
      "Batch   2610/3606 - Accuracy: 52.4%\n",
      "Batch   2620/3606 - Accuracy: 52.4%\n",
      "Batch   2630/3606 - Accuracy: 52.4%\n",
      "Batch   2640/3606 - Accuracy: 52.4%\n",
      "Batch   2650/3606 - Accuracy: 52.4%\n",
      "Batch   2660/3606 - Accuracy: 52.4%\n",
      "Batch   2670/3606 - Accuracy: 52.4%\n",
      "Batch   2680/3606 - Accuracy: 52.4%\n",
      "Batch   2690/3606 - Accuracy: 52.4%\n",
      "Batch   2700/3606 - Accuracy: 52.4%\n",
      "Batch   2710/3606 - Accuracy: 52.4%\n",
      "Batch   2720/3606 - Accuracy: 52.4%\n",
      "Batch   2730/3606 - Accuracy: 52.4%\n",
      "Batch   2740/3606 - Accuracy: 52.4%\n",
      "Batch   2750/3606 - Accuracy: 52.4%\n",
      "Batch   2760/3606 - Accuracy: 52.4%\n",
      "Batch   2770/3606 - Accuracy: 52.4%\n",
      "Batch   2780/3606 - Accuracy: 52.4%\n",
      "Batch   2790/3606 - Accuracy: 52.4%\n",
      "Batch   2800/3606 - Accuracy: 52.4%\n",
      "Batch   2810/3606 - Accuracy: 52.4%\n",
      "Batch   2820/3606 - Accuracy: 52.4%\n",
      "Batch   2830/3606 - Accuracy: 52.4%\n",
      "Batch   2840/3606 - Accuracy: 52.4%\n",
      "Batch   2850/3606 - Accuracy: 52.4%\n",
      "Batch   2860/3606 - Accuracy: 52.4%\n",
      "Batch   2870/3606 - Accuracy: 52.4%\n",
      "Batch   2880/3606 - Accuracy: 52.4%\n",
      "Batch   2890/3606 - Accuracy: 52.4%\n",
      "Batch   2900/3606 - Accuracy: 52.4%\n",
      "Batch   2910/3606 - Accuracy: 52.4%\n",
      "Batch   2920/3606 - Accuracy: 52.4%\n",
      "Batch   2930/3606 - Accuracy: 52.4%\n",
      "Batch   2940/3606 - Accuracy: 52.4%\n",
      "Batch   2950/3606 - Accuracy: 52.4%\n",
      "Batch   2960/3606 - Accuracy: 52.4%\n",
      "Batch   2970/3606 - Accuracy: 52.4%\n",
      "Batch   2980/3606 - Accuracy: 52.4%\n",
      "Batch   2990/3606 - Accuracy: 52.4%\n",
      "Batch   3000/3606 - Accuracy: 52.4%\n",
      "Batch   3010/3606 - Accuracy: 52.4%\n",
      "Batch   3020/3606 - Accuracy: 52.4%\n",
      "Batch   3030/3606 - Accuracy: 52.4%\n",
      "Batch   3040/3606 - Accuracy: 52.4%\n",
      "Batch   3050/3606 - Accuracy: 52.4%\n",
      "Batch   3060/3606 - Accuracy: 52.4%\n",
      "Batch   3070/3606 - Accuracy: 52.4%\n",
      "Batch   3080/3606 - Accuracy: 52.4%\n",
      "Batch   3090/3606 - Accuracy: 52.4%\n",
      "Batch   3100/3606 - Accuracy: 52.4%\n",
      "Batch   3110/3606 - Accuracy: 52.4%\n",
      "Batch   3120/3606 - Accuracy: 52.4%\n",
      "Batch   3130/3606 - Accuracy: 52.4%\n",
      "Batch   3140/3606 - Accuracy: 52.4%\n",
      "Batch   3150/3606 - Accuracy: 52.4%\n",
      "Batch   3160/3606 - Accuracy: 52.4%\n",
      "Batch   3170/3606 - Accuracy: 52.4%\n",
      "Batch   3180/3606 - Accuracy: 52.4%\n",
      "Batch   3190/3606 - Accuracy: 52.4%\n",
      "Batch   3200/3606 - Accuracy: 52.4%\n",
      "Batch   3210/3606 - Accuracy: 52.4%\n",
      "Batch   3220/3606 - Accuracy: 52.4%\n",
      "Batch   3230/3606 - Accuracy: 52.4%\n",
      "Batch   3240/3606 - Accuracy: 52.4%\n",
      "Batch   3250/3606 - Accuracy: 52.4%\n",
      "Batch   3260/3606 - Accuracy: 52.4%\n",
      "Batch   3270/3606 - Accuracy: 52.4%\n",
      "Batch   3280/3606 - Accuracy: 52.4%\n",
      "Batch   3290/3606 - Accuracy: 52.4%\n",
      "Batch   3300/3606 - Accuracy: 52.4%\n",
      "Batch   3310/3606 - Accuracy: 52.4%\n",
      "Batch   3320/3606 - Accuracy: 52.4%\n",
      "Batch   3330/3606 - Accuracy: 52.4%\n",
      "Batch   3340/3606 - Accuracy: 52.4%\n",
      "Batch   3350/3606 - Accuracy: 52.4%\n",
      "Batch   3360/3606 - Accuracy: 52.4%\n",
      "Batch   3370/3606 - Accuracy: 52.4%\n",
      "Batch   3380/3606 - Accuracy: 52.4%\n",
      "Batch   3390/3606 - Accuracy: 52.4%\n",
      "Batch   3400/3606 - Accuracy: 52.4%\n",
      "Batch   3410/3606 - Accuracy: 52.4%\n",
      "Batch   3420/3606 - Accuracy: 52.4%\n",
      "Batch   3430/3606 - Accuracy: 52.4%\n",
      "Batch   3440/3606 - Accuracy: 52.4%\n",
      "Batch   3450/3606 - Accuracy: 52.4%\n",
      "Batch   3460/3606 - Accuracy: 52.4%\n",
      "Batch   3470/3606 - Accuracy: 52.4%\n",
      "Batch   3480/3606 - Accuracy: 52.4%\n",
      "Batch   3490/3606 - Accuracy: 52.4%\n",
      "Batch   3500/3606 - Accuracy: 52.4%\n",
      "Batch   3510/3606 - Accuracy: 52.4%\n",
      "Batch   3520/3606 - Accuracy: 52.4%\n",
      "Batch   3530/3606 - Accuracy: 52.4%\n",
      "Batch   3540/3606 - Accuracy: 52.4%\n",
      "Batch   3550/3606 - Accuracy: 52.4%\n",
      "Batch   3560/3606 - Accuracy: 52.4%\n",
      "Batch   3570/3606 - Accuracy: 52.4%\n",
      "Batch   3580/3606 - Accuracy: 52.4%\n",
      "Batch   3590/3606 - Accuracy: 52.4%\n",
      "Batch   3600/3606 - Accuracy: 52.4%\n",
      "Final accuracy = 52.4%\n",
      "\n",
      "Reloading existing graph to continue training.\n",
      "INFO:tensorflow:Restoring parameters from ./large_graph/best_model.ckpt\n",
      "Epoch   3/4 Batch    100/32453 Inputs (000)    8320 - Loss:  0.077 - Validation loss:  0.049\n",
      "Epoch   3/4 Batch    200/32453 Inputs (000)    8333 - Loss:  0.069 - Validation loss:  0.048\n",
      "Epoch   3/4 Batch    300/32453 Inputs (000)    8346 - Loss:  0.073 - Validation loss:  0.049\n",
      "Epoch   3/4 Batch    400/32453 Inputs (000)    8359 - Loss:  0.062 - Validation loss:  0.049\n",
      "Epoch   3/4 Batch    500/32453 Inputs (000)    8371 - Loss:  0.065 - Validation loss:  0.050\n",
      "Epoch   3/4 Batch    600/32453 Inputs (000)    8384 - Loss:  0.078 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch    700/32453 Inputs (000)    8397 - Loss:  0.070 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch    800/32453 Inputs (000)    8410 - Loss:  0.066 - Validation loss:  0.048\n",
      "Epoch   3/4 Batch    900/32453 Inputs (000)    8423 - Loss:  0.057 - Validation loss:  0.047\n",
      "Epoch   3/4 Batch   1000/32453 Inputs (000)    8435 - Loss:  0.075 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch   1100/32453 Inputs (000)    8448 - Loss:  0.052 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch   1200/32453 Inputs (000)    8461 - Loss:  0.071 - Validation loss:  0.047\n",
      "Epoch   3/4 Batch   1300/32453 Inputs (000)    8474 - Loss:  0.062 - Validation loss:  0.051\n",
      "Epoch   3/4 Batch   1400/32453 Inputs (000)    8487 - Loss:  0.067 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch   1500/32453 Inputs (000)    8499 - Loss:  0.058 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch   1600/32453 Inputs (000)    8512 - Loss:  0.048 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch   1700/32453 Inputs (000)    8525 - Loss:  0.064 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch   1800/32453 Inputs (000)    8538 - Loss:  0.059 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch   1900/32453 Inputs (000)    8551 - Loss:  0.066 - Validation loss:  0.049\n",
      "Epoch   3/4 Batch   2000/32453 Inputs (000)    8563 - Loss:  0.053 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch   2100/32453 Inputs (000)    8576 - Loss:  0.064 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch   2200/32453 Inputs (000)    8589 - Loss:  0.073 - Validation loss:  0.054\n",
      "Epoch   3/4 Batch   2300/32453 Inputs (000)    8602 - Loss:  0.057 - Validation loss:  0.050\n",
      "Epoch   3/4 Batch   2400/32453 Inputs (000)    8615 - Loss:  0.070 - Validation loss:  0.047\n",
      "Epoch   3/4 Batch   2500/32453 Inputs (000)    8627 - Loss:  0.058 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch   2600/32453 Inputs (000)    8640 - Loss:  0.055 - Validation loss:  0.048\n",
      "Epoch   3/4 Batch   2700/32453 Inputs (000)    8653 - Loss:  0.065 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch   2800/32453 Inputs (000)    8666 - Loss:  0.064 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch   2900/32453 Inputs (000)    8679 - Loss:  0.055 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch   3000/32453 Inputs (000)    8691 - Loss:  0.059 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch   3100/32453 Inputs (000)    8704 - Loss:  0.084 - Validation loss:  0.048\n",
      "Epoch   3/4 Batch   3200/32453 Inputs (000)    8717 - Loss:  0.083 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch   3300/32453 Inputs (000)    8730 - Loss:  0.081 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch   3400/32453 Inputs (000)    8743 - Loss:  0.071 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch   3500/32453 Inputs (000)    8755 - Loss:  0.054 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch   3600/32453 Inputs (000)    8768 - Loss:  0.077 - Validation loss:  0.048\n",
      "Epoch   3/4 Batch   3700/32453 Inputs (000)    8781 - Loss:  0.067 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch   3800/32453 Inputs (000)    8794 - Loss:  0.058 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch   3900/32453 Inputs (000)    8807 - Loss:  0.072 - Validation loss:  0.048\n",
      "Epoch   3/4 Batch   4000/32453 Inputs (000)    8819 - Loss:  0.075 - Validation loss:  0.044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/4 Batch   4100/32453 Inputs (000)    8832 - Loss:  0.073 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch   4200/32453 Inputs (000)    8845 - Loss:  0.066 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch   4300/32453 Inputs (000)    8858 - Loss:  0.086 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch   4400/32453 Inputs (000)    8871 - Loss:  0.070 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch   4500/32453 Inputs (000)    8883 - Loss:  0.061 - Validation loss:  0.047\n",
      "Epoch   3/4 Batch   4600/32453 Inputs (000)    8896 - Loss:  0.066 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch   4700/32453 Inputs (000)    8909 - Loss:  0.079 - Validation loss:  0.048\n",
      "Epoch   3/4 Batch   4800/32453 Inputs (000)    8922 - Loss:  0.055 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch   4900/32453 Inputs (000)    8935 - Loss:  0.084 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch   5000/32453 Inputs (000)    8947 - Loss:  0.057 - Validation loss:  0.047\n",
      "Epoch   3/4 Batch   5100/32453 Inputs (000)    8960 - Loss:  0.068 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch   5200/32453 Inputs (000)    8973 - Loss:  0.067 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch   5300/32453 Inputs (000)    8986 - Loss:  0.067 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch   5400/32453 Inputs (000)    8999 - Loss:  0.057 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch   5500/32453 Inputs (000)    9011 - Loss:  0.070 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch   5600/32453 Inputs (000)    9024 - Loss:  0.047 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch   5700/32453 Inputs (000)    9037 - Loss:  0.059 - Validation loss:  0.048\n",
      "Epoch   3/4 Batch   5800/32453 Inputs (000)    9050 - Loss:  0.084 - Validation loss:  0.069\n",
      "Epoch   3/4 Batch   5900/32453 Inputs (000)    9063 - Loss:  0.053 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch   6000/32453 Inputs (000)    9075 - Loss:  0.062 - Validation loss:  0.048\n",
      "Epoch   3/4 Batch   6100/32453 Inputs (000)    9088 - Loss:  0.066 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch   6200/32453 Inputs (000)    9101 - Loss:  0.051 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch   6300/32453 Inputs (000)    9114 - Loss:  0.062 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch   6400/32453 Inputs (000)    9127 - Loss:  0.056 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch   6500/32453 Inputs (000)    9139 - Loss:  0.069 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch   6600/32453 Inputs (000)    9152 - Loss:  0.065 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch   6700/32453 Inputs (000)    9165 - Loss:  0.068 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch   6800/32453 Inputs (000)    9178 - Loss:  0.066 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch   6900/32453 Inputs (000)    9191 - Loss:  0.082 - Validation loss:  0.049\n",
      "Epoch   3/4 Batch   7000/32453 Inputs (000)    9203 - Loss:  0.060 - Validation loss:  0.050\n",
      "Epoch   3/4 Batch   7100/32453 Inputs (000)    9216 - Loss:  0.071 - Validation loss:  0.048\n",
      "Epoch   3/4 Batch   7200/32453 Inputs (000)    9229 - Loss:  0.058 - Validation loss:  0.047\n",
      "Epoch   3/4 Batch   7300/32453 Inputs (000)    9242 - Loss:  0.082 - Validation loss:  0.048\n",
      "Epoch   3/4 Batch   7400/32453 Inputs (000)    9255 - Loss:  0.067 - Validation loss:  0.049\n",
      "Epoch   3/4 Batch   7500/32453 Inputs (000)    9267 - Loss:  0.068 - Validation loss:  0.048\n",
      "Epoch   3/4 Batch   7600/32453 Inputs (000)    9280 - Loss:  0.070 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch   7700/32453 Inputs (000)    9293 - Loss:  0.067 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch   7800/32453 Inputs (000)    9306 - Loss:  0.066 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch   7900/32453 Inputs (000)    9319 - Loss:  0.062 - Validation loss:  0.051\n",
      "Epoch   3/4 Batch   8000/32453 Inputs (000)    9331 - Loss:  0.073 - Validation loss:  0.047\n",
      "Epoch   3/4 Batch   8100/32453 Inputs (000)    9344 - Loss:  0.078 - Validation loss:  0.047\n",
      "Epoch   3/4 Batch   8200/32453 Inputs (000)    9357 - Loss:  0.061 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch   8300/32453 Inputs (000)    9370 - Loss:  0.067 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch   8400/32453 Inputs (000)    9383 - Loss:  0.060 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch   8500/32453 Inputs (000)    9395 - Loss:  0.049 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch   8600/32453 Inputs (000)    9408 - Loss:  0.059 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch   8700/32453 Inputs (000)    9421 - Loss:  0.056 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch   8800/32453 Inputs (000)    9434 - Loss:  0.058 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch   8900/32453 Inputs (000)    9447 - Loss:  0.068 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch   9000/32453 Inputs (000)    9459 - Loss:  0.052 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch   9100/32453 Inputs (000)    9472 - Loss:  0.059 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch   9200/32453 Inputs (000)    9485 - Loss:  0.188 - Validation loss:  0.113\n",
      "Epoch   3/4 Batch   9300/32453 Inputs (000)    9498 - Loss:  0.086 - Validation loss:  0.050\n",
      "Epoch   3/4 Batch   9400/32453 Inputs (000)    9511 - Loss:  0.083 - Validation loss:  0.047\n",
      "Epoch   3/4 Batch   9500/32453 Inputs (000)    9523 - Loss:  0.076 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch   9600/32453 Inputs (000)    9536 - Loss:  0.072 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch   9700/32453 Inputs (000)    9549 - Loss:  0.063 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch   9800/32453 Inputs (000)    9562 - Loss:  0.063 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch   9900/32453 Inputs (000)    9575 - Loss:  0.056 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch  10000/32453 Inputs (000)    9587 - Loss:  0.056 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch  10100/32453 Inputs (000)    9600 - Loss:  0.053 - Validation loss:  0.040\n",
      "Epoch   3/4 Batch  10200/32453 Inputs (000)    9613 - Loss:  0.068 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  10300/32453 Inputs (000)    9626 - Loss:  0.063 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  10400/32453 Inputs (000)    9639 - Loss:  0.063 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  10500/32453 Inputs (000)    9651 - Loss:  0.060 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  10600/32453 Inputs (000)    9664 - Loss:  0.057 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch  10700/32453 Inputs (000)    9677 - Loss:  0.068 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  10800/32453 Inputs (000)    9690 - Loss:  0.068 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch  10900/32453 Inputs (000)    9703 - Loss:  0.051 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  11000/32453 Inputs (000)    9715 - Loss:  0.064 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  11100/32453 Inputs (000)    9728 - Loss:  0.061 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch  11200/32453 Inputs (000)    9741 - Loss:  0.065 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  11300/32453 Inputs (000)    9754 - Loss:  0.068 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  11400/32453 Inputs (000)    9767 - Loss:  0.053 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch  11500/32453 Inputs (000)    9779 - Loss:  0.060 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch  11600/32453 Inputs (000)    9792 - Loss:  0.069 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch  11700/32453 Inputs (000)    9805 - Loss:  0.054 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch  11800/32453 Inputs (000)    9818 - Loss:  0.057 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  11900/32453 Inputs (000)    9831 - Loss:  0.057 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  12000/32453 Inputs (000)    9843 - Loss:  0.067 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  12100/32453 Inputs (000)    9856 - Loss:  0.072 - Validation loss:  0.040\n",
      "Epoch   3/4 Batch  12200/32453 Inputs (000)    9869 - Loss:  0.054 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch  12300/32453 Inputs (000)    9882 - Loss:  0.034 - Validation loss:  0.040\n",
      "Epoch   3/4 Batch  12400/32453 Inputs (000)    9895 - Loss:  0.058 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch  12500/32453 Inputs (000)    9907 - Loss:  0.063 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch  12600/32453 Inputs (000)    9920 - Loss:  0.064 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch  12700/32453 Inputs (000)    9933 - Loss:  0.060 - Validation loss:  0.040\n",
      "Epoch   3/4 Batch  12800/32453 Inputs (000)    9946 - Loss:  0.060 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  12900/32453 Inputs (000)    9959 - Loss:  0.063 - Validation loss:  0.041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   3/4 Batch  13000/32453 Inputs (000)    9971 - Loss:  0.051 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch  13100/32453 Inputs (000)    9984 - Loss:  0.053 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  13200/32453 Inputs (000)    9997 - Loss:  0.121 - Validation loss:  0.075\n",
      "Epoch   3/4 Batch  13300/32453 Inputs (000)   10010 - Loss:  0.065 - Validation loss:  0.047\n",
      "Epoch   3/4 Batch  13400/32453 Inputs (000)   10023 - Loss:  0.053 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  13500/32453 Inputs (000)   10035 - Loss:  0.069 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  13600/32453 Inputs (000)   10048 - Loss:  0.063 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  13700/32453 Inputs (000)   10061 - Loss:  0.059 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  13800/32453 Inputs (000)   10074 - Loss:  0.081 - Validation loss:  0.051\n",
      "Epoch   3/4 Batch  13900/32453 Inputs (000)   10087 - Loss:  0.065 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch  14000/32453 Inputs (000)   10099 - Loss:  0.082 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch  14100/32453 Inputs (000)   10112 - Loss:  0.071 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  14200/32453 Inputs (000)   10125 - Loss:  0.067 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch  14300/32453 Inputs (000)   10138 - Loss:  0.052 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  14400/32453 Inputs (000)   10151 - Loss:  0.076 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  14500/32453 Inputs (000)   10163 - Loss:  0.067 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  14600/32453 Inputs (000)   10176 - Loss:  0.050 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  14700/32453 Inputs (000)   10189 - Loss:  0.054 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  14800/32453 Inputs (000)   10202 - Loss:  0.050 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  14900/32453 Inputs (000)   10215 - Loss:  0.069 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  15000/32453 Inputs (000)   10227 - Loss:  0.055 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  15100/32453 Inputs (000)   10240 - Loss:  0.060 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch  15200/32453 Inputs (000)   10253 - Loss:  0.076 - Validation loss:  0.048\n",
      "Epoch   3/4 Batch  15300/32453 Inputs (000)   10266 - Loss:  0.062 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch  15400/32453 Inputs (000)   10279 - Loss:  0.057 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  15500/32453 Inputs (000)   10291 - Loss:  0.062 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  15600/32453 Inputs (000)   10304 - Loss:  0.060 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  15700/32453 Inputs (000)   10317 - Loss:  0.064 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  15800/32453 Inputs (000)   10330 - Loss:  0.070 - Validation loss:  0.049\n",
      "Epoch   3/4 Batch  15900/32453 Inputs (000)   10343 - Loss:  0.061 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch  16000/32453 Inputs (000)   10355 - Loss:  0.062 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  16100/32453 Inputs (000)   10368 - Loss:  0.045 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  16200/32453 Inputs (000)   10381 - Loss:  0.055 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  16300/32453 Inputs (000)   10394 - Loss:  0.054 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  16400/32453 Inputs (000)   10407 - Loss:  0.051 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  16500/32453 Inputs (000)   10419 - Loss:  0.061 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  16600/32453 Inputs (000)   10432 - Loss:  0.061 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch  16700/32453 Inputs (000)   10445 - Loss:  0.060 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  16800/32453 Inputs (000)   10458 - Loss:  0.047 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  16900/32453 Inputs (000)   10471 - Loss:  0.069 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  17000/32453 Inputs (000)   10483 - Loss:  0.063 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch  17100/32453 Inputs (000)   10496 - Loss:  0.060 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  17200/32453 Inputs (000)   10509 - Loss:  0.055 - Validation loss:  0.040\n",
      "Epoch   3/4 Batch  17300/32453 Inputs (000)   10522 - Loss:  0.042 - Validation loss:  0.040\n",
      "Epoch   3/4 Batch  17400/32453 Inputs (000)   10535 - Loss:  0.067 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  17500/32453 Inputs (000)   10547 - Loss:  0.060 - Validation loss:  0.039\n",
      "Epoch   3/4 Batch  17600/32453 Inputs (000)   10560 - Loss:  0.060 - Validation loss:  0.039\n",
      "Epoch   3/4 Batch  17700/32453 Inputs (000)   10573 - Loss:  0.053 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch  17800/32453 Inputs (000)   10586 - Loss:  0.073 - Validation loss:  0.053\n",
      "Epoch   3/4 Batch  17900/32453 Inputs (000)   10599 - Loss:  0.064 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  18000/32453 Inputs (000)   10611 - Loss:  0.071 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  18100/32453 Inputs (000)   10624 - Loss:  0.077 - Validation loss:  0.040\n",
      "Epoch   3/4 Batch  18200/32453 Inputs (000)   10637 - Loss:  0.071 - Validation loss:  0.041\n",
      "Epoch   3/4 Batch  18300/32453 Inputs (000)   10650 - Loss:  0.056 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  18400/32453 Inputs (000)   10663 - Loss:  0.047 - Validation loss:  0.043\n",
      "Epoch   3/4 Batch  18500/32453 Inputs (000)   10675 - Loss:  0.061 - Validation loss:  0.048\n",
      "Epoch   3/4 Batch  18600/32453 Inputs (000)   10688 - Loss:  0.065 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  18700/32453 Inputs (000)   10701 - Loss:  0.063 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  18800/32453 Inputs (000)   10714 - Loss:  0.055 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  18900/32453 Inputs (000)   10727 - Loss:  0.060 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch  19000/32453 Inputs (000)   10739 - Loss:  0.053 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  19100/32453 Inputs (000)   10752 - Loss:  0.053 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  19200/32453 Inputs (000)   10765 - Loss:  0.065 - Validation loss:  0.040\n",
      "Epoch   3/4 Batch  19300/32453 Inputs (000)   10778 - Loss:  0.053 - Validation loss:  0.040\n",
      "Epoch   3/4 Batch  19400/32453 Inputs (000)   10791 - Loss:  0.054 - Validation loss:  0.042\n",
      "Epoch   3/4 Batch  19500/32453 Inputs (000)   10803 - Loss:  0.096 - Validation loss:  0.066\n",
      "Epoch   3/4 Batch  19600/32453 Inputs (000)   10816 - Loss:  0.075 - Validation loss:  0.049\n",
      "Epoch   3/4 Batch  19700/32453 Inputs (000)   10829 - Loss:  0.064 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  19800/32453 Inputs (000)   10842 - Loss:  0.066 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch  19900/32453 Inputs (000)   10855 - Loss:  0.056 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  20000/32453 Inputs (000)   10867 - Loss:  0.078 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  20100/32453 Inputs (000)   10880 - Loss:  0.077 - Validation loss:  0.052\n",
      "Epoch   3/4 Batch  20200/32453 Inputs (000)   10893 - Loss:  0.061 - Validation loss:  0.048\n",
      "Epoch   3/4 Batch  20300/32453 Inputs (000)   10906 - Loss:  0.065 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch  20400/32453 Inputs (000)   10919 - Loss:  0.042 - Validation loss:  0.044\n",
      "Epoch   3/4 Batch  20500/32453 Inputs (000)   10931 - Loss:  0.048 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch  20600/32453 Inputs (000)   10944 - Loss:  0.045 - Validation loss:  0.046\n",
      "Epoch   3/4 Batch  20700/32453 Inputs (000)   10957 - Loss:  0.067 - Validation loss:  0.045\n",
      "Epoch   3/4 Batch  20800/32453 Inputs (000)   10970 - Loss:  0.049 - Validation loss:  0.044\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-1614611c177c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_hyperparameters_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmessage\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch_i\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m# Print time spent training the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-cecc7ddd803c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch_i)\u001b[0m\n\u001b[0;32m     64\u001b[0m                  \u001b[0mtarget_sequence_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtargets_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                  \u001b[0msource_sequence_length\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msources_lengths\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m                  keep_prob: keep_probability})\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_i\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;31m# batch_i starts at zero so batch is the batch number\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Run through all the epoch, computing the accuracy after each and sending the results via email\n",
    "for epoch_i in range(1, epochs + 1):\n",
    "    \n",
    "    message = get_hyperparameters_message()\n",
    "    message += train(epoch_i)\n",
    "\n",
    "    # Print time spent training the model\n",
    "    end = time.time()\n",
    "    seconds = end - start\n",
    "    m, s = divmod(seconds, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print(\"Model Trained in {}h:{}m:{}s and Saved\".format(int(h), int(m), int(s)))\n",
    "    message += \"\\nModel training for {}h:{}m:{}s and saved.\".format(int(h), int(m), int(s))\n",
    "    \n",
    "    # Get current accuracy\n",
    "    accuracy = get_accuracy(validation_source_sentences, validation_target_sentences)\n",
    "    message += \"\\nCurrent accuracy = {:.1%}\".format(accuracy)\n",
    "    \n",
    "    # Send email updates if using AWS\n",
    "#     subject = \"Completed training epoch {} - Accuracy = {:.1%}\".format(epoch_i, accuracy)\n",
    "#     if(small):\n",
    "#         if(epoch_i % 10 == 0): # Only send email every 10 epoch when using small data\n",
    "#             send_email(subject, message)\n",
    "#     else: # Send an email after every epoch with large data\n",
    "#         send_email(subject, message)\n",
    "    \n",
    "print(\"\\nTraining completed.\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
